<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>7 Regression | Marketing Analytics 2021</title>
  <meta name="description" content="An Introduction to Data Analytics Using R" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="7 Regression | Marketing Analytics 2021" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="An Introduction to Data Analytics Using R" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="7 Regression | Marketing Analytics 2021" />
  
  <meta name="twitter:description" content="An Introduction to Data Analytics Using R" />
  




  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  <link rel="shortcut icon" href="favicon.ico" type="image/x-icon" />
<link rel="prev" href="hypothesis-testing.html"/>
<link rel="next" href="exploratory-factor-analysis.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<link href="libs/pagedtable-1.1/css/pagedtable.css" rel="stylesheet" />
<script src="libs/pagedtable-1.1/js/pagedtable.js"></script>
<link href="libs/anchor-sections-1.0/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />
<script src="libs/htmlwidgets-1.5.2/htmlwidgets.js"></script>
<script src="libs/plotly-binding-4.9.2.1/plotly.js"></script>
<script src="libs/typedarray-0.1/typedarray.min.js"></script>
<link href="libs/crosstalk-1.1.0.1/css/crosstalk.css" rel="stylesheet" />
<script src="libs/crosstalk-1.1.0.1/js/crosstalk.min.js"></script>
<link href="libs/plotly-htmlwidgets-css-1.52.2/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotly-main-1.52.2/plotly-latest.min.js"></script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><strong><a href="./">MA 2021</a></strong></li>

<li class="divider"></li>
<li class="part"><span><b>I Course outline</b></span></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Welcome!</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i>Introduction</a><ul>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#course-structure"><i class="fa fa-check"></i>Course structure</a><ul>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#schedule"><i class="fa fa-check"></i>Schedule</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#grading"><i class="fa fa-check"></i>Grading</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#course-materials"><i class="fa fa-check"></i>Course materials</a><ul>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#main-reference"><i class="fa fa-check"></i>Main reference</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#further-readings"><i class="fa fa-check"></i>Further readings</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#discussion-forum"><i class="fa fa-check"></i>Discussion forum</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#datacamp"><i class="fa fa-check"></i>DataCamp</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#other-web-resources"><i class="fa fa-check"></i>Other web-resources</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#contact"><i class="fa fa-check"></i>Contact</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>II Lecture notes</b></span></li>
<li class="chapter" data-level="1" data-path="preliminaries.html"><a href="preliminaries.html"><i class="fa fa-check"></i><b>1</b> Preliminaries</a><ul>
<li class="chapter" data-level="1.1" data-path="preliminaries.html"><a href="preliminaries.html#marketing-foundations-recap"><i class="fa fa-check"></i><b>1.1</b> Marketing foundations (recap)</a></li>
<li class="chapter" data-level="1.2" data-path="preliminaries.html"><a href="preliminaries.html#the-research-process"><i class="fa fa-check"></i><b>1.2</b> The research process</a><ul>
<li class="chapter" data-level="1.2.1" data-path="preliminaries.html"><a href="preliminaries.html#research-question-and-hypothesis"><i class="fa fa-check"></i><b>1.2.1</b> Research question and hypothesis</a></li>
<li class="chapter" data-level="1.2.2" data-path="preliminaries.html"><a href="preliminaries.html#choosing-a-research-design"><i class="fa fa-check"></i><b>1.2.2</b> Choosing a research design</a></li>
<li class="chapter" data-level="1.2.3" data-path="preliminaries.html"><a href="preliminaries.html#collecting-data"><i class="fa fa-check"></i><b>1.2.3</b> Collecting data</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="preliminaries.html"><a href="preliminaries.html#learning-check"><i class="fa fa-check"></i>Learning check</a></li>
<li class="chapter" data-level="" data-path="preliminaries.html"><a href="preliminaries.html#references"><i class="fa fa-check"></i>References</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="getting-started-with-r.html"><a href="getting-started-with-r.html"><i class="fa fa-check"></i><b>2</b> Getting started with R</a><ul>
<li class="chapter" data-level="2.1" data-path="getting-started-with-r.html"><a href="getting-started-with-r.html#how-to-download-and-install-r-and-rstudio"><i class="fa fa-check"></i><b>2.1</b> How to download and install R and RStudio</a></li>
<li class="chapter" data-level="2.2" data-path="getting-started-with-r.html"><a href="getting-started-with-r.html#the-r-studio-interface"><i class="fa fa-check"></i><b>2.2</b> The R Studio interface</a></li>
<li class="chapter" data-level="2.3" data-path="getting-started-with-r.html"><a href="getting-started-with-r.html#functions"><i class="fa fa-check"></i><b>2.3</b> Functions</a></li>
<li class="chapter" data-level="2.4" data-path="getting-started-with-r.html"><a href="getting-started-with-r.html#packages"><i class="fa fa-check"></i><b>2.4</b> Packages</a></li>
<li class="chapter" data-level="2.5" data-path="getting-started-with-r.html"><a href="getting-started-with-r.html#a-typical-r-session"><i class="fa fa-check"></i><b>2.5</b> A typical R session</a></li>
<li class="chapter" data-level="2.6" data-path="getting-started-with-r.html"><a href="getting-started-with-r.html#getting-help"><i class="fa fa-check"></i><b>2.6</b> Getting help</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="data-handling.html"><a href="data-handling.html"><i class="fa fa-check"></i><b>3</b> Data handling</a><ul>
<li class="chapter" data-level="3.1" data-path="data-handling.html"><a href="data-handling.html#basic-data-handling"><i class="fa fa-check"></i><b>3.1</b> Basic data handling</a><ul>
<li class="chapter" data-level="3.1.1" data-path="data-handling.html"><a href="data-handling.html#creating-objects"><i class="fa fa-check"></i><b>3.1.1</b> Creating objects</a></li>
<li class="chapter" data-level="3.1.2" data-path="data-handling.html"><a href="data-handling.html#data-types"><i class="fa fa-check"></i><b>3.1.2</b> Data types</a></li>
<li class="chapter" data-level="3.1.3" data-path="data-handling.html"><a href="data-handling.html#data-structures"><i class="fa fa-check"></i><b>3.1.3</b> Data structures</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="data-handling.html"><a href="data-handling.html#data-import-and-export"><i class="fa fa-check"></i><b>3.2</b> Data import and export</a><ul>
<li class="chapter" data-level="3.2.1" data-path="data-handling.html"><a href="data-handling.html#getting-data-for-this-course"><i class="fa fa-check"></i><b>3.2.1</b> Getting data for this course</a></li>
<li class="chapter" data-level="3.2.2" data-path="data-handling.html"><a href="data-handling.html#import-data-created-by-other-software-packages"><i class="fa fa-check"></i><b>3.2.2</b> Import data created by other software packages</a></li>
<li class="chapter" data-level="3.2.3" data-path="data-handling.html"><a href="data-handling.html#import-data-from-qualtrics"><i class="fa fa-check"></i><b>3.2.3</b> Import data from Qualtrics</a></li>
<li class="chapter" data-level="3.2.4" data-path="data-handling.html"><a href="data-handling.html#export-data"><i class="fa fa-check"></i><b>3.2.4</b> Export data</a></li>
<li class="chapter" data-level="3.2.5" data-path="data-handling.html"><a href="data-handling.html#import-data-from-the-web"><i class="fa fa-check"></i><b>3.2.5</b> Import data from the Web</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="data-handling.html"><a href="data-handling.html#learning-check-1"><i class="fa fa-check"></i>Learning check</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="summarizing-data.html"><a href="summarizing-data.html"><i class="fa fa-check"></i><b>4</b> Summarizing data</a><ul>
<li class="chapter" data-level="4.1" data-path="summarizing-data.html"><a href="summarizing-data.html#summary-statistics"><i class="fa fa-check"></i><b>4.1</b> Summary statistics</a><ul>
<li class="chapter" data-level="4.1.1" data-path="summarizing-data.html"><a href="summarizing-data.html#categorical-variables"><i class="fa fa-check"></i><b>4.1.1</b> Categorical variables</a></li>
<li class="chapter" data-level="4.1.2" data-path="summarizing-data.html"><a href="summarizing-data.html#continuous-variables"><i class="fa fa-check"></i><b>4.1.2</b> Continuous variables</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="summarizing-data.html"><a href="summarizing-data.html#data-visualization"><i class="fa fa-check"></i><b>4.2</b> Data visualization</a><ul>
<li class="chapter" data-level="4.2.1" data-path="summarizing-data.html"><a href="summarizing-data.html#categorical-variables-1"><i class="fa fa-check"></i><b>4.2.1</b> Categorical variables</a></li>
<li class="chapter" data-level="4.2.2" data-path="summarizing-data.html"><a href="summarizing-data.html#continuous-variables-1"><i class="fa fa-check"></i><b>4.2.2</b> Continuous variables</a></li>
<li class="chapter" data-level="4.2.3" data-path="summarizing-data.html"><a href="summarizing-data.html#saving-plots"><i class="fa fa-check"></i><b>4.2.3</b> Saving plots</a></li>
<li class="chapter" data-level="4.2.4" data-path="summarizing-data.html"><a href="summarizing-data.html#ggplot-extensions"><i class="fa fa-check"></i><b>4.2.4</b> ggplot extensions</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="summarizing-data.html"><a href="summarizing-data.html#learning-check-2"><i class="fa fa-check"></i>Learning check</a></li>
<li class="chapter" data-level="" data-path="summarizing-data.html"><a href="summarizing-data.html#references-1"><i class="fa fa-check"></i>References</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="statistical-inference.html"><a href="statistical-inference.html"><i class="fa fa-check"></i><b>5</b> Statistical inference</a><ul>
<li class="chapter" data-level="5.1" data-path="statistical-inference.html"><a href="statistical-inference.html#if-we-knew-it-all"><i class="fa fa-check"></i><b>5.1</b> If we knew it all</a><ul>
<li class="chapter" data-level="5.1.1" data-path="statistical-inference.html"><a href="statistical-inference.html#sampling-from-a-known-population"><i class="fa fa-check"></i><b>5.1.1</b> Sampling from a known population</a></li>
<li class="chapter" data-level="5.1.2" data-path="statistical-inference.html"><a href="statistical-inference.html#standard-error-of-the-mean"><i class="fa fa-check"></i><b>5.1.2</b> Standard error of the mean</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="statistical-inference.html"><a href="statistical-inference.html#the-central-limit-theorem"><i class="fa fa-check"></i><b>5.2</b> The Central Limit Theorem</a></li>
<li class="chapter" data-level="5.3" data-path="statistical-inference.html"><a href="statistical-inference.html#using-what-we-actually-know"><i class="fa fa-check"></i><b>5.3</b> Using what we actually know</a></li>
<li class="chapter" data-level="5.4" data-path="statistical-inference.html"><a href="statistical-inference.html#confidence-intervals-for-the-sample-mean"><i class="fa fa-check"></i><b>5.4</b> Confidence Intervals for the Sample Mean</a></li>
<li class="chapter" data-level="" data-path="statistical-inference.html"><a href="statistical-inference.html#learning-check-3"><i class="fa fa-check"></i>Learning check</a></li>
<li class="chapter" data-level="" data-path="statistical-inference.html"><a href="statistical-inference.html#references-2"><i class="fa fa-check"></i>References</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html"><i class="fa fa-check"></i><b>6</b> Hypothesis testing</a><ul>
<li class="chapter" data-level="6.1" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#introduction-1"><i class="fa fa-check"></i><b>6.1</b> Introduction</a><ul>
<li class="chapter" data-level="6.1.1" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#the-null-hypothesis"><i class="fa fa-check"></i><b>6.1.1</b> The null hypothesis</a></li>
<li class="chapter" data-level="6.1.2" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#statistical-inference-on-a-sample"><i class="fa fa-check"></i><b>6.1.2</b> Statistical inference on a sample</a></li>
<li class="chapter" data-level="6.1.3" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#choosing-the-right-test"><i class="fa fa-check"></i><b>6.1.3</b> Choosing the right test</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#one-sample-t-test"><i class="fa fa-check"></i><b>6.2</b> One sample t-test</a></li>
<li class="chapter" data-level="6.3" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#comparing-two-means"><i class="fa fa-check"></i><b>6.3</b> Comparing two means</a><ul>
<li class="chapter" data-level="6.3.1" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#independent-means-t-test"><i class="fa fa-check"></i><b>6.3.1</b> Independent-means t-test</a></li>
<li class="chapter" data-level="6.3.2" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#dependent-means-t-test"><i class="fa fa-check"></i><b>6.3.2</b> Dependent-means t-test</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#nhst-considerations"><i class="fa fa-check"></i><b>6.4</b> NHST considerations</a><ul>
<li class="chapter" data-level="6.4.1" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#type-i-and-type-ii-errors"><i class="fa fa-check"></i><b>6.4.1</b> Type I and Type II Errors</a></li>
<li class="chapter" data-level="6.4.2" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#significance-level-sample-size-power-and-effect-size"><i class="fa fa-check"></i><b>6.4.2</b> Significance level, sample size, power, and effect size</a></li>
<li class="chapter" data-level="6.4.3" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#p-values-stopping-rules-and-p-hacking"><i class="fa fa-check"></i><b>6.4.3</b> P-values, stopping rules and p-hacking</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#comparing-several-means"><i class="fa fa-check"></i><b>6.5</b> Comparing several means</a><ul>
<li class="chapter" data-level="6.5.1" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#introduction-2"><i class="fa fa-check"></i><b>6.5.1</b> Introduction</a></li>
<li class="chapter" data-level="6.5.2" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#decomposing-variance"><i class="fa fa-check"></i><b>6.5.2</b> Decomposing variance</a></li>
<li class="chapter" data-level="6.5.3" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#one-way-anova"><i class="fa fa-check"></i><b>6.5.3</b> One-way ANOVA</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#non-parametric-tests"><i class="fa fa-check"></i><b>6.6</b> Non-parametric tests</a><ul>
<li class="chapter" data-level="6.6.1" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#mann-whitney-u-test-a.k.a.-wilcoxon-rank-sum-test"><i class="fa fa-check"></i><b>6.6.1</b> Mann-Whitney U Test (a.k.a. Wilcoxon rank-sum test)</a></li>
<li class="chapter" data-level="6.6.2" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#wilcoxon-signed-rank-test"><i class="fa fa-check"></i><b>6.6.2</b> Wilcoxon signed-rank test</a></li>
<li class="chapter" data-level="6.6.3" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#kruskal-wallis-test"><i class="fa fa-check"></i><b>6.6.3</b> Kruskal-Wallis test</a></li>
</ul></li>
<li class="chapter" data-level="6.7" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#categorical-data"><i class="fa fa-check"></i><b>6.7</b> Categorical data</a><ul>
<li class="chapter" data-level="6.7.1" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#confidence-intervals-for-proportions"><i class="fa fa-check"></i><b>6.7.1</b> Confidence intervals for proportions</a></li>
<li class="chapter" data-level="6.7.2" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#chi-square-test"><i class="fa fa-check"></i><b>6.7.2</b> Chi-square test</a></li>
<li class="chapter" data-level="6.7.3" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#sample-size-1"><i class="fa fa-check"></i><b>6.7.3</b> Sample size</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#learning-check-4"><i class="fa fa-check"></i>Learning check</a></li>
<li class="chapter" data-level="" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#references-3"><i class="fa fa-check"></i>References</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="regression.html"><a href="regression.html"><i class="fa fa-check"></i><b>7</b> Regression</a><ul>
<li class="chapter" data-level="7.1" data-path="regression.html"><a href="regression.html#correlation"><i class="fa fa-check"></i><b>7.1</b> Correlation</a><ul>
<li class="chapter" data-level="7.1.1" data-path="regression.html"><a href="regression.html#correlation-coefficient"><i class="fa fa-check"></i><b>7.1.1</b> Correlation coefficient</a></li>
<li class="chapter" data-level="7.1.2" data-path="regression.html"><a href="regression.html#significance-testing"><i class="fa fa-check"></i><b>7.1.2</b> Significance testing</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="regression.html"><a href="regression.html#regression-1"><i class="fa fa-check"></i><b>7.2</b> Regression</a><ul>
<li class="chapter" data-level="7.2.1" data-path="regression.html"><a href="regression.html#simple-linear-regression"><i class="fa fa-check"></i><b>7.2.1</b> Simple linear regression</a></li>
<li class="chapter" data-level="7.2.2" data-path="regression.html"><a href="regression.html#multiple-linear-regression"><i class="fa fa-check"></i><b>7.2.2</b> Multiple linear regression</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="regression.html"><a href="regression.html#potential-problems"><i class="fa fa-check"></i><b>7.3</b> Potential problems</a><ul>
<li class="chapter" data-level="7.3.1" data-path="regression.html"><a href="regression.html#outliers"><i class="fa fa-check"></i><b>7.3.1</b> Outliers</a></li>
<li class="chapter" data-level="7.3.2" data-path="regression.html"><a href="regression.html#influential-observations"><i class="fa fa-check"></i><b>7.3.2</b> Influential observations</a></li>
<li class="chapter" data-level="7.3.3" data-path="regression.html"><a href="regression.html#non-linearity"><i class="fa fa-check"></i><b>7.3.3</b> Non-linearity</a></li>
<li class="chapter" data-level="7.3.4" data-path="regression.html"><a href="regression.html#non-constant-error-variance"><i class="fa fa-check"></i><b>7.3.4</b> Non-constant error variance</a></li>
<li class="chapter" data-level="7.3.5" data-path="regression.html"><a href="regression.html#non-normally-distributed-errors"><i class="fa fa-check"></i><b>7.3.5</b> Non-normally distributed errors</a></li>
<li class="chapter" data-level="7.3.6" data-path="regression.html"><a href="regression.html#correlation-of-errors"><i class="fa fa-check"></i><b>7.3.6</b> Correlation of errors</a></li>
<li class="chapter" data-level="7.3.7" data-path="regression.html"><a href="regression.html#collinearity"><i class="fa fa-check"></i><b>7.3.7</b> Collinearity</a></li>
<li class="chapter" data-level="7.3.8" data-path="regression.html"><a href="regression.html#omitted-variables"><i class="fa fa-check"></i><b>7.3.8</b> Omitted Variables</a></li>
<li class="chapter" data-level="7.3.9" data-path="regression.html"><a href="regression.html#overfitting"><i class="fa fa-check"></i><b>7.3.9</b> Overfitting</a></li>
<li class="chapter" data-level="7.3.10" data-path="regression.html"><a href="regression.html#variable-selection"><i class="fa fa-check"></i><b>7.3.10</b> Variable selection</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="regression.html"><a href="regression.html#categorical-predictors"><i class="fa fa-check"></i><b>7.4</b> Categorical predictors</a><ul>
<li class="chapter" data-level="7.4.1" data-path="regression.html"><a href="regression.html#two-categories"><i class="fa fa-check"></i><b>7.4.1</b> Two categories</a></li>
<li class="chapter" data-level="7.4.2" data-path="regression.html"><a href="regression.html#more-than-two-categories"><i class="fa fa-check"></i><b>7.4.2</b> More than two categories</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="regression.html"><a href="regression.html#extensions-of-the-linear-model"><i class="fa fa-check"></i><b>7.5</b> Extensions of the linear model</a><ul>
<li class="chapter" data-level="7.5.1" data-path="regression.html"><a href="regression.html#interaction-effects"><i class="fa fa-check"></i><b>7.5.1</b> Interaction effects</a></li>
<li class="chapter" data-level="7.5.2" data-path="regression.html"><a href="regression.html#non-linear-relationships"><i class="fa fa-check"></i><b>7.5.2</b> Non-linear relationships</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="regression.html"><a href="regression.html#logistic-regression"><i class="fa fa-check"></i><b>7.6</b> Logistic regression</a><ul>
<li class="chapter" data-level="7.6.1" data-path="regression.html"><a href="regression.html#motivation-and-intuition"><i class="fa fa-check"></i><b>7.6.1</b> Motivation and intuition</a></li>
<li class="chapter" data-level="7.6.2" data-path="regression.html"><a href="regression.html#technical-details-of-the-model"><i class="fa fa-check"></i><b>7.6.2</b> Technical details of the model</a></li>
<li class="chapter" data-level="7.6.3" data-path="regression.html"><a href="regression.html#estimation-in-r"><i class="fa fa-check"></i><b>7.6.3</b> Estimation in R</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="regression.html"><a href="regression.html#learning-check-5"><i class="fa fa-check"></i>Learning check</a></li>
<li class="chapter" data-level="" data-path="regression.html"><a href="regression.html#references-4"><i class="fa fa-check"></i>References</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="exploratory-factor-analysis.html"><a href="exploratory-factor-analysis.html"><i class="fa fa-check"></i><b>8</b> Exploratory factor analysis</a><ul>
<li class="chapter" data-level="8.1" data-path="exploratory-factor-analysis.html"><a href="exploratory-factor-analysis.html#introduction-3"><i class="fa fa-check"></i><b>8.1</b> Introduction</a></li>
<li class="chapter" data-level="8.2" data-path="exploratory-factor-analysis.html"><a href="exploratory-factor-analysis.html#steps-in-factor-analysis"><i class="fa fa-check"></i><b>8.2</b> Steps in factor analysis</a><ul>
<li class="chapter" data-level="8.2.1" data-path="exploratory-factor-analysis.html"><a href="exploratory-factor-analysis.html#are-the-assumptions-satisfied"><i class="fa fa-check"></i><b>8.2.1</b> Are the assumptions satisfied?</a></li>
<li class="chapter" data-level="8.2.2" data-path="exploratory-factor-analysis.html"><a href="exploratory-factor-analysis.html#deriving-factors"><i class="fa fa-check"></i><b>8.2.2</b> Deriving factors</a></li>
<li class="chapter" data-level="8.2.3" data-path="exploratory-factor-analysis.html"><a href="exploratory-factor-analysis.html#factor-interpretation"><i class="fa fa-check"></i><b>8.2.3</b> Factor interpretation</a></li>
<li class="chapter" data-level="8.2.4" data-path="exploratory-factor-analysis.html"><a href="exploratory-factor-analysis.html#creating-new-variables"><i class="fa fa-check"></i><b>8.2.4</b> Creating new variables</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="exploratory-factor-analysis.html"><a href="exploratory-factor-analysis.html#reliability-analysis"><i class="fa fa-check"></i><b>8.3</b> Reliability analysis</a></li>
<li class="chapter" data-level="" data-path="exploratory-factor-analysis.html"><a href="exploratory-factor-analysis.html#learning-check-6"><i class="fa fa-check"></i>Learning check</a></li>
<li class="chapter" data-level="" data-path="exploratory-factor-analysis.html"><a href="exploratory-factor-analysis.html#references-5"><i class="fa fa-check"></i>References</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="cluster-analysis.html"><a href="cluster-analysis.html"><i class="fa fa-check"></i><b>9</b> Cluster analysis</a><ul>
<li class="chapter" data-level="9.1" data-path="cluster-analysis.html"><a href="cluster-analysis.html#k-means"><i class="fa fa-check"></i><b>9.1</b> K-Means</a></li>
<li class="chapter" data-level="9.2" data-path="cluster-analysis.html"><a href="cluster-analysis.html#hierarchical-clustering"><i class="fa fa-check"></i><b>9.2</b> Hierarchical Clustering</a></li>
</ul></li>
<li class="part"><span><b>III Assignments</b></span></li>
<li class="chapter" data-level="10" data-path="r-markdown.html"><a href="r-markdown.html"><i class="fa fa-check"></i><b>10</b> R Markdown</a><ul>
<li class="chapter" data-level="10.1" data-path="r-markdown.html"><a href="r-markdown.html#introduction-to-r-markdown"><i class="fa fa-check"></i><b>10.1</b> Introduction to R Markdown</a><ul>
<li class="chapter" data-level="" data-path="r-markdown.html"><a href="r-markdown.html#creating-a-new-r-markdown-document"><i class="fa fa-check"></i>Creating a new R Markdown document</a></li>
<li class="chapter" data-level="" data-path="r-markdown.html"><a href="r-markdown.html#text-and-equations"><i class="fa fa-check"></i>Text and Equations</a></li>
<li class="chapter" data-level="" data-path="r-markdown.html"><a href="r-markdown.html#r-code"><i class="fa fa-check"></i>R-Code</a></li>
<li class="chapter" data-level="" data-path="r-markdown.html"><a href="r-markdown.html#latex-math"><i class="fa fa-check"></i>LaTeX Math</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>IV Group project</b></span></li>
<li class="chapter" data-level="11" data-path="survey-design-analysis.html"><a href="survey-design-analysis.html"><i class="fa fa-check"></i><b>11</b> Survey design &amp; analysis</a><ul>
<li class="chapter" data-level="11.1" data-path="survey-design-analysis.html"><a href="survey-design-analysis.html#your-tasks"><i class="fa fa-check"></i><b>11.1</b> Your tasks</a></li>
<li class="chapter" data-level="11.2" data-path="survey-design-analysis.html"><a href="survey-design-analysis.html#topics-for-the-group-project"><i class="fa fa-check"></i><b>11.2</b> Topics for the group project</a></li>
<li class="chapter" data-level="11.3" data-path="survey-design-analysis.html"><a href="survey-design-analysis.html#general-information"><i class="fa fa-check"></i><b>11.3</b> General information</a></li>
<li class="chapter" data-level="11.4" data-path="survey-design-analysis.html"><a href="survey-design-analysis.html#questionnaire-design"><i class="fa fa-check"></i><b>11.4</b> Questionnaire design</a><ul>
<li class="chapter" data-level="11.4.1" data-path="survey-design-analysis.html"><a href="survey-design-analysis.html#guidelines-for-the-submission-and-presentation"><i class="fa fa-check"></i><b>11.4.1</b> Guidelines for the submission and presentation</a></li>
<li class="chapter" data-level="11.4.2" data-path="survey-design-analysis.html"><a href="survey-design-analysis.html#guidelines-for-the-design-of-your-questionnaire"><i class="fa fa-check"></i><b>11.4.2</b> Guidelines for the design of your questionnaire</a></li>
<li class="chapter" data-level="11.4.3" data-path="survey-design-analysis.html"><a href="survey-design-analysis.html#test-your-questionnaire"><i class="fa fa-check"></i><b>11.4.3</b> Test your questionnaire</a></li>
</ul></li>
<li class="chapter" data-level="11.5" data-path="survey-design-analysis.html"><a href="survey-design-analysis.html#part-2-data-collection-and-analysis"><i class="fa fa-check"></i><b>11.5</b> Part 2: Data collection and analysis</a><ul>
<li class="chapter" data-level="11.5.1" data-path="survey-design-analysis.html"><a href="survey-design-analysis.html#collecting-data-1"><i class="fa fa-check"></i><b>11.5.1</b> Collecting data</a></li>
<li class="chapter" data-level="11.5.2" data-path="survey-design-analysis.html"><a href="survey-design-analysis.html#data-analysis"><i class="fa fa-check"></i><b>11.5.2</b> Data analysis</a></li>
<li class="chapter" data-level="11.5.3" data-path="survey-design-analysis.html"><a href="survey-design-analysis.html#reporting"><i class="fa fa-check"></i><b>11.5.3</b> Reporting</a></li>
</ul></li>
<li class="chapter" data-level="11.6" data-path="survey-design-analysis.html"><a href="survey-design-analysis.html#frequently-asked-questions"><i class="fa fa-check"></i><b>11.6</b> Frequently asked questions</a><ul>
<li class="chapter" data-level="11.6.1" data-path="survey-design-analysis.html"><a href="survey-design-analysis.html#multiple-answers-stored-in-one-cell-in-csv"><i class="fa fa-check"></i><b>11.6.1</b> Multiple answers stored in one cell in CSV</a></li>
<li class="chapter" data-level="11.6.2" data-path="survey-design-analysis.html"><a href="survey-design-analysis.html#labels-for-numeric-values-in-csv-output"><i class="fa fa-check"></i><b>11.6.2</b> Labels for numeric values in CSV output</a></li>
<li class="chapter" data-level="11.6.3" data-path="survey-design-analysis.html"><a href="survey-design-analysis.html#bar-chart-for-multiple-choice-question-with-multiple-answers"><i class="fa fa-check"></i><b>11.6.3</b> Bar chart for multiple choice question with multiple answers</a></li>
<li class="chapter" data-level="11.6.4" data-path="survey-design-analysis.html"><a href="survey-design-analysis.html#how-to-create-a-radar-plot"><i class="fa fa-check"></i><b>11.6.4</b> How to create a radar plot?</a></li>
<li class="chapter" data-level="11.6.5" data-path="survey-design-analysis.html"><a href="survey-design-analysis.html#vertical-likert-line-chart"><i class="fa fa-check"></i><b>11.6.5</b> Vertical Likert line chart</a></li>
<li class="chapter" data-level="11.6.6" data-path="survey-design-analysis.html"><a href="survey-design-analysis.html#diverging-stacked-barchart"><i class="fa fa-check"></i><b>11.6.6</b> Diverging stacked barchart</a></li>
<li class="chapter" data-level="11.6.7" data-path="survey-design-analysis.html"><a href="survey-design-analysis.html#collapserecode-categories"><i class="fa fa-check"></i><b>11.6.7</b> Collapse/recode categories</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>V FAQ</b></span></li>
<li class="chapter" data-level="12" data-path="faq.html"><a href="faq.html"><i class="fa fa-check"></i><b>12</b> FAQ</a><ul>
<li class="chapter" data-level="12.1" data-path="faq.html"><a href="faq.html#common-error-messages"><i class="fa fa-check"></i><b>12.1</b> Common error messages</a><ul>
<li class="chapter" data-level="" data-path="faq.html"><a href="faq.html#a-general-note-on-error-messages"><i class="fa fa-check"></i>A general note on error messages</a></li>
<li class="chapter" data-level="" data-path="faq.html"><a href="faq.html#error-in-filefile-rt-cannot-open-the-connection"><i class="fa fa-check"></i>Error in file(file, “rt”): cannot open the connection</a></li>
<li class="chapter" data-level="" data-path="faq.html"><a href="faq.html#error-unexpected-symbol-in-call"><i class="fa fa-check"></i>Error: unexpected ‘SYMBOL’ in “CALL”</a></li>
<li class="chapter" data-level="" data-path="faq.html"><a href="faq.html#error-in-call-object-of-type-closure-is-not-subsettable"><i class="fa fa-check"></i>Error in CALL : object of type ‘closure’ is not subsettable</a></li>
<li class="chapter" data-level="" data-path="faq.html"><a href="faq.html#error-in-call_with_-operator-is-invalid-for-atomic-vectors"><i class="fa fa-check"></i>Error in CALL_WITH_$: $ operator is invalid for atomic vectors</a></li>
<li class="chapter" data-level="" data-path="faq.html"><a href="faq.html#error-in-call-object-name-not-found"><i class="fa fa-check"></i>Error in CALL: object ‘NAME’ not found</a></li>
<li class="chapter" data-level="" data-path="faq.html"><a href="faq.html#error-in-call-could-not-find-function-function"><i class="fa fa-check"></i>Error in CALL: could not find function “FUNCTION”</a></li>
<li class="chapter" data-level="" data-path="faq.html"><a href="faq.html#error-in-call-incorrect-number-of-dimension"><i class="fa fa-check"></i>Error in CALL: incorrect number of dimension</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="faq.html"><a href="faq.html#installation-of-r-packages"><i class="fa fa-check"></i><b>12.2</b> Installation of R packages</a><ul>
<li class="chapter" data-level="" data-path="faq.html"><a href="faq.html#what-are-the-different-ways-to-install-r-packages"><i class="fa fa-check"></i>What are the different ways to install R packages?</a></li>
<li class="chapter" data-level="" data-path="faq.html"><a href="faq.html#i-cannot-install-packages-due-to-error-in-contrib.urlrepossource-or-warning-message-package-packagename-is-not-available-for-this-version-of-r"><i class="fa fa-check"></i>I cannot install packages due to “Error in contrib.url(repos,”source“)” or “Warning message: package ‘PACKAGENAME’ is not available for this version of R”</a></li>
<li class="chapter" data-level="" data-path="faq.html"><a href="faq.html#some-libraries-with-graphical-output-e.g.-summarytools-magick-fail-to-installload-properly-on-macos"><i class="fa fa-check"></i>Some libraries with graphical output (e.g., summarytools, magick) fail to install/load properly on MacOS</a></li>
<li class="chapter" data-level="" data-path="faq.html"><a href="faq.html#i-cannot-install-some-packages-on-macos"><i class="fa fa-check"></i>I cannot install some packages on MacOS</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="faq.html"><a href="faq.html#issues-with-statistics-and-data"><i class="fa fa-check"></i><b>12.3</b> Issues with statistics and data</a><ul>
<li class="chapter" data-level="" data-path="faq.html"><a href="faq.html#why-does-a-multi-item-scale-lead-to-increased-reliability"><i class="fa fa-check"></i>Why does a multi-item scale lead to increased reliability?</a></li>
<li class="chapter" data-level="" data-path="faq.html"><a href="faq.html#why-can-demeaningstandardization-lead-to-missing-values"><i class="fa fa-check"></i>Why can demeaning/standardization lead to missing values?</a></li>
<li class="chapter" data-level="" data-path="faq.html"><a href="faq.html#the-confidence-interval-ci-of-the-mean-seems-very-small-compared-to-the-dispersion-of-my-sample.-can-this-be-correct"><i class="fa fa-check"></i>The confidence interval (CI) of the mean seems very small compared to the dispersion of my sample. Can this be correct?</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="faq.html"><a href="faq.html#errors-related-to-specific-methods"><i class="fa fa-check"></i><b>12.4</b> Errors related to specific methods</a><ul>
<li class="chapter" data-level="" data-path="faq.html"><a href="faq.html#logistic-regression-1"><i class="fa fa-check"></i>Logistic regression</a></li>
<li class="chapter" data-level="" data-path="faq.html"><a href="faq.html#when-using-logistic-regression-error-in-weights-y-non-numeric-argument-to-binary-operator"><i class="fa fa-check"></i>When using logistic regression: Error in weights * y : non-numeric argument to binary operator</a></li>
</ul></li>
<li class="chapter" data-level="12.5" data-path="faq.html"><a href="faq.html#general-settings-and-options"><i class="fa fa-check"></i><b>12.5</b> General settings and options</a><ul>
<li class="chapter" data-level="" data-path="faq.html"><a href="faq.html#numbers-are-formatted-weirdly"><i class="fa fa-check"></i>Numbers are formatted weirdly</a></li>
</ul></li>
<li class="chapter" data-level="12.6" data-path="faq.html"><a href="faq.html#data-visualizationoutput-issues"><i class="fa fa-check"></i><b>12.6</b> Data visualization/output issues</a><ul>
<li><a href="faq.html#how-can-the-geom-colors-in-a-ggplot-be-changed">How can the <code>geom</code> colors in a <code>ggplot</code> be changed?</a></li>
<li class="chapter" data-level="" data-path="faq.html"><a href="faq.html#why-are-some-histograms-displayed-differently"><i class="fa fa-check"></i>Why are some histograms displayed differently?</a></li>
<li class="chapter" data-level="" data-path="faq.html"><a href="faq.html#some-labels-in-plots-are-cut-off.-how-can-i-extend-the-plot-margins"><i class="fa fa-check"></i>Some labels in plots are cut off. How can I extend the plot margins?</a></li>
</ul></li>
<li class="chapter" data-level="12.7" data-path="faq.html"><a href="faq.html#issues-with-functions-and-function-arguments"><i class="fa fa-check"></i><b>12.7</b> Issues with functions and function arguments</a><ul>
<li><a href="faq.html#problems-with-factor-and-as.factor">Problems with <code>factor</code> and <code>as.factor</code></a></li>
<li class="chapter" data-level="" data-path="faq.html"><a href="faq.html#how-can-i-find-an-explanation-of-the-output-of-a-function"><i class="fa fa-check"></i>How can I find an explanation of the output of a function?</a></li>
<li><a href="faq.html#what-does-the-margin-argument-do">What does the <code>MARGIN</code> argument do?</a></li>
</ul></li>
<li class="chapter" data-level="12.8" data-path="faq.html"><a href="faq.html#issues-with-r-markdown"><i class="fa fa-check"></i><b>12.8</b> Issues with R Markdown</a><ul>
<li><a href="faq.html#i-get-an-error-when-knitting-to-pdf-but-it-works-for-html">I get an error when <code>knit</code>ting to PDF but it works for HTML</a></li>
<li class="chapter" data-level="" data-path="faq.html"><a href="faq.html#i-am-not-sure-where-r-code-latex-math-and-text-go"><i class="fa fa-check"></i>I am not sure where R-code, LaTeX math, and text go</a></li>
</ul></li>
<li class="chapter" data-level="12.9" data-path="faq.html"><a href="faq.html#new-questions"><i class="fa fa-check"></i><b>12.9</b> New questions</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Marketing Analytics 2021</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="regression" class="section level1">
<h1><span class="header-section-number">7</span> Regression</h1>
<div class="infobox download">
<p><a href="./Code/10-regression.R">You can download the corresponding R-Code here</a></p>
</div>
<div id="correlation" class="section level2">
<h2><span class="header-section-number">7.1</span> Correlation</h2>
<br>
<div align="center">
<iframe width="560" height="315" src="https://www.youtube.com/embed/HeIW9_ijHF4" frameborder="0" allowfullscreen>
</iframe>
</div>
<p><br></p>
<p>Before we start with regression analysis, we will review the basic concept of correlation first. Correlation helps us to determine the degree to which the variation in one variable, X, is related to the variation in another variable, Y.</p>
<div id="correlation-coefficient" class="section level3">
<h3><span class="header-section-number">7.1.1</span> Correlation coefficient</h3>
<p>The correlation coefficient summarizes the strength of the linear relationship between two metric (interval or ratio scaled) variables. Let’s consider a simple example. Say you conduct a survey to investigate the relationship between the attitude towards a city and the duration of residency. The “Attitude” variable can take values between 1 (very unfavorable) and 12 (very favorable), and the “duration of residency” is measured in years. Let’s further assume for this example that the attitude measurement represents an interval scale (although it is usually not realistic to assume that the scale points on an itemized rating scale have the same distance). To keep it simple, let’s further assume that you only asked 12 people. We can create a short data set like this:</p>
<div class="sourceCode" id="cb456"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb456-1"><a href="regression.html#cb456-1"></a><span class="kw">library</span>(psych)</span>
<span id="cb456-2"><a href="regression.html#cb456-2"></a>attitude &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">6</span>, <span class="dv">9</span>, <span class="dv">8</span>, <span class="dv">3</span>, <span class="dv">10</span>, <span class="dv">4</span>, <span class="dv">5</span>, <span class="dv">2</span>, <span class="dv">11</span>, <span class="dv">9</span>, <span class="dv">10</span>, <span class="dv">2</span>)</span>
<span id="cb456-3"><a href="regression.html#cb456-3"></a>duration &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">10</span>, <span class="dv">12</span>, <span class="dv">12</span>, <span class="dv">4</span>, <span class="dv">12</span>, <span class="dv">6</span>, <span class="dv">8</span>, <span class="dv">2</span>, <span class="dv">18</span>, <span class="dv">9</span>, <span class="dv">17</span>, </span>
<span id="cb456-4"><a href="regression.html#cb456-4"></a>    <span class="dv">2</span>)</span>
<span id="cb456-5"><a href="regression.html#cb456-5"></a>att_data &lt;-<span class="st"> </span><span class="kw">data.frame</span>(attitude, duration)</span>
<span id="cb456-6"><a href="regression.html#cb456-6"></a>att_data &lt;-<span class="st"> </span>att_data[<span class="kw">order</span>(<span class="op">-</span>attitude), ]</span>
<span id="cb456-7"><a href="regression.html#cb456-7"></a>att_data<span class="op">$</span>respodentID &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">12</span>)</span>
<span id="cb456-8"><a href="regression.html#cb456-8"></a><span class="kw">str</span>(att_data)</span></code></pre></div>
<pre><code>## &#39;data.frame&#39;:    12 obs. of  3 variables:
##  $ attitude   : num  11 10 10 9 9 8 6 5 4 3 ...
##  $ duration   : num  18 12 17 12 9 12 10 8 6 4 ...
##  $ respodentID: int  1 2 3 4 5 6 7 8 9 10 ...</code></pre>
<div class="sourceCode" id="cb458"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb458-1"><a href="regression.html#cb458-1"></a>psych<span class="op">::</span><span class="kw">describe</span>(att_data[, <span class="kw">c</span>(<span class="st">&quot;attitude&quot;</span>, <span class="st">&quot;duration&quot;</span>)])</span></code></pre></div>
<pre><code>##          vars  n mean   sd median trimmed  mad min max range  skew kurtosis
## attitude    1 12 6.58 3.32    7.0     6.6 4.45   2  11     9 -0.14    -1.74
## duration    2 12 9.33 5.26    9.5     9.2 4.45   2  18    16  0.10    -1.27
##            se
## attitude 0.96
## duration 1.52</code></pre>
<div class="sourceCode" id="cb460"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb460-1"><a href="regression.html#cb460-1"></a>att_data</span></code></pre></div>
<pre><code>##    attitude duration respodentID
## 9        11       18           1
## 5        10       12           2
## 11       10       17           3
## 2         9       12           4
## 10        9        9           5
## 3         8       12           6
## 1         6       10           7
## 7         5        8           8
## 6         4        6           9
## 4         3        4          10
## 8         2        2          11
## 12        2        2          12</code></pre>
<p>Let’s look at the data first. The following graph shows the individual data points for the “duration of residency”" variable, where the y-axis shows the duration of residency in years and the x-axis shows the respondent ID. The blue horizontal line represents the mean of the variable (9.33) and the vertical lines show the distance of the individual data points from the mean.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-4"></span>
<img src="10-Regression_files/figure-html/unnamed-chunk-4-1.png" alt="Scores for duration of residency variable" width="672" />
<p class="caption">
Figure 1.3: Scores for duration of residency variable
</p>
</div>
<p>You can see that there are some respondents that have been living in the city longer than average and some respondents that have been living in the city shorter than average. Let’s do the same for the second variable (“Attitude”). Again, the y-axis shows the observed scores for this variable and the x-axis shows the respondent ID.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-5"></span>
<img src="10-Regression_files/figure-html/unnamed-chunk-5-1.png" alt="Scores for attitude variable" width="672" />
<p class="caption">
Figure 1.4: Scores for attitude variable
</p>
</div>
<p>Again, we can see that some respondents have an above average attitude towards the city (more favorable) and some respondents have a below average attitude towards the city. Let’s combine both variables in one graph now to see if there is some co-movement:</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-6"></span>
<img src="10-Regression_files/figure-html/unnamed-chunk-6-1.png" alt="Scores for attitude and duration of residency variables" width="672" />
<p class="caption">
Figure 5.1: Scores for attitude and duration of residency variables
</p>
</div>
<p>We can see that there is indeed some co-movement here. The variables <b>covary</b> because respondents who have an above (below) average attitude towards the city also appear to have been living in the city for an above (below) average amount of time and vice versa. Correlation helps us to quantify this relationship. Before you proceed to compute the correlation coefficient, you should first look at the data. We usually use a scatterplot to visualize the relationship between two metric variables:</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-7"></span>
<img src="10-Regression_files/figure-html/unnamed-chunk-7-1.png" alt="Scatterplot for durationand attitute variables" width="672" />
<p class="caption">
Figure 1.5: Scatterplot for durationand attitute variables
</p>
</div>
<p>How can we compute the correlation coefficient? Remember that the variance measures the average deviation from the mean of a variable:</p>
<p><span class="math display" id="eq:variance">\[\begin{equation} 
\begin{split}
s_x^2&amp;=\frac{\sum_{i=1}^{N} (X_i-\overline{X})^2}{N-1} \\
     &amp;= \frac{\sum_{i=1}^{N} (X_i-\overline{X})*(X_i-\overline{X})}{N-1}
\end{split}
\tag{7.1}
\end{equation}\]</span></p>
<p>When we consider two variables, we multiply the deviation for one variable by the respective deviation for the second variable:</p>
<p style="text-align:center;">
<span class="math inline">\((X_i-\overline{X})*(Y_i-\overline{Y})\)</span>
</p>
<p>This is called the cross-product deviation. Then we sum the cross-product deviations:</p>
<p style="text-align:center;">
<span class="math inline">\(\sum_{i=1}^{N}(X_i-\overline{X})*(Y_i-\overline{Y})\)</span>
</p>
<p>… and compute the average of the sum of all cross-product deviations to get the <b>covariance</b>:</p>
<p><span class="math display" id="eq:covariance">\[\begin{equation} 
Cov(x, y) =\frac{\sum_{i=1}^{N}(X_i-\overline{X})*(Y_i-\overline{Y})}{N-1}
\tag{7.2}
\end{equation}\]</span></p>
<p>You can easily compute the covariance manually as follows</p>
<div class="sourceCode" id="cb462"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb462-1"><a href="regression.html#cb462-1"></a>x &lt;-<span class="st"> </span>att_data<span class="op">$</span>duration</span>
<span id="cb462-2"><a href="regression.html#cb462-2"></a>x_bar &lt;-<span class="st"> </span><span class="kw">mean</span>(att_data<span class="op">$</span>duration)</span>
<span id="cb462-3"><a href="regression.html#cb462-3"></a>y &lt;-<span class="st"> </span>att_data<span class="op">$</span>attitude</span>
<span id="cb462-4"><a href="regression.html#cb462-4"></a>y_bar &lt;-<span class="st"> </span><span class="kw">mean</span>(att_data<span class="op">$</span>attitude)</span>
<span id="cb462-5"><a href="regression.html#cb462-5"></a>N &lt;-<span class="st"> </span><span class="kw">nrow</span>(att_data)</span>
<span id="cb462-6"><a href="regression.html#cb462-6"></a>cov &lt;-<span class="st"> </span>(<span class="kw">sum</span>((x <span class="op">-</span><span class="st"> </span>x_bar) <span class="op">*</span><span class="st"> </span>(y <span class="op">-</span><span class="st"> </span>y_bar)))<span class="op">/</span>(N <span class="op">-</span><span class="st"> </span><span class="dv">1</span>)</span>
<span id="cb462-7"><a href="regression.html#cb462-7"></a>cov</span></code></pre></div>
<pre><code>## [1] 16.333333</code></pre>
<p>Or you simply use the built-in <code>cov()</code> function:</p>
<div class="sourceCode" id="cb464"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb464-1"><a href="regression.html#cb464-1"></a><span class="kw">cov</span>(att_data<span class="op">$</span>duration, att_data<span class="op">$</span>attitude)  <span class="co"># apply the cov function </span></span></code></pre></div>
<pre><code>## [1] 16.333333</code></pre>
<p>A positive covariance indicates that as one variable deviates from the mean, the other variable deviates in the same direction. A negative covariance indicates that as one variable deviates from the mean (e.g., increases), the other variable deviates in the opposite direction (e.g., decreases).</p>
<p>However, the size of the covariance depends on the scale of measurement. Larger scale units will lead to larger covariance. To overcome the problem of dependence on measurement scale, we need to convert the covariance to a standard set of units through standardization by dividing the covariance by the standard deviation (similar to how we compute z-scores).</p>
<p>With two variables, there are two standard deviations. We simply multiply the two standard deviations. We then divide the covariance by the product of the two standard deviations to get the standardized covariance, which is known as a correlation coefficient r:</p>
<p><span class="math display" id="eq:corcoeff">\[\begin{equation} 
r=\frac{Cov_{xy}}{s_x*s_y}
\tag{7.3}
\end{equation}\]</span></p>
<p>This is known as the product moment correlation (r) and it is straight-forward to compute:</p>
<div class="sourceCode" id="cb466"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb466-1"><a href="regression.html#cb466-1"></a>x_sd &lt;-<span class="st"> </span><span class="kw">sd</span>(att_data<span class="op">$</span>duration)</span>
<span id="cb466-2"><a href="regression.html#cb466-2"></a>y_sd &lt;-<span class="st"> </span><span class="kw">sd</span>(att_data<span class="op">$</span>attitude)</span>
<span id="cb466-3"><a href="regression.html#cb466-3"></a>r &lt;-<span class="st"> </span>cov<span class="op">/</span>(x_sd <span class="op">*</span><span class="st"> </span>y_sd)</span>
<span id="cb466-4"><a href="regression.html#cb466-4"></a>r</span></code></pre></div>
<pre><code>## [1] 0.93607782</code></pre>
<p>Or you could just use the <code>cor()</code> function:</p>
<div class="sourceCode" id="cb468"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb468-1"><a href="regression.html#cb468-1"></a><span class="kw">cor</span>(att_data[, <span class="kw">c</span>(<span class="st">&quot;attitude&quot;</span>, <span class="st">&quot;duration&quot;</span>)], <span class="dt">method =</span> <span class="st">&quot;pearson&quot;</span>, </span>
<span id="cb468-2"><a href="regression.html#cb468-2"></a>    <span class="dt">use =</span> <span class="st">&quot;complete&quot;</span>)</span></code></pre></div>
<pre><code>##            attitude   duration
## attitude 1.00000000 0.93607782
## duration 0.93607782 1.00000000</code></pre>
<p>The properties of the correlation coefficient (‘r’) are:</p>
<ul>
<li>ranges from -1 to + 1</li>
<li>+1 indicates perfect linear relationship</li>
<li>-1 indicates perfect negative relationship</li>
<li>0 indicates no linear relationship</li>
<li>± .1 represents small effect</li>
<li>± .3 represents medium effect</li>
<li>± .5 represents large effect</li>
</ul>
</div>
<div id="significance-testing" class="section level3">
<h3><span class="header-section-number">7.1.2</span> Significance testing</h3>
<p>How can we determine if our two variables are significantly related? To test this, we denote the population moment correlation <em>ρ</em>. Then we test the null of no relationship between variables:</p>
<p><span class="math display">\[H_0:\rho=0\]</span>
<span class="math display">\[H_1:\rho\ne0\]</span></p>
<p>The test statistic is:</p>
<p><span class="math display" id="eq:cortest">\[\begin{equation} 
t=\frac{r*\sqrt{N-2}}{\sqrt{1-r^2}}
\tag{7.4}
\end{equation}\]</span></p>
<p>It has a t distribution with n - 2 degrees of freedom. Then, we follow the usual procedure of calculating the test statistic and comparing the test statistic to the critical value of the underlying probability distribution. If the calculated test statistic is larger than the critical value, the null hypothesis of no relationship between X and Y is rejected.</p>
<div class="sourceCode" id="cb470"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb470-1"><a href="regression.html#cb470-1"></a>t_calc &lt;-<span class="st"> </span>r <span class="op">*</span><span class="st"> </span><span class="kw">sqrt</span>(N <span class="op">-</span><span class="st"> </span><span class="dv">2</span>)<span class="op">/</span><span class="kw">sqrt</span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>r<span class="op">^</span><span class="dv">2</span>)  <span class="co">#calculated test statistic</span></span>
<span id="cb470-2"><a href="regression.html#cb470-2"></a>t_calc</span></code></pre></div>
<pre><code>## [1] 8.4144314</code></pre>
<div class="sourceCode" id="cb472"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb472-1"><a href="regression.html#cb472-1"></a>df &lt;-<span class="st"> </span>(N <span class="op">-</span><span class="st"> </span><span class="dv">2</span>)  <span class="co">#degrees of freedom</span></span>
<span id="cb472-2"><a href="regression.html#cb472-2"></a>t_crit &lt;-<span class="st"> </span><span class="kw">qt</span>(<span class="fl">0.975</span>, df)  <span class="co">#critical value</span></span>
<span id="cb472-3"><a href="regression.html#cb472-3"></a>t_crit</span></code></pre></div>
<pre><code>## [1] 2.2281389</code></pre>
<div class="sourceCode" id="cb474"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb474-1"><a href="regression.html#cb474-1"></a><span class="kw">pt</span>(<span class="dt">q =</span> t_calc, <span class="dt">df =</span> df, <span class="dt">lower.tail =</span> F) <span class="op">*</span><span class="st"> </span><span class="dv">2</span>  <span class="co">#p-value </span></span></code></pre></div>
<pre><code>## [1] 0.0000075451612</code></pre>
<p>Or you can simply use the <code>cor.test()</code> function, which also produces the 95% confidence interval:</p>
<div class="sourceCode" id="cb476"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb476-1"><a href="regression.html#cb476-1"></a><span class="kw">cor.test</span>(att_data<span class="op">$</span>attitude, att_data<span class="op">$</span>duration, <span class="dt">alternative =</span> <span class="st">&quot;two.sided&quot;</span>, </span>
<span id="cb476-2"><a href="regression.html#cb476-2"></a>    <span class="dt">method =</span> <span class="st">&quot;pearson&quot;</span>, <span class="dt">conf.level =</span> <span class="fl">0.95</span>)</span></code></pre></div>
<pre><code>## 
##  Pearson&#39;s product-moment correlation
## 
## data:  att_data$attitude and att_data$duration
## t = 8.41443, df = 10, p-value = 0.0000075452
## alternative hypothesis: true correlation is not equal to 0
## 95 percent confidence interval:
##  0.78260411 0.98228152
## sample estimates:
##        cor 
## 0.93607782</code></pre>
<p>To determine the linear relationship between variables, the data only needs to be measured using interval scales. If you want to test the significance of the association, the sampling distribution needs to be normally distributed (we usually assume this when our data are normally distributed or when N is large). If parametric assumptions are violated, you should use non-parametric tests:</p>
<ul>
<li>Spearman’s correlation coefficient: requires ordinal data and ranks the data before applying Pearson’s equation.</li>
<li>Kendall’s tau: use when N is small or the number of tied ranks is large.</li>
</ul>
<div class="sourceCode" id="cb478"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb478-1"><a href="regression.html#cb478-1"></a><span class="kw">cor.test</span>(att_data<span class="op">$</span>attitude, att_data<span class="op">$</span>duration, <span class="dt">alternative =</span> <span class="st">&quot;two.sided&quot;</span>, </span>
<span id="cb478-2"><a href="regression.html#cb478-2"></a>    <span class="dt">method =</span> <span class="st">&quot;spearman&quot;</span>, <span class="dt">conf.level =</span> <span class="fl">0.95</span>)</span></code></pre></div>
<pre><code>## 
##  Spearman&#39;s rank correlation rho
## 
## data:  att_data$attitude and att_data$duration
## S = 14.1969, p-value = 0.0000021833
## alternative hypothesis: true rho is not equal to 0
## sample estimates:
##        rho 
## 0.95036059</code></pre>
<div class="sourceCode" id="cb480"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb480-1"><a href="regression.html#cb480-1"></a><span class="kw">cor.test</span>(att_data<span class="op">$</span>attitude, att_data<span class="op">$</span>duration, <span class="dt">alternative =</span> <span class="st">&quot;two.sided&quot;</span>, </span>
<span id="cb480-2"><a href="regression.html#cb480-2"></a>    <span class="dt">method =</span> <span class="st">&quot;kendall&quot;</span>, <span class="dt">conf.level =</span> <span class="fl">0.95</span>)</span></code></pre></div>
<pre><code>## 
##  Kendall&#39;s rank correlation tau
## 
## data:  att_data$attitude and att_data$duration
## z = 3.90948, p-value = 0.000092496
## alternative hypothesis: true tau is not equal to 0
## sample estimates:
##        tau 
## 0.89602867</code></pre>
<p>Report the results:</p>
<p>A Pearson product-moment correlation coefficient was computed to assess the relationship between the duration of residence in a city and the attitude toward the city. There was a positive correlation between the two variables, r = 0.936, n = 12, p &lt; 0.05. A scatterplot summarizes the results (Figure XY).</p>
<p><strong>A note on the interpretation of correlation coefficients:</strong></p>
<p>As we have already seen in chapter 1, correlation coefficients give no indication of the direction of causality. In our example, we can conclude that the attitude toward the city is more positive as the years of residence increases. However, we cannot say that the years of residence cause the attitudes to be more positive. There are two main reasons for caution when interpreting correlations:</p>
<ul>
<li>Third-variable problem: there may be other unobserved factors that affect both the ‘attitude towards a city’ and the ‘duration of residency’ variables</li>
<li>Direction of causality: Correlations say nothing about which variable causes the other to change (reverse causality: attitudes may just as well cause the years of residence variable).</li>
</ul>
</div>
</div>
<div id="regression-1" class="section level2">
<h2><span class="header-section-number">7.2</span> Regression</h2>
<br>
<div align="center">
<iframe width="560" height="315" src="https://www.youtube.com/embed/rtvDHLuXUEI" frameborder="0" allowfullscreen>
</iframe>
</div>
<p><br></p>
<p>Correlations measure relationships between variables (i.e., how much two variables covary). Using regression analysis we can predict the outcome of a dependent variable (Y) from one or more independent variables (X). For example, we could be interested in how many products will we will sell if we increase the advertising expenditures by 1000 Euros? In regression analysis, we fit a model to our data and use it to predict the values of the dependent variable from one predictor variable (bivariate regression) or several predictor variables (multiple regression). The following table shows a comparison of correlation and regression analysis:</p>
<p><br></p>
<table>
<colgroup>
<col width="20%" />
<col width="40%" />
<col width="40%" />
</colgroup>
<thead>
<tr class="header">
<th> </th>
<th>Correlation</th>
<th>Regression</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Estimated coefficient</td>
<td>Coefficient of correlation (bounded between -1 and +1)</td>
<td>Regression coefficient (not bounded a priori)</td>
</tr>
<tr class="even">
<td>Interpretation</td>
<td>Linear association between two variables; Association is bidirectional</td>
<td>(Linear) relation between one or more independent variables and dependent variable; Relation is directional</td>
</tr>
<tr class="odd">
<td>Role of theory</td>
<td>Theory neither required nor testable</td>
<td>Theory required and testable</td>
</tr>
</tbody>
</table>
<p><br></p>
<div id="simple-linear-regression" class="section level3">
<h3><span class="header-section-number">7.2.1</span> Simple linear regression</h3>
<p>In simple linear regression, we assess the relationship between one dependent (regressand) and one independent (regressor) variable. The goal is to fit a line through a scatterplot of observations in order to find the line that best describes the data (scatterplot).</p>
<p>Suppose you are a marketing research analyst at a music label and your task is to suggest, on the basis of historical data, a marketing plan for the next year that will maximize product sales. The data set that is available to you includes information on the sales of music downloads (thousands of units), advertising expenditures (in Euros), the number of radio plays an artist received per week (airplay), the number of previous releases of an artist (starpower), repertoire origin (country; 0 = local, 1 = international), and genre (1 = rock, 2 = pop, 3 = electronic). Let’s load and inspect the data first:</p>
<div class="sourceCode" id="cb482"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb482-1"><a href="regression.html#cb482-1"></a>regression &lt;-<span class="st"> </span><span class="kw">read.table</span>(<span class="st">&quot;https://raw.githubusercontent.com/IMSMWU/Teaching/master/MRDA2017/music_sales_regression.dat&quot;</span>, </span>
<span id="cb482-2"><a href="regression.html#cb482-2"></a>    <span class="dt">sep =</span> <span class="st">&quot;</span><span class="ch">\t</span><span class="st">&quot;</span>, <span class="dt">header =</span> <span class="ot">TRUE</span>)  <span class="co">#read in data</span></span>
<span id="cb482-3"><a href="regression.html#cb482-3"></a>regression<span class="op">$</span>country &lt;-<span class="st"> </span><span class="kw">factor</span>(regression<span class="op">$</span>country, <span class="dt">levels =</span> <span class="kw">c</span>(<span class="dv">0</span><span class="op">:</span><span class="dv">1</span>), </span>
<span id="cb482-4"><a href="regression.html#cb482-4"></a>    <span class="dt">labels =</span> <span class="kw">c</span>(<span class="st">&quot;local&quot;</span>, <span class="st">&quot;international&quot;</span>))  <span class="co">#convert grouping variable to factor</span></span>
<span id="cb482-5"><a href="regression.html#cb482-5"></a>regression<span class="op">$</span>genre &lt;-<span class="st"> </span><span class="kw">factor</span>(regression<span class="op">$</span>genre, <span class="dt">levels =</span> <span class="kw">c</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">3</span>), </span>
<span id="cb482-6"><a href="regression.html#cb482-6"></a>    <span class="dt">labels =</span> <span class="kw">c</span>(<span class="st">&quot;rock&quot;</span>, <span class="st">&quot;pop&quot;</span>, <span class="st">&quot;electronic&quot;</span>))  <span class="co">#convert grouping variable to factor</span></span>
<span id="cb482-7"><a href="regression.html#cb482-7"></a><span class="kw">head</span>(regression)</span></code></pre></div>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["sales"],"name":[1],"type":["int"],"align":["right"]},{"label":["adspend"],"name":[2],"type":["dbl"],"align":["right"]},{"label":["airplay"],"name":[3],"type":["int"],"align":["right"]},{"label":["starpower"],"name":[4],"type":["int"],"align":["right"]},{"label":["genre"],"name":[5],"type":["fct"],"align":["left"]},{"label":["country"],"name":[6],"type":["fct"],"align":["left"]}],"data":[{"1":"330","2":"10.256","3":"43","4":"10","5":"electronic","6":"international"},{"1":"300","2":"174.093","3":"40","4":"7","5":"electronic","6":"international"},{"1":"250","2":"1000.000","3":"5","4":"7","5":"pop","6":"international"},{"1":"120","2":"75.896","3":"34","4":"6","5":"rock","6":"local"},{"1":"290","2":"1351.254","3":"37","4":"9","5":"electronic","6":"local"},{"1":"60","2":"202.705","3":"13","4":"8","5":"rock","6":"local"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<div class="sourceCode" id="cb483"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb483-1"><a href="regression.html#cb483-1"></a>psych<span class="op">::</span><span class="kw">describe</span>(regression)  <span class="co">#descriptive statistics using psych</span></span></code></pre></div>
<pre><code>##           vars   n   mean     sd median trimmed    mad  min     max   range
## sales        1 200 193.20  80.70 200.00  192.69  88.96 10.0  360.00  350.00
## adspend      2 200 614.41 485.66 531.92  560.81 489.09  9.1 2271.86 2262.76
## airplay      3 200  27.50  12.27  28.00   27.46  11.86  0.0   63.00   63.00
## starpower    4 200   6.77   1.40   7.00    6.88   1.48  1.0   10.00    9.00
## genre*       5 200   2.40   0.79   3.00    2.50   0.00  1.0    3.00    2.00
## country*     6 200   1.17   0.38   1.00    1.09   0.00  1.0    2.00    1.00
##            skew kurtosis    se
## sales      0.04    -0.72  5.71
## adspend    0.84     0.17 34.34
## airplay    0.06    -0.09  0.87
## starpower -1.27     3.56  0.10
## genre*    -0.83    -0.91  0.06
## country*   1.74     1.05  0.03</code></pre>
<p>As stated above, regression analysis may be used to relate a quantitative response (“dependent variable”) to one or more predictor variables (“independent variables”). In a simple linear regression, we have one dependent and one independent variable and we regress the dependent variable on the independent variable.</p>
<p>Here are a few important questions that we might seek to address based on the data:</p>
<ul>
<li>Is there a relationship between advertising budget and sales?</li>
<li>How strong is the relationship between advertising budget and sales?</li>
<li>Which other variables contribute to sales?</li>
<li>How accurately can we estimate the effect of each variable on sales?</li>
<li>How accurately can we predict future sales?</li>
<li>Is the relationship linear?</li>
<li>Is there synergy among the advertising activities?</li>
</ul>
<p>We may use linear regression to answer these questions. We will see later that the interpretation of the results strongly depends on the goal of the analysis - whether you would like to simply predict an outcome variable or you would like to explain the causal effect of the independent variable on the dependent variable (see chapter 1). Let’s start with the first question and investigate the relationship between advertising and sales.</p>
<div id="estimating-the-coefficients" class="section level4">
<h4><span class="header-section-number">7.2.1.1</span> Estimating the coefficients</h4>
<p>A simple linear regression model only has one predictor and can be written as:</p>
<p><span class="math display" id="eq:regequ">\[\begin{equation} 
Y=\beta_0+\beta_1X+\epsilon
\tag{7.5}
\end{equation}\]</span></p>
<p>In our specific context, let’s consider only the influence of advertising on sales for now:</p>
<p><span class="math display" id="eq:regequadv">\[\begin{equation} 
Sales=\beta_0+\beta_1*adspend+\epsilon
\tag{7.6}
\end{equation}\]</span></p>
<p>The word “adspend” represents data on advertising expenditures that we have observed and β<sub>1</sub> (the “slope”“) represents the unknown relationship between advertising expenditures and sales. It tells you by how much sales will increase for an additional Euro spent on advertising. β<sub>0</sub> (the”intercept") is the number of sales we would expect if no money is spent on advertising. Together, β<sub>0</sub> and β<sub>1</sub> represent the model coefficients or <em>parameters</em>. The error term (ε) captures everything that we miss by using our model, including, (1) misspecifications (the true relationship might not be linear), (2) omitted variables (other variables might drive sales), and (3) measurement error (our measurement of the variables might be imperfect).</p>
<p>Once we have used our training data to produce estimates for the model coefficients, we can predict future sales on the basis of a particular value of advertising expenditures by computing:</p>
<p><span class="math display" id="eq:predreg">\[\begin{equation} 
\hat{Sales}=\hat{\beta_0}+\hat{\beta_1}*adspend
\tag{7.7}
\end{equation}\]</span></p>
<p>We use the hat symbol, <sup>^</sup>, to denote the estimated value for an unknown parameter or coefficient, or to denote the predicted value of the response (sales). In practice, β<sub>0</sub> and β<sub>1</sub> are unknown and must be estimated from the data to make predictions. In the case of our advertising example, the data set consists of the advertising budget and product sales of 200 music songs (n = 200). Our goal is to obtain coefficient estimates such that the linear model fits the available data well. In other words, we fit a line through the scatterplot of observations and try to find the line that best describes the data. The following graph shows the scatterplot for our data, where the black line shows the regression line. The grey vertical lines shows the difference between the predicted values (the regression line) and the observed values. This difference is referred to as the residuals (“e”).</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-17"></span>
<img src="10-Regression_files/figure-html/unnamed-chunk-17-1.png" alt="Ordinary least squares (OLS)" width="672" />
<p class="caption">
Figure 1.14: Ordinary least squares (OLS)
</p>
</div>
<p>The estimation of the regression function is based on the idea of the method of least squares (OLS = ordinary least squares). The first step is to calculate the residuals by subtracting the observed values from the predicted values.</p>
<p style="text-align:center;">
<span class="math inline">\(e_i = Y_i-(\beta_0+\beta_1X_i)\)</span>
</p>
<p>This difference is then minimized by minimizing the sum of the squared residuals:</p>
<p><span class="math display" id="eq:rss">\[\begin{equation} 
\sum_{i=1}^{N} e_i^2= \sum_{i=1}^{N} [Y_i-(\beta_0+\beta_1X_i)]^2\rightarrow min!
\tag{7.8}
\end{equation}\]</span></p>
<p>e<sub>i</sub>: Residuals (i = 1,2,…,N)<br>
Y<sub>i</sub>: Values of the dependent variable (i = 1,2,…,N) <br>
β<sub>0</sub>: Intercept<br>
β<sub>1</sub>: Regression coefficient / slope parameters<br>
X<sub>ni</sub>: Values of the nth independent variables and the i<em>th</em> observation<br>
N: Number of observations<br></p>
<p>This is also referred to as the <b>residual sum of squares (RSS)</b>, which you may still remember from the previous chapter on ANOVA. Now we need to choose the values for β<sub>0</sub> and β<sub>1</sub> that minimize RSS. So how can we derive these values for the regression coefficient? The equation for β<sub>1</sub> is given by:</p>
<p><span class="math display" id="eq:slope">\[\begin{equation} 
\hat{\beta_1}=\frac{COV_{XY}}{s_x^2}
\tag{7.9}
\end{equation}\]</span></p>
<p>The exact mathematical derivation of this formula is beyond the scope of this script, but the intuition is to calculate the first derivative of the squared residuals with respect to β<sub>1</sub> and set it to zero, thereby finding the β<sub>1</sub> that minimizes the term. Using the above formula, you can easily compute β<sub>1</sub> using the following code:</p>
<div class="sourceCode" id="cb485"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb485-1"><a href="regression.html#cb485-1"></a>cov_y_x &lt;-<span class="st"> </span><span class="kw">cov</span>(regression<span class="op">$</span>adspend, regression<span class="op">$</span>sales)</span>
<span id="cb485-2"><a href="regression.html#cb485-2"></a>cov_y_x</span></code></pre></div>
<pre><code>## [1] 22672.016</code></pre>
<div class="sourceCode" id="cb487"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb487-1"><a href="regression.html#cb487-1"></a>var_x &lt;-<span class="st"> </span><span class="kw">var</span>(regression<span class="op">$</span>adspend)</span>
<span id="cb487-2"><a href="regression.html#cb487-2"></a>var_x</span></code></pre></div>
<pre><code>## [1] 235860.98</code></pre>
<div class="sourceCode" id="cb489"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb489-1"><a href="regression.html#cb489-1"></a>beta_<span class="dv">1</span> &lt;-<span class="st"> </span>cov_y_x<span class="op">/</span>var_x</span>
<span id="cb489-2"><a href="regression.html#cb489-2"></a>beta_<span class="dv">1</span></span></code></pre></div>
<pre><code>## [1] 0.096124486</code></pre>
<p>The interpretation of β<sub>1</sub> is as follows:</p>
<p>For every extra Euro spent on advertising, sales can be expected to increase by 0.096 units. Or, in other words, if we increase our marketing budget by 1,000 Euros, sales can be expected to increase by 96 units.</p>
<p>Using the estimated coefficient for β<sub>1</sub>, it is easy to compute β<sub>0</sub> (the intercept) as follows:</p>
<p><span class="math display" id="eq:intercept">\[\begin{equation} 
\hat{\beta_0}=\overline{Y}-\hat{\beta_1}\overline{X}
\tag{7.10}
\end{equation}\]</span></p>
<p>The R code for this is:</p>
<div class="sourceCode" id="cb491"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb491-1"><a href="regression.html#cb491-1"></a>beta_<span class="dv">0</span> &lt;-<span class="st"> </span><span class="kw">mean</span>(regression<span class="op">$</span>sales) <span class="op">-</span><span class="st"> </span>beta_<span class="dv">1</span> <span class="op">*</span><span class="st"> </span><span class="kw">mean</span>(regression<span class="op">$</span>adspend)</span>
<span id="cb491-2"><a href="regression.html#cb491-2"></a>beta_<span class="dv">0</span></span></code></pre></div>
<pre><code>## [1] 134.13994</code></pre>
<p>The interpretation of β<sub>0</sub> is as follows:</p>
<p>If we spend no money on advertising, we would expect to sell 134.14 units.</p>
<p>You may also verify this based on a scatterplot of the data. The following plot shows the scatterplot including the regression line, which is estimated using OLS.</p>
<div class="sourceCode" id="cb493"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb493-1"><a href="regression.html#cb493-1"></a><span class="kw">ggplot</span>(regression, <span class="dt">mapping =</span> <span class="kw">aes</span>(adspend, sales)) <span class="op">+</span><span class="st"> </span></span>
<span id="cb493-2"><a href="regression.html#cb493-2"></a><span class="st">    </span><span class="kw">geom_point</span>(<span class="dt">shape =</span> <span class="dv">1</span>) <span class="op">+</span><span class="st"> </span><span class="kw">geom_smooth</span>(<span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>, </span>
<span id="cb493-3"><a href="regression.html#cb493-3"></a>    <span class="dt">fill =</span> <span class="st">&quot;blue&quot;</span>, <span class="dt">alpha =</span> <span class="fl">0.1</span>) <span class="op">+</span><span class="st"> </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&quot;Advertising expenditures (EUR)&quot;</span>, </span>
<span id="cb493-4"><a href="regression.html#cb493-4"></a>    <span class="dt">y =</span> <span class="st">&quot;Number of sales&quot;</span>) <span class="op">+</span><span class="st"> </span><span class="kw">theme_bw</span>()</span></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-20"></span>
<img src="10-Regression_files/figure-html/unnamed-chunk-20-1.png" alt="Scatterplot" width="672" />
<p class="caption">
Figure 1.17: Scatterplot
</p>
</div>
<p>You can see that the regression line intersects with the y-axis at 134.14, which corresponds to the expected sales level when advertising expenditure (on the x-axis) is zero (i.e., the intercept β<sub>0</sub>). The slope coefficient (β<sub>1</sub>) tells you by how much sales (on the y-axis) would increase if advertising expenditures (on the x-axis) are increased by one unit.</p>
</div>
<div id="significance-testing-1" class="section level4">
<h4><span class="header-section-number">7.2.1.2</span> Significance testing</h4>
<p>In a next step, we assess if the effect of advertising on sales is statistically significant. This means that we test the null hypothesis H<sub>0</sub>: “There is no relationship between advertising and sales” versus the alternative hypothesis H<sub>1</sub>: “The is some relationship between advertising and sales”. Or, to state this formally:</p>
<p><span class="math display">\[H_0:\beta_1=0\]</span>
<span class="math display">\[H_1:\beta_1\ne0\]</span></p>
<p>How can we test if the effect is statistically significant? Recall the generalized equation to derive a test statistic:</p>
<p><span class="math display" id="eq:teststatgeneral">\[\begin{equation} 
test\ statistic = \frac{effect}{error}
\tag{7.11}
\end{equation}\]</span></p>
<p>The effect is given by the β<sub>1</sub> coefficient in this case. To compute the test statistic, we need to come up with a measure of uncertainty around this estimate (the error). This is because we use information from a sample to estimate the least squares line to make inferences regarding the regression line in the entire population. Since we only have access to one sample, the regression line will be slightly different every time we take a different sample from the population. This is sampling variation and it is perfectly normal! It just means that we need to take into account the uncertainty around the estimate, which is achieved by the standard error. Thus, the test statistic for our hypothesis is given by:</p>
<p><span class="math display" id="eq:teststatreg">\[\begin{equation} 
t = \frac{\hat{\beta_1}}{SE(\hat{\beta_1})}
\tag{7.12}
\end{equation}\]</span></p>
<p>After calculating the test statistic, we compare its value to the values that we would expect to find if there was no effect based on the t-distribution. In a regression context, the degrees of freedom are given by <code>N - p - 1</code> where N is the sample size and p is the number of predictors. In our case, we have 200 observations and one predictor. Thus, the degrees of freedom is 200 - 1 - 1 = 198. In the regression output below, R provides the exact probability of observing a t value of this magnitude (or larger) if the null hypothesis was true. This probability - as we already saw in chapter 6 - is the p-value. A small p-value indicates that it is unlikely to observe such a substantial association between the predictor and the outcome variable due to chance in the absence of any real association between the predictor and the outcome.</p>
<p>To estimate the regression model in R, you can use the <code>lm()</code> function. Within the function, you first specify the dependent variable (“sales”) and independent variable (“adspend”) separated by a <code>~</code> (tilde). As mentioned previously, this is known as <em>formula notation</em> in R. The <code>data = regression</code> argument specifies that the variables come from the data frame named “regression”. Strictly speaking, you use the <code>lm()</code> function to create an object called “simple_regression,” which holds the regression output. You can then view the results using the <code>summary()</code> function:</p>
<div class="sourceCode" id="cb494"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb494-1"><a href="regression.html#cb494-1"></a>simple_regression &lt;-<span class="st"> </span><span class="kw">lm</span>(sales <span class="op">~</span><span class="st"> </span>adspend, <span class="dt">data =</span> regression)  <span class="co">#estimate linear model</span></span>
<span id="cb494-2"><a href="regression.html#cb494-2"></a><span class="kw">summary</span>(simple_regression)  <span class="co">#summary of results</span></span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = sales ~ adspend, data = regression)
## 
## Residuals:
##       Min        1Q    Median        3Q       Max 
## -152.9493  -43.7961   -0.3933   37.0404  211.8658 
## 
## Coefficients:
##                Estimate  Std. Error t value              Pr(&gt;|t|)    
## (Intercept) 134.1399378   7.5365747 17.7985 &lt; 0.00000000000000022 ***
## adspend       0.0961245   0.0096324  9.9793 &lt; 0.00000000000000022 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 65.991 on 198 degrees of freedom
## Multiple R-squared:  0.33465,    Adjusted R-squared:  0.33129 
## F-statistic: 99.587 on 1 and 198 DF,  p-value: &lt; 0.000000000000000222</code></pre>
<p>Note that the estimated coefficients for β<sub>0</sub> (134.14) and β<sub>1</sub> (0.096) correspond to the results of our manual computation above. The associated t-values and p-values are given in the output. The t-values are larger than the critical t-values for the 95% confidence level, since the associated p-values are smaller than 0.05. In case of the coefficient for β<sub>1</sub>, this means that the probability of an association between the advertising and sales of the observed magnitude (or larger) is smaller than 0.05, if the value of β<sub>1</sub> was, in fact, 0. This finding leads us to reject the null hypothesis of no association between advertising and sales.</p>
<p>The coefficients associated with the respective variables represent <b>point estimates</b>. To obtain a better understanding of the range of values that the coefficients could take, it is helpful to compute <b>confidence intervals</b>. A 95% confidence interval is defined as a range of values such that with a 95% probability, the range will contain the true unknown value of the parameter. For example, for β<sub>1</sub>, the confidence interval can be computed as.</p>
<p><span class="math display" id="eq:regCI">\[\begin{equation} 
CI = \hat{\beta_1}\pm(t_{1-\frac{\alpha}{2}}*SE(\beta_1))
\tag{7.13}
\end{equation}\]</span></p>
<p>It is easy to compute confidence intervals in R using the <code>confint()</code> function. You just have to provide the name of you estimated model as an argument:</p>
<div class="sourceCode" id="cb496"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb496-1"><a href="regression.html#cb496-1"></a><span class="kw">confint</span>(simple_regression)</span></code></pre></div>
<pre><code>##                     2.5 %       97.5 %
## (Intercept) 119.277680821 149.00219480
## adspend       0.077129291   0.11511968</code></pre>
<p>For our model, the 95% confidence interval for β<sub>0</sub> is [119.28,149], and the 95% confidence interval for β<sub>1</sub> is [0.08,0.12]. Thus, we can conclude that when we do not spend any money on advertising, sales will be somewhere between 119 and 149 units on average. In addition, for each increase in advertising expenditures by one Euro, there will be an average increase in sales of between 0.08 and 0.12. If you revisit the graphic depiction of the regression model above, the uncertainty regarding the intercept and slope parameters can be seen in the confidence bounds (blue area) around the regression line.</p>
</div>
<div id="assessing-model-fit" class="section level4">
<h4><span class="header-section-number">7.2.1.3</span> Assessing model fit</h4>
<br>
<div align="center">
<iframe width="560" height="315" src="https://www.youtube.com/embed/nG4_st29Qe8" frameborder="0" allowfullscreen>
</iframe>
</div>
<p><br></p>
<p>Once we have rejected the null hypothesis in favor of the alternative hypothesis, the next step is to investigate how well the model represents (“fits”) the data. How can we assess the model fit?</p>
<ul>
<li>First, we calculate the fit of the most basic model (i.e., the mean)</li>
<li>Then, we calculate the fit of the best model (i.e., the regression model)</li>
<li>A good model should fit the data significantly better than the basic model</li>
<li>R<sup>2</sup>: Represents the percentage of the variation in the outcome that can be explained by the model</li>
<li>The F-ratio measures how much the model has improved the prediction of the outcome compared to the level of inaccuracy in the model</li>
</ul>
<p>Similar to ANOVA, the calculation of model fit statistics relies on estimating the different sum of squares values. SS<sub>T</sub> is the difference between the observed data and the mean value of Y (aka. total variation). In the absence of any other information, the mean value of Y (<span class="math inline">\(\overline{Y}\)</span>) represents the best guess on where a particular observation <span class="math inline">\(Y_{i}\)</span> at a given level of advertising will fall:</p>
<p><span class="math display" id="eq:regSST">\[\begin{equation} 
SS_T= \sum_{i=1}^{N} (Y_i-\overline{Y})^2
\tag{7.14}
\end{equation}\]</span></p>
<p>The following graph shows the total sum of squares:</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-23"></span>
<img src="10-Regression_files/figure-html/unnamed-chunk-23-1.png" alt="Total sum of squares" width="672" />
<p class="caption">
Figure 1.20: Total sum of squares
</p>
</div>
<p>Based on our linear model, the best guess about the sales level at a given level of advertising is the predicted value <span class="math inline">\(\hat{Y}_i\)</span>. The model sum of squares (SS<sub>M</sub>) therefore has the mathematical representation:</p>
<p><span class="math display" id="eq:regSSM">\[\begin{equation} 
SS_M= \sum_{i=1}^{N}  (\hat{Y}_i-\overline{Y})^2
\tag{7.15}
\end{equation}\]</span></p>
<p>The model sum of squares represents the improvement in prediction resulting from using the regression model rather than the mean of the data. The following graph shows the model sum of squares for our example:</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-24"></span>
<img src="10-Regression_files/figure-html/unnamed-chunk-24-1.png" alt="Ordinary least squares (OLS)" width="672" />
<p class="caption">
Figure 1.21: Ordinary least squares (OLS)
</p>
</div>
<p>The residual sum of squares (SS<sub>R</sub>) is the difference between the observed data points (<span class="math inline">\(Y_{i}\)</span>) and the predicted values along the regression line (<span class="math inline">\(\hat{Y}_{i}\)</span>), i.e., the variation <em>not</em> explained by the model.</p>
<p><span class="math display" id="eq:regSSR">\[\begin{equation} 
SS_R= \sum_{i=1}^{N} ({Y}_{i}-\hat{Y}_{i})^2
\tag{7.16}
\end{equation}\]</span></p>
<p>The following graph shows the residual sum of squares for our example:</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-25"></span>
<img src="10-Regression_files/figure-html/unnamed-chunk-25-1.png" alt="Ordinary least squares (OLS)" width="672" />
<p class="caption">
Figure 1.22: Ordinary least squares (OLS)
</p>
</div>
<p>Based on these statistics, we can determine have well the model fits the data as we will see next.</p>
<div id="r-squared" class="section level5 unnumbered">
<h5>R-squared</h5>
<p>The R<sup>2</sup> statistic represents the proportion of variance that is explained by the model and is computed as:</p>
<p><span class="math display" id="eq:regSSR">\[\begin{equation} 
R^2= \frac{SS_M}{SS_T}
\tag{7.16}
\end{equation}\]</span></p>
<p>It takes values between 0 (very bad fit) and 1 (very good fit). Note that when the goal of your model is to <em>predict</em> future outcomes, a “too good” model fit can pose severe challenges. The reason is that the model might fit your specific sample so well, that it will only predict well within the sample but not generalize to other samples. This is called <strong>overfitting</strong> and it shows that there is a trade-off between model fit and out-of-sample predictive ability of the model, if the goal is to predict beyond the sample. We will come back to this point later in this chapter.</p>
<p>You can get a first impression of the fit of the model by inspecting the scatter plot as can be seen in the plot below. If the observations are highly dispersed around the regression line (left plot), the fit will be lower compared to a data set where the values are less dispersed (right plot).</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-26"></span>
<img src="10-Regression_files/figure-html/unnamed-chunk-26-1.png" alt="Good vs. bad model fit" width="960" />
<p class="caption">
Figure 1.23: Good vs. bad model fit
</p>
</div>
<p>The R<sup>2</sup> statistic is reported in the regression output (see above). However, you could also extract the relevant sum of squares statistics from the regression object using the <code>anova()</code> function to compute it manually:</p>
<div class="sourceCode" id="cb498"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb498-1"><a href="regression.html#cb498-1"></a><span class="kw">anova</span>(simple_regression)  <span class="co">#anova results</span></span></code></pre></div>
<pre><code>## Analysis of Variance Table
## 
## Response: sales
##            Df Sum Sq Mean Sq F value              Pr(&gt;F)    
## adspend     1 433688  433688    99.6 &lt;0.0000000000000002 ***
## Residuals 198 862264    4355                                
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>Now we can compute R<sup>2</sup> in the same way that we have computed Eta<sup>2</sup> in the last section:</p>
<div class="sourceCode" id="cb500"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb500-1"><a href="regression.html#cb500-1"></a>r2 &lt;-<span class="st"> </span><span class="kw">anova</span>(simple_regression)<span class="op">$</span><span class="st">&quot;Sum Sq&quot;</span>[<span class="dv">1</span>]<span class="op">/</span>(<span class="kw">anova</span>(simple_regression)<span class="op">$</span><span class="st">&quot;Sum Sq&quot;</span>[<span class="dv">1</span>] <span class="op">+</span><span class="st"> </span></span>
<span id="cb500-2"><a href="regression.html#cb500-2"></a><span class="st">    </span><span class="kw">anova</span>(simple_regression)<span class="op">$</span><span class="st">&quot;Sum Sq&quot;</span>[<span class="dv">2</span>])  <span class="co">#compute R2</span></span>
<span id="cb500-3"><a href="regression.html#cb500-3"></a>r2</span></code></pre></div>
<pre><code>## [1] 0.33</code></pre>
</div>
<div id="adjusted-r-squared" class="section level5 unnumbered">
<h5>Adjusted R-squared</h5>
<p>Due to the way the R<sup>2</sup> statistic is calculated, it will never decrease if a new explanatory variable is introduced into the model. This means that every new independent variable either doesn’t change the R<sup>2</sup> or increases it, even if there is no real relationship between the new variable and the dependent variable. Hence, one could be tempted to just add as many variables as possible to increase the R<sup>2</sup> and thus obtain a “better” model. However, this actually only leads to more noise and therefore a worse model.</p>
<p>To account for this, there exists a test statistic closely related to the R<sup>2</sup>, the <strong>adjusted R<sup>2</sup></strong>. It can be calculated as follows:</p>
<p><span class="math display" id="eq:adjustedR2">\[\begin{equation} 
\overline{R^2} = 1 - (1 - R^2)\frac{n-1}{n - k - 1}
\tag{7.17}
\end{equation}\]</span></p>
<p>where <code>n</code> is the total number of observations and <code>k</code> is the total number of explanatory variables. The adjusted R<sup>2</sup> is equal to or less than the regular R<sup>2</sup> and can be negative. It will only increase if the added variable adds more explanatory power than one would expect by pure chance. Essentially, it contains a “penalty” for including unnecessary variables and therefore favors more parsimonious models. As such, it is a measure of suitability, good for comparing different models and is very useful in the model selection stage of a project. In R, the standard <code>lm()</code> function automatically also reports the adjusted R<sup>2</sup> as you can see above.</p>
</div>
<div id="f-test" class="section level5 unnumbered">
<h5>F-test</h5>
<p>Similar to the ANOVA in chapter 6, another significance test is the F-test, which tests the null hypothesis:</p>
<p><span class="math display">\[H_0:R^2=0\]</span></p>
<p><br></p>
<p>Or, to state it slightly differently:</p>
<p><span class="math display">\[H_0:\beta_1=\beta_2=\beta_3=\beta_k=0\]</span>
<br>
This means that, similar to the ANOVA, we test whether any of the included independent variables has a significant effect on the dependent variable. So far, we have only included one independent variable, but we will extend the set of predictor variables below.</p>
<p>The F-test statistic is calculated as follows:</p>
<p><span class="math display" id="eq:regSSR">\[\begin{equation} 
F=\frac{\frac{SS_M}{k}}{\frac{SS_R}{(n-k-1)}}=\frac{MS_M}{MS_R}
\tag{7.16}
\end{equation}\]</span></p>
<p>which has a F distribution with k number of predictors and n degrees of freedom. In other words, you divide the systematic (“explained”) variation due to the predictor variables by the unsystematic (“unexplained”) variation.</p>
<p>The result of the F-test is provided in the regression output. However, you might manually compute the F-test using the ANOVA results from the model:</p>
<div class="sourceCode" id="cb502"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb502-1"><a href="regression.html#cb502-1"></a><span class="kw">anova</span>(simple_regression)  <span class="co">#anova results</span></span></code></pre></div>
<pre><code>## Analysis of Variance Table
## 
## Response: sales
##            Df Sum Sq Mean Sq F value              Pr(&gt;F)    
## adspend     1 433688  433688    99.6 &lt;0.0000000000000002 ***
## Residuals 198 862264    4355                                
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<div class="sourceCode" id="cb504"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb504-1"><a href="regression.html#cb504-1"></a>f_calc &lt;-<span class="st"> </span><span class="kw">anova</span>(simple_regression)<span class="op">$</span><span class="st">&quot;Mean Sq&quot;</span>[<span class="dv">1</span>]<span class="op">/</span><span class="kw">anova</span>(simple_regression)<span class="op">$</span><span class="st">&quot;Mean Sq&quot;</span>[<span class="dv">2</span>]  <span class="co">#compute F</span></span>
<span id="cb504-2"><a href="regression.html#cb504-2"></a>f_calc</span></code></pre></div>
<pre><code>## [1] 100</code></pre>
<div class="sourceCode" id="cb506"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb506-1"><a href="regression.html#cb506-1"></a>f_crit &lt;-<span class="st"> </span><span class="kw">qf</span>(<span class="fl">0.95</span>, <span class="dt">df1 =</span> <span class="dv">1</span>, <span class="dt">df2 =</span> <span class="dv">100</span>)  <span class="co">#critical value</span></span>
<span id="cb506-2"><a href="regression.html#cb506-2"></a>f_crit</span></code></pre></div>
<pre><code>## [1] 3.9</code></pre>
<div class="sourceCode" id="cb508"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb508-1"><a href="regression.html#cb508-1"></a>f_calc <span class="op">&gt;</span><span class="st"> </span>f_crit  <span class="co">#test if calculated test statistic is larger than critical value</span></span></code></pre></div>
<pre><code>## [1] TRUE</code></pre>
</div>
</div>
<div id="using-the-model" class="section level4">
<h4><span class="header-section-number">7.2.1.4</span> Using the model</h4>
<p>After fitting the model, we can use the estimated coefficients to predict sales for different values of advertising. Suppose you want to predict sales for a new product, and the company plans to spend 800 Euros on advertising. How much will it sell? You can easily compute this either by hand:</p>
<p><span class="math display">\[\hat{sales}=134.134 + 0.09612*800=211\]</span></p>
<p><br></p>
<p>… or by extracting the estimated coefficients from the model summary:</p>
<div class="sourceCode" id="cb510"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb510-1"><a href="regression.html#cb510-1"></a><span class="kw">summary</span>(simple_regression)<span class="op">$</span>coefficients[<span class="dv">1</span>,<span class="dv">1</span>] <span class="op">+</span><span class="st"> </span><span class="co"># the intercept</span></span>
<span id="cb510-2"><a href="regression.html#cb510-2"></a><span class="kw">summary</span>(simple_regression)<span class="op">$</span>coefficients[<span class="dv">2</span>,<span class="dv">1</span>]<span class="op">*</span><span class="dv">800</span> <span class="co"># the slope * 800</span></span></code></pre></div>
<pre><code>## [1] 211</code></pre>
<p>The predicted value of the dependent variable is 211 units, i.e., the product will (on average) sell 211 units.</p>
</div>
</div>
<div id="multiple-linear-regression" class="section level3">
<h3><span class="header-section-number">7.2.2</span> Multiple linear regression</h3>
<br>
<div align="center">
<iframe width="560" height="315" src="https://www.youtube.com/embed/SDB2PhHMgxg" frameborder="0" allowfullscreen>
</iframe>
</div>
<p><br></p>
<p>Multiple linear regression is a statistical technique that simultaneously tests the relationships between two or more independent variables and an interval-scaled dependent variable. The general form of the equation is given by:</p>
<p><span class="math display" id="eq:regequ">\[\begin{equation} 
Y=(\beta_0+\beta_1*X_1+\beta_2*X_2+\beta_n*X_n)+\epsilon
\tag{7.5}
\end{equation}\]</span></p>
<p>Again, we aim to find the linear combination of predictors that correlate maximally with the outcome variable. Note that if you change the composition of predictors, the partial regression coefficient of an independent variable will be different from that of the bivariate regression coefficient. This is because the regressors are usually correlated, and any variation in Y that was shared by X1 and X2 was attributed to X1. The interpretation of the partial regression coefficients is the expected change in Y when X is changed by one unit and all other predictors are held constant.</p>
<p>Let’s extend the previous example. Say, in addition to the influence of advertising, you are interested in estimating the influence of radio airplay on the number of album downloads. The corresponding equation would then be given by:</p>
<p><span class="math display" id="eq:regequadv">\[\begin{equation} 
Sales=\beta_0+\beta_1*adspend+\beta_2*airplay+\epsilon
\tag{7.6}
\end{equation}\]</span></p>
<p>The words “adspend” and “airplay” represent data that we have observed on advertising expenditures and number of radio plays, and β<sub>1</sub> and β<sub>2</sub> represent the unknown relationship between sales and advertising expenditures and radio airplay, respectively. The corresponding coefficients tell you by how much sales will increase for an additional Euro spent on advertising (when radio airplay is held constant) and by how much sales will increase for an additional radio play (when advertising expenditures are held constant). Thus, we can make predictions about album sales based not only on advertising spending, but also on radio airplay.</p>
<p>With several predictors, the partitioning of sum of squares is the same as in the bivariate model, except that the model is no longer a 2-D straight line. With two predictors, the regression line becomes a 3-D regression plane. In our example:</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-31"></span>
<div id="htmlwidget-d5079acbc85567375398" style="width:672px;height:480px;" class="plotly html-widget"></div>
<script type="application/json" data-for="htmlwidget-d5079acbc85567375398">{"x":{"visdat":{"139c8271515f2":["function () ","plotlyVisDat"],"139c82a734874":["function () ","data"]},"cur_data":"139c82a734874","attrs":{"139c8271515f2":{"x":{},"y":{},"z":{},"colors":["#A9D0F5","#08088A"],"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"type":"surface"},"139c82a734874":{"x":[10.256,174.093,1000,75.896,1351.254,202.705,365.985,305.268,263.268,513.694,152.609,35.987,1720.806,102.568,215.368,426.784,507.772,233.291,1035.433,102.642,526.142,624.538,912.349,611.479,215.994,561.963,474.76,231.523,678.596,70.922,1567.548,263.598,1423.568,715.678,251.192,777.237,509.43,964.11,583.627,923.373,344.392,1095.578,100.025,30.425,1080.342,97.972,799.899,1071.752,893.355,283.161,917.017,234.568,456.897,206.973,1294.099,826.859,406.814,564.158,192.607,10.652,45.689,42.568,20.456,635.192,1002.273,1177.047,507.638,265.398,215.689,526.48,26.895,883.877,9.104,103.568,169.583,429.504,223.639,145.585,1323.287,985.968,500.922,226.652,1051.168,68.093,1547.159,393.774,804.282,801.577,450.562,196.65,26.598,179.061,345.687,295.84,2271.86,1134.575,601.434,45.298,759.518,832.869,1326.598,56.894,709.399,56.895,767.134,503.172,700.929,910.851,888.569,800.615,1500,985.685,1380.689,785.694,792.345,957.167,1789.659,656.137,613.697,313.362,336.51,1544.899,68.954,1445.563,785.692,125.628,377.925,217.994,759.862,1163.444,842.957,125.179,236.598,669.811,1188.193,612.234,922.019,50,2000,1054.027,385.045,1507.972,102.568,204.568,1170.918,574.513,689.547,784.22,405.913,179.778,607.258,1542.329,1112.47,856.985,836.331,236.908,568.954,1077.855,579.321,1500,731.364,25.689,391.749,233.999,275.7,56.895,255.117,471.814,566.501,102.568,250.568,68.594,642.786,1500,102.563,756.984,51.229,644.151,537.352,15.313,243.237,256.894,22.464,45.689,724.938,1126.461,1985.119,1837.516,135.986,514.068,237.703,976.641,1452.689,1600,268.598,900.889,982.063,201.356,746.024,1132.877],"y":[43,40,5,34,37,13,23,54,18,2,11,30,32,22,36,37,9,2,12,5,14,20,57,20,19,35,22,16,53,4,29,43,26,28,24,37,32,34,30,15,23,31,21,28,18,38,28,37,26,30,10,21,18,14,38,36,24,32,9,39,24,45,13,17,32,23,0,25,35,26,19,26,53,29,28,17,26,42,35,17,36,45,20,15,28,27,17,32,46,36,47,19,22,55,31,39,21,36,21,44,27,27,16,33,33,21,35,26,14,34,11,28,33,20,33,28,30,34,49,40,20,42,35,35,8,49,19,42,6,36,32,28,25,34,33,21,34,63,31,25,42,37,25,26,39,44,46,36,12,2,29,33,28,10,38,19,19,13,30,38,22,23,22,20,18,37,16,20,32,26,53,28,32,24,37,30,19,47,22,22,10,1,1,39,8,38,35,40,22,21,27,31,19,24,1,38,26,11,34,55],"z":[330,300,250,120,290,60,140,290,160,100,160,150,290,140,230,230,30,80,190,90,120,150,230,70,150,210,180,140,360,10,240,270,290,220,150,230,220,240,260,170,130,270,140,60,210,190,210,240,210,200,140,90,120,100,360,180,240,150,110,90,160,230,40,60,230,230,120,100,150,120,60,280,120,230,230,40,140,360,250,210,260,250,200,150,250,100,260,210,290,210,220,70,110,250,320,300,180,180,200,320,280,140,100,120,230,150,250,190,240,250,230,120,230,110,210,230,320,210,230,250,60,330,150,360,150,180,80,180,130,320,280,200,130,190,270,150,230,310,340,240,180,220,40,190,290,220,340,250,190,120,230,190,210,170,310,90,170,140,300,340,170,100,200,80,100,70,50,70,240,160,290,140,210,300,230,280,160,200,210,110,110,70,100,190,70,360,360,300,120,200,150,220,280,300,140,290,180,140,210,250],"colors":["#A9D0F5","#08088A"],"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"type":"scatter3d","mode":"markers","marker":{"color":["darkgray","steelblue","steelblue","darkgray","steelblue","darkgray","darkgray","darkgray","darkgray","darkgray","darkgray","steelblue","darkgray","steelblue","steelblue","darkgray","darkgray","darkgray","darkgray","darkgray","darkgray","darkgray","steelblue","darkgray","darkgray","steelblue","darkgray","darkgray","steelblue","darkgray","darkgray","darkgray","darkgray","steelblue","darkgray","steelblue","steelblue","steelblue","darkgray","steelblue","steelblue","darkgray","darkgray","darkgray","darkgray","steelblue","darkgray","darkgray","steelblue","darkgray","steelblue","darkgray","steelblue","darkgray","steelblue","darkgray","darkgray","darkgray","darkgray","steelblue","steelblue","darkgray","darkgray","steelblue","darkgray","darkgray","darkgray","darkgray","darkgray","steelblue","steelblue","darkgray","darkgray","darkgray","darkgray","darkgray","darkgray","darkgray","darkgray","steelblue","darkgray","darkgray","darkgray","darkgray","darkgray","darkgray","darkgray","darkgray","darkgray","steelblue","darkgray","darkgray","steelblue","darkgray","darkgray","steelblue","darkgray","darkgray","darkgray","steelblue","darkgray","darkgray","steelblue","steelblue","darkgray","darkgray","steelblue","steelblue","darkgray","darkgray","darkgray","darkgray","steelblue","darkgray","darkgray","darkgray","darkgray","darkgray","darkgray","darkgray","steelblue","steelblue","darkgray","steelblue","steelblue","steelblue","darkgray","darkgray","steelblue","darkgray","darkgray","darkgray","darkgray","darkgray","darkgray","darkgray","steelblue","darkgray","darkgray","darkgray","darkgray","darkgray","darkgray","darkgray","darkgray","darkgray","darkgray","darkgray","steelblue","darkgray","darkgray","darkgray","darkgray","darkgray","darkgray","darkgray","darkgray","darkgray","darkgray","darkgray","darkgray","darkgray","darkgray","darkgray","darkgray","steelblue","steelblue","darkgray","darkgray","darkgray","steelblue","steelblue","darkgray","darkgray","darkgray","darkgray","darkgray","darkgray","darkgray","darkgray","steelblue","steelblue","darkgray","darkgray","darkgray","darkgray","darkgray","darkgray","steelblue","steelblue","darkgray","darkgray","darkgray","steelblue","darkgray","darkgray","darkgray","darkgray","steelblue","darkgray"],"size":3,"opacity":0.8,"symbol":75},"inherit":true}},"layout":{"margin":{"b":40,"l":60,"t":25,"r":10},"scene":{"xaxis":{"title":"adspend"},"yaxis":{"title":"airplay"},"zaxis":{"title":"sales"}},"hovermode":"closest","showlegend":false,"legend":{"yanchor":"top","y":0.5}},"source":"A","config":{"showSendToCloud":false},"data":[{"colorbar":{"title":"z","ticklen":2,"len":0.5,"lenmode":"fraction","y":1,"yanchor":"top"},"colorscale":[["0","rgba(169,208,245,1)"],["0.0416666666666667","rgba(164,199,241,1)"],["0.0833333333333333","rgba(159,191,236,1)"],["0.125","rgba(154,182,232,1)"],["0.166666666666667","rgba(149,174,227,1)"],["0.208333333333333","rgba(144,165,223,1)"],["0.25","rgba(139,157,218,1)"],["0.291666666666667","rgba(134,148,214,1)"],["0.333333333333333","rgba(128,140,209,1)"],["0.375","rgba(123,132,205,1)"],["0.416666666666667","rgba(118,124,200,1)"],["0.458333333333333","rgba(112,115,196,1)"],["0.5","rgba(106,107,191,1)"],["0.541666666666667","rgba(101,99,187,1)"],["0.583333333333333","rgba(95,91,183,1)"],["0.625","rgba(89,84,178,1)"],["0.666666666666667","rgba(83,76,174,1)"],["0.708333333333333","rgba(76,68,169,1)"],["0.75","rgba(70,60,165,1)"],["0.791666666666667","rgba(63,52,160,1)"],["0.833333333333333","rgba(55,44,156,1)"],["0.875","rgba(47,36,151,1)"],["0.916666666666667","rgba(37,28,147,1)"],["0.958333333333333","rgba(26,19,142,1)"],["1","rgba(8,8,138,1)"]],"showscale":true,"x":[9.104,103.3855,197.667,291.9485,386.23,480.5115,574.793,669.0745,763.356,857.6375,951.919,1046.2005,1140.482,1234.7635,1329.045,1423.3265,1517.608,1611.8895,1706.171,1800.4525,1894.734,1989.0155,2083.297,2177.5785,2271.86],"y":[0,2.625,5.25,7.875,10.5,13.125,15.75,18.375,21,23.625,26.25,28.875,31.5,34.125,36.75,39.375,42,44.625,47.25,49.875,52.5,55.125,57.75,60.375,63],"z":[[41.9148312294276,50.1066740811476,58.2985169328676,66.4903597845876,74.6822026363075,82.8740454880275,91.0658883397475,99.2577311914675,107.449574043187,115.641416894907,123.833259746627,132.025102598347,140.216945450067,148.408788301787,156.600631153507,164.792474005227,172.984316856947,181.176159708667,189.368002560387,197.559845412107,205.751688263827,213.943531115547,222.135373967267,230.327216818987,238.519059670707],[51.3354036298894,59.5272464816094,67.7190893333294,75.9109321850494,84.1027750367694,92.2946178884894,100.486460740209,108.678303591929,116.870146443649,125.061989295369,133.253832147089,141.445674998809,149.637517850529,157.829360702249,166.021203553969,174.213046405689,182.404889257409,190.596732109129,198.788574960849,206.980417812569,215.172260664289,223.364103516009,231.555946367729,239.747789219449,247.939632071169],[60.7559760303513,68.9478188820713,77.1396617337913,85.3315045855112,93.5233474372312,101.715190288951,109.907033140671,118.098875992391,126.290718844111,134.482561695831,142.674404547551,150.866247399271,159.058090250991,167.249933102711,175.441775954431,183.633618806151,191.825461657871,200.017304509591,208.209147361311,216.400990213031,224.592833064751,232.784675916471,240.976518768191,249.168361619911,257.360204471631],[70.1765484308131,78.3683912825331,86.5602341342531,94.7520769859731,102.943919837693,111.135762689413,119.327605541133,127.519448392853,135.711291244573,143.903134096293,152.094976948013,160.286819799733,168.478662651453,176.670505503173,184.862348354893,193.054191206613,201.246034058333,209.437876910053,217.629719761773,225.821562613493,234.013405465213,242.205248316933,250.397091168653,258.588934020373,266.780776872093],[79.597120831275,87.788963682995,95.9808065347149,104.172649386435,112.364492238155,120.556335089875,128.748177941595,136.940020793315,145.131863645035,153.323706496755,161.515549348475,169.707392200195,177.899235051915,186.091077903635,194.282920755355,202.474763607075,210.666606458795,218.858449310515,227.050292162235,235.242135013955,243.433977865675,251.625820717395,259.817663569115,268.009506420835,276.201349272555],[89.0176932317368,97.2095360834568,105.401378935177,113.593221786897,121.785064638617,129.976907490337,138.168750342057,146.360593193777,154.552436045497,162.744278897217,170.936121748937,179.127964600657,187.319807452377,195.511650304097,203.703493155817,211.895336007537,220.087178859257,228.279021710977,236.470864562696,244.662707414416,252.854550266136,261.046393117856,269.238235969576,277.430078821296,285.621921673016],[98.4382656321987,106.630108483919,114.821951335639,123.013794187359,131.205637039079,139.397479890799,147.589322742519,155.781165594239,163.973008445958,172.164851297678,180.356694149398,188.548537001118,196.740379852838,204.932222704558,213.124065556278,221.315908407998,229.507751259718,237.699594111438,245.891436963158,254.083279814878,262.275122666598,270.466965518318,278.658808370038,286.850651221758,295.042494073478],[107.85883803266,116.05068088438,124.2425237361,132.43436658782,140.62620943954,148.81805229126,157.00989514298,165.2017379947,173.39358084642,181.58542369814,189.77726654986,197.96910940158,206.1609522533,214.35279510502,222.54463795674,230.73648080846,238.92832366018,247.1201665119,255.31200936362,263.50385221534,271.69569506706,279.88753791878,288.0793807705,296.27122362222,304.46306647394],[117.279410433122,125.471253284842,133.663096136562,141.854938988282,150.046781840002,158.238624691722,166.430467543442,174.622310395162,182.814153246882,191.005996098602,199.197838950322,207.389681802042,215.581524653762,223.773367505482,231.965210357202,240.157053208922,248.348896060642,256.540738912362,264.732581764082,272.924424615802,281.116267467522,289.308110319242,297.499953170962,305.691796022682,313.883638874402],[126.699982833584,134.891825685304,143.083668537024,151.275511388744,159.467354240464,167.659197092184,175.851039943904,184.042882795624,192.234725647344,200.426568499064,208.618411350784,216.810254202504,225.002097054224,233.193939905944,241.385782757664,249.577625609384,257.769468461104,265.961311312824,274.153154164544,282.344997016264,290.536839867984,298.728682719704,306.920525571424,315.112368423144,323.304211274864],[136.120555234046,144.312398085766,152.504240937486,160.696083789206,168.887926640926,177.079769492646,185.271612344366,193.463455196086,201.655298047806,209.847140899526,218.038983751246,226.230826602966,234.422669454686,242.614512306406,250.806355158126,258.998198009846,267.190040861566,275.381883713286,283.573726565006,291.765569416726,299.957412268446,308.149255120166,316.341097971886,324.532940823606,332.724783675326],[145.541127634508,153.732970486228,161.924813337948,170.116656189668,178.308499041388,186.500341893108,194.692184744828,202.884027596548,211.075870448268,219.267713299988,227.459556151708,235.651399003428,243.843241855148,252.035084706868,260.226927558588,268.418770410308,276.610613262028,284.802456113748,292.994298965468,301.186141817187,309.377984668907,317.569827520627,325.761670372347,333.953513224067,342.145356075787],[154.96170003497,163.15354288669,171.34538573841,179.53722859013,187.72907144185,195.92091429357,204.11275714529,212.30459999701,220.49644284873,228.68828570045,236.88012855217,245.07197140389,253.263814255609,261.455657107329,269.647499959049,277.839342810769,286.031185662489,294.223028514209,302.414871365929,310.606714217649,318.798557069369,326.990399921089,335.182242772809,343.374085624529,351.565928476249],[164.382272435432,172.574115287152,180.765958138871,188.957800990591,197.149643842311,205.341486694031,213.533329545751,221.725172397471,229.917015249191,238.108858100911,246.300700952631,254.492543804351,262.684386656071,270.876229507791,279.068072359511,287.259915211231,295.451758062951,303.643600914671,311.835443766391,320.027286618111,328.219129469831,336.410972321551,344.602815173271,352.794658024991,360.986500876711],[173.802844835893,181.994687687613,190.186530539333,198.378373391053,206.570216242773,214.762059094493,222.953901946213,231.145744797933,239.337587649653,247.529430501373,255.721273353093,263.913116204813,272.104959056533,280.296801908253,288.488644759973,296.680487611693,304.872330463413,313.064173315133,321.256016166853,329.447859018573,337.639701870293,345.831544722013,354.023387573733,362.215230425453,370.407073277173],[183.223417236355,191.415260088075,199.607102939795,207.798945791515,215.990788643235,224.182631494955,232.374474346675,240.566317198395,248.758160050115,256.950002901835,265.141845753555,273.333688605275,281.525531456995,289.717374308715,297.909217160435,306.101060012155,314.292902863875,322.484745715595,330.676588567315,338.868431419035,347.060274270755,355.252117122475,363.443959974195,371.635802825915,379.827645677635],[192.643989636817,200.835832488537,209.027675340257,217.219518191977,225.411361043697,233.603203895417,241.795046747137,249.986889598857,258.178732450577,266.370575302297,274.562418154017,282.754261005737,290.946103857457,299.137946709177,307.329789560897,315.521632412617,323.713475264337,331.905318116057,340.097160967777,348.289003819497,356.480846671217,364.672689522937,372.864532374657,381.056375226377,389.248218078097],[202.064562037279,210.256404888999,218.448247740719,226.640090592439,234.831933444159,243.023776295879,251.215619147599,259.407461999319,267.599304851039,275.791147702759,283.982990554479,292.174833406199,300.366676257919,308.558519109639,316.750361961359,324.942204813079,333.134047664799,341.325890516519,349.517733368239,357.709576219959,365.901419071679,374.093261923398,382.285104775118,390.476947626838,398.668790478558],[211.485134437741,219.676977289461,227.868820141181,236.060662992901,244.252505844621,252.444348696341,260.636191548061,268.828034399781,277.019877251501,285.211720103221,293.403562954941,301.595405806661,309.787248658381,317.9790915101,326.170934361821,334.36277721354,342.55462006526,350.74646291698,358.9383057687,367.13014862042,375.32199147214,383.51383432386,391.70567717558,399.8975200273,408.08936287902],[220.905706838203,229.097549689923,237.289392541643,245.481235393363,253.673078245083,261.864921096803,270.056763948522,278.248606800242,286.440449651962,294.632292503682,302.824135355402,311.015978207122,319.207821058842,327.399663910562,335.591506762282,343.783349614002,351.975192465722,360.167035317442,368.358878169162,376.550721020882,384.742563872602,392.934406724322,401.126249576042,409.318092427762,417.509935279482],[230.326279238664,238.518122090384,246.709964942104,254.901807793824,263.093650645544,271.285493497264,279.477336348984,287.669179200704,295.861022052424,304.052864904144,312.244707755864,320.436550607584,328.628393459304,336.820236311024,345.012079162744,353.203922014464,361.395764866184,369.587607717904,377.779450569624,385.971293421344,394.163136273064,402.354979124784,410.546821976504,418.738664828224,426.930507679944],[239.746851639126,247.938694490846,256.130537342566,264.322380194286,272.514223046006,280.706065897726,288.897908749446,297.089751601166,305.281594452886,313.473437304606,321.665280156326,329.857123008046,338.048965859766,346.240808711486,354.432651563206,362.624494414926,370.816337266646,379.008180118366,387.200022970086,395.391865821806,403.583708673526,411.775551525246,419.967394376966,428.159237228686,436.351080080406],[249.167424039588,257.359266891308,265.551109743028,273.742952594748,281.934795446468,290.126638298188,298.318481149908,306.510324001628,314.702166853348,322.894009705068,331.085852556788,339.277695408508,347.469538260228,355.661381111948,363.853223963668,372.045066815388,380.236909667108,388.428752518828,396.620595370548,404.812438222268,413.004281073988,421.196123925708,429.387966777428,437.579809629148,445.771652480868],[258.58799644005,266.77983929177,274.97168214349,283.16352499521,291.35536784693,299.54721069865,307.73905355037,315.93089640209,324.12273925381,332.31458210553,340.50642495725,348.69826780897,356.89011066069,365.08195351241,373.27379636413,381.46563921585,389.65748206757,397.84932491929,406.04116777101,414.23301062273,422.42485347445,430.61669632617,438.80853917789,447.00038202961,455.192224881329],[268.008568840512,276.200411692232,284.392254543952,292.584097395672,300.775940247392,308.967783099112,317.159625950832,325.351468802552,333.543311654272,341.735154505992,349.926997357712,358.118840209432,366.310683061152,374.502525912872,382.694368764592,390.886211616311,399.078054468031,407.269897319751,415.461740171471,423.653583023191,431.845425874911,440.037268726631,448.229111578351,456.420954430071,464.612797281791]],"type":"surface","frame":null},{"x":[10.256,174.093,1000,75.896,1351.254,202.705,365.985,305.268,263.268,513.694,152.609,35.987,1720.806,102.568,215.368,426.784,507.772,233.291,1035.433,102.642,526.142,624.538,912.349,611.479,215.994,561.963,474.76,231.523,678.596,70.922,1567.548,263.598,1423.568,715.678,251.192,777.237,509.43,964.11,583.627,923.373,344.392,1095.578,100.025,30.425,1080.342,97.972,799.899,1071.752,893.355,283.161,917.017,234.568,456.897,206.973,1294.099,826.859,406.814,564.158,192.607,10.652,45.689,42.568,20.456,635.192,1002.273,1177.047,507.638,265.398,215.689,526.48,26.895,883.877,9.104,103.568,169.583,429.504,223.639,145.585,1323.287,985.968,500.922,226.652,1051.168,68.093,1547.159,393.774,804.282,801.577,450.562,196.65,26.598,179.061,345.687,295.84,2271.86,1134.575,601.434,45.298,759.518,832.869,1326.598,56.894,709.399,56.895,767.134,503.172,700.929,910.851,888.569,800.615,1500,985.685,1380.689,785.694,792.345,957.167,1789.659,656.137,613.697,313.362,336.51,1544.899,68.954,1445.563,785.692,125.628,377.925,217.994,759.862,1163.444,842.957,125.179,236.598,669.811,1188.193,612.234,922.019,50,2000,1054.027,385.045,1507.972,102.568,204.568,1170.918,574.513,689.547,784.22,405.913,179.778,607.258,1542.329,1112.47,856.985,836.331,236.908,568.954,1077.855,579.321,1500,731.364,25.689,391.749,233.999,275.7,56.895,255.117,471.814,566.501,102.568,250.568,68.594,642.786,1500,102.563,756.984,51.229,644.151,537.352,15.313,243.237,256.894,22.464,45.689,724.938,1126.461,1985.119,1837.516,135.986,514.068,237.703,976.641,1452.689,1600,268.598,900.889,982.063,201.356,746.024,1132.877],"y":[43,40,5,34,37,13,23,54,18,2,11,30,32,22,36,37,9,2,12,5,14,20,57,20,19,35,22,16,53,4,29,43,26,28,24,37,32,34,30,15,23,31,21,28,18,38,28,37,26,30,10,21,18,14,38,36,24,32,9,39,24,45,13,17,32,23,0,25,35,26,19,26,53,29,28,17,26,42,35,17,36,45,20,15,28,27,17,32,46,36,47,19,22,55,31,39,21,36,21,44,27,27,16,33,33,21,35,26,14,34,11,28,33,20,33,28,30,34,49,40,20,42,35,35,8,49,19,42,6,36,32,28,25,34,33,21,34,63,31,25,42,37,25,26,39,44,46,36,12,2,29,33,28,10,38,19,19,13,30,38,22,23,22,20,18,37,16,20,32,26,53,28,32,24,37,30,19,47,22,22,10,1,1,39,8,38,35,40,22,21,27,31,19,24,1,38,26,11,34,55],"z":[330,300,250,120,290,60,140,290,160,100,160,150,290,140,230,230,30,80,190,90,120,150,230,70,150,210,180,140,360,10,240,270,290,220,150,230,220,240,260,170,130,270,140,60,210,190,210,240,210,200,140,90,120,100,360,180,240,150,110,90,160,230,40,60,230,230,120,100,150,120,60,280,120,230,230,40,140,360,250,210,260,250,200,150,250,100,260,210,290,210,220,70,110,250,320,300,180,180,200,320,280,140,100,120,230,150,250,190,240,250,230,120,230,110,210,230,320,210,230,250,60,330,150,360,150,180,80,180,130,320,280,200,130,190,270,150,230,310,340,240,180,220,40,190,290,220,340,250,190,120,230,190,210,170,310,90,170,140,300,340,170,100,200,80,100,70,50,70,240,160,290,140,210,300,230,280,160,200,210,110,110,70,100,190,70,360,360,300,120,200,150,220,280,300,140,290,180,140,210,250],"type":"scatter3d","mode":"markers","marker":{"color":["darkgray","steelblue","steelblue","darkgray","steelblue","darkgray","darkgray","darkgray","darkgray","darkgray","darkgray","steelblue","darkgray","steelblue","steelblue","darkgray","darkgray","darkgray","darkgray","darkgray","darkgray","darkgray","steelblue","darkgray","darkgray","steelblue","darkgray","darkgray","steelblue","darkgray","darkgray","darkgray","darkgray","steelblue","darkgray","steelblue","steelblue","steelblue","darkgray","steelblue","steelblue","darkgray","darkgray","darkgray","darkgray","steelblue","darkgray","darkgray","steelblue","darkgray","steelblue","darkgray","steelblue","darkgray","steelblue","darkgray","darkgray","darkgray","darkgray","steelblue","steelblue","darkgray","darkgray","steelblue","darkgray","darkgray","darkgray","darkgray","darkgray","steelblue","steelblue","darkgray","darkgray","darkgray","darkgray","darkgray","darkgray","darkgray","darkgray","steelblue","darkgray","darkgray","darkgray","darkgray","darkgray","darkgray","darkgray","darkgray","darkgray","steelblue","darkgray","darkgray","steelblue","darkgray","darkgray","steelblue","darkgray","darkgray","darkgray","steelblue","darkgray","darkgray","steelblue","steelblue","darkgray","darkgray","steelblue","steelblue","darkgray","darkgray","darkgray","darkgray","steelblue","darkgray","darkgray","darkgray","darkgray","darkgray","darkgray","darkgray","steelblue","steelblue","darkgray","steelblue","steelblue","steelblue","darkgray","darkgray","steelblue","darkgray","darkgray","darkgray","darkgray","darkgray","darkgray","darkgray","steelblue","darkgray","darkgray","darkgray","darkgray","darkgray","darkgray","darkgray","darkgray","darkgray","darkgray","darkgray","steelblue","darkgray","darkgray","darkgray","darkgray","darkgray","darkgray","darkgray","darkgray","darkgray","darkgray","darkgray","darkgray","darkgray","darkgray","darkgray","darkgray","steelblue","steelblue","darkgray","darkgray","darkgray","steelblue","steelblue","darkgray","darkgray","darkgray","darkgray","darkgray","darkgray","darkgray","darkgray","steelblue","steelblue","darkgray","darkgray","darkgray","darkgray","darkgray","darkgray","steelblue","steelblue","darkgray","darkgray","darkgray","steelblue","darkgray","darkgray","darkgray","darkgray","steelblue","darkgray"],"size":3,"opacity":0.8,"symbol":75,"line":{"color":"rgba(255,127,14,1)"}},"error_y":{"color":"rgba(255,127,14,1)"},"error_x":{"color":"rgba(255,127,14,1)"},"line":{"color":"rgba(255,127,14,1)"},"frame":null}],"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.2,"selected":{"opacity":1},"debounce":0},"shinyEvents":["plotly_hover","plotly_click","plotly_selected","plotly_relayout","plotly_brushed","plotly_brushing","plotly_clickannotation","plotly_doubleclick","plotly_deselect","plotly_afterplot","plotly_sunburstclick"],"base_url":"https://plot.ly"},"evals":[],"jsHooks":[]}</script>
<p class="caption">
Figure 4.4: Regression plane
</p>
</div>
<p>Like in the bivariate case, the plane is fitted to the data with the aim to predict the observed data as good as possible. The deviation of the observations from the plane represent the residuals (the error we make in predicting the observed data from the model). Note that this is conceptually the same as in the bivariate case, except that the computation is more complex (we won’t go into details here). The model is fairly easy to plot using a 3-D scatterplot, because we only have two predictors. While multiple regression models that have more than two predictors are not as easy to visualize, you may apply the same principles when interpreting the model outcome:</p>
<ul>
<li>Total sum of squares (SS<sub>T</sub>) is still the difference between the observed data and the mean value of Y (total variation)</li>
<li>Residual sum of squares (SS<sub>R</sub>) is still the difference between the observed data and the values predicted by the model (unexplained variation)</li>
<li>Model sum of squares (SS<sub>M</sub>) is still the difference between the values predicted by the model and the mean value of Y (explained variation)</li>
<li>R measures the multiple correlation between the predictors and the outcome</li>
<li>R<sup>2</sup> is the amount of variation in the outcome variable explained by the model</li>
</ul>
<p>Estimating multiple regression models is straightforward using the <code>lm()</code> function. You just need to separate the individual predictors on the right hand side of the equation using the <code>+</code> symbol. For example, the model:</p>
<p><span class="math display" id="eq:regequadv">\[\begin{equation} 
Sales=\beta_0+\beta_1*adspend+\beta_2*airplay+\beta_3*starpower+\epsilon
\tag{7.6}
\end{equation}\]</span></p>
<p>could be estimated as follows:</p>
<div class="sourceCode" id="cb512"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb512-1"><a href="regression.html#cb512-1"></a>multiple_regression &lt;-<span class="st"> </span><span class="kw">lm</span>(sales <span class="op">~</span><span class="st"> </span>adspend <span class="op">+</span><span class="st"> </span>airplay <span class="op">+</span><span class="st"> </span></span>
<span id="cb512-2"><a href="regression.html#cb512-2"></a><span class="st">    </span>starpower, <span class="dt">data =</span> regression)  <span class="co">#estimate linear model</span></span>
<span id="cb512-3"><a href="regression.html#cb512-3"></a><span class="kw">summary</span>(multiple_regression)  <span class="co">#summary of results</span></span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = sales ~ adspend + airplay + starpower, data = regression)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -121.32  -28.34   -0.45   28.97  144.13 
## 
## Coefficients:
##              Estimate Std. Error t value             Pr(&gt;|t|)    
## (Intercept) -26.61296   17.35000   -1.53                 0.13    
## adspend       0.08488    0.00692   12.26 &lt; 0.0000000000000002 ***
## airplay       3.36743    0.27777   12.12 &lt; 0.0000000000000002 ***
## starpower    11.08634    2.43785    4.55            0.0000095 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 47 on 196 degrees of freedom
## Multiple R-squared:  0.665,  Adjusted R-squared:  0.66 
## F-statistic:  129 on 3 and 196 DF,  p-value: &lt;0.0000000000000002</code></pre>
<p>The interpretation of the coefficients is as follows:</p>
<ul>
<li>adspend (β<sub>1</sub>): when advertising expenditures increase by 1 Euro, sales will increase by 0.085 units</li>
<li>airplay (β<sub>2</sub>): when radio airplay increases by 1 play per week, sales will increase by 3.367 units</li>
<li>starpower (β<sub>3</sub>): when the number of previous albums increases by 1, sales will increase by 11.086 units</li>
</ul>
<p>The associated t-values and p-values are also given in the output. You can see that the p-values are smaller than 0.05 for all three coefficients. Hence, all effects are “significant”. This means that if the null hypothesis was true (i.e., there was no effect between the variables and sales), the probability of observing associations of the estimated magnitudes (or larger) is very small (e.g., smaller than 0.05).</p>
<p>Again, to get a better feeling for the range of values that the coefficients could take, it is helpful to compute <b>confidence intervals</b>.</p>
<div class="sourceCode" id="cb514"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb514-1"><a href="regression.html#cb514-1"></a><span class="kw">confint</span>(multiple_regression)</span></code></pre></div>
<pre><code>##               2.5 % 97.5 %
## (Intercept) -60.830  7.604
## adspend       0.071  0.099
## airplay       2.820  3.915
## starpower     6.279 15.894</code></pre>
<p>What does this tell you? Recall that a 95% confidence interval is defined as a range of values such that with a 95% probability, the range will contain the true unknown value of the parameter. For example, for β<sub>3</sub>, the confidence interval is [6.2785522,15.8941182]. Thus, although we have computed a point estimate of 11.086 for the effect of starpower on sales based on our sample, the effect might actually just as well take any other value within this range, considering the sample size and the variability in our data. You could also visualize the output from your regression model including the confidence intervals using the <code>ggstatsplot</code> package as follows:</p>
<div class="sourceCode" id="cb516"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb516-1"><a href="regression.html#cb516-1"></a><span class="kw">library</span>(ggstatsplot)</span>
<span id="cb516-2"><a href="regression.html#cb516-2"></a><span class="kw">ggcoefstats</span>(<span class="dt">x =</span> multiple_regression, <span class="dt">title =</span> <span class="st">&quot;Sales predicted by adspend, airplay, &amp; starpower&quot;</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-34"></span>
<img src="10-Regression_files/figure-html/unnamed-chunk-34-1.png" alt="Confidence intervals for regression model" width="672" />
<p class="caption">
Figure 4.5: Confidence intervals for regression model
</p>
</div>
<p>The output also tells us that 66.4667687% of the variation can be explained by our model. You may also visually inspect the fit of the model by plotting the predicted values against the observed values. We can extract the predicted values using the <code>predict()</code> function. So let’s create a new variable <code>yhat</code>, which contains those predicted values.</p>
<div class="sourceCode" id="cb517"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb517-1"><a href="regression.html#cb517-1"></a>regression<span class="op">$</span>yhat &lt;-<span class="st"> </span><span class="kw">predict</span>(simple_regression)</span></code></pre></div>
<p>We can now use this variable to plot the predicted values against the observed values. In the following plot, the model fit would be perfect if all points would fall on the diagonal line. The larger the distance between the points and the line, the worse the model fit. In other words, if all points would fall exactly on the diagonal line, the model would perfectly predict the observed values.</p>
<div class="sourceCode" id="cb518"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb518-1"><a href="regression.html#cb518-1"></a><span class="kw">ggplot</span>(regression,<span class="kw">aes</span>(yhat,sales)) <span class="op">+</span><span class="st">  </span></span>
<span id="cb518-2"><a href="regression.html#cb518-2"></a><span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">size=</span><span class="dv">2</span>,<span class="dt">shape=</span><span class="dv">1</span>) <span class="op">+</span><span class="st">  </span><span class="co">#Use hollow circles</span></span>
<span id="cb518-3"><a href="regression.html#cb518-3"></a><span class="st">  </span><span class="kw">scale_x_continuous</span>(<span class="dt">name=</span><span class="st">&quot;predicted values&quot;</span>) <span class="op">+</span></span>
<span id="cb518-4"><a href="regression.html#cb518-4"></a><span class="st">  </span><span class="kw">scale_y_continuous</span>(<span class="dt">name=</span><span class="st">&quot;observed values&quot;</span>) <span class="op">+</span></span>
<span id="cb518-5"><a href="regression.html#cb518-5"></a><span class="st">  </span><span class="kw">geom_abline</span>(<span class="dt">intercept =</span> <span class="dv">0</span>, <span class="dt">slope =</span> <span class="dv">1</span>) <span class="op">+</span></span>
<span id="cb518-6"><a href="regression.html#cb518-6"></a><span class="st">  </span><span class="kw">theme_bw</span>()</span></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-36"></span>
<img src="10-Regression_files/figure-html/unnamed-chunk-36-1.png" alt="Model fit" width="672" />
<p class="caption">
Figure 4.6: Model fit
</p>
</div>
<p><strong>Partial plots</strong></p>
<p>In the context of a simple linear regression (i.e., with a single independent variable), a scatter plot of the dependent variable against the independent variable provides a good indication of the nature of the relationship. If there is more than one independent variable, however, things become more complicated. The reason is that although the scatter plot still show the relationship between the two variables, it does not take into account the effect of the other independent variables in the model. Partial regression plot show the effect of adding another variable to a model that already controls for the remaining variables in the model. In other words, it is a scatterplot of the residuals of the outcome variable and each predictor when both variables are regressed separately on the remaining predictors. As an example, consider the effect of advertising expenditures on sales. In this case, the partial plot would show the effect of adding advertising expenditures as an explanatory variable while controlling for the variation that is explained by airplay and starpower in both variables (sales and advertising). Think of it as the purified relationship between advertising and sales that remains after controlling for other factors. The partial plots can easily be created using the <code>avPlots()</code> function from the <code>car</code> package:</p>
<div class="sourceCode" id="cb519"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb519-1"><a href="regression.html#cb519-1"></a><span class="kw">library</span>(car)</span>
<span id="cb519-2"><a href="regression.html#cb519-2"></a><span class="kw">avPlots</span>(multiple_regression)</span></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-37"></span>
<img src="10-Regression_files/figure-html/unnamed-chunk-37-1.png" alt="Partial plots" width="672" />
<p class="caption">
Figure 4.7: Partial plots
</p>
</div>
<p><strong>Using the model</strong></p>
<p>After fitting the model, we can use the estimated coefficients to predict sales for different values of advertising, airplay, and starpower. Suppose you would like to predict sales for a new music album with advertising expenditures of 800, airplay of 30 and starpower of 5. How much will it sell?</p>
<p><span class="math display">\[\hat{sales}=−26.61 + 0.084 * 800 + 3.367*30 + 11.08 ∗ 5= 197.74\]</span></p>
<p><br></p>
<p>… or by extracting the estimated coefficients:</p>
<div class="sourceCode" id="cb520"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb520-1"><a href="regression.html#cb520-1"></a><span class="kw">summary</span>(multiple_regression)<span class="op">$</span>coefficients[<span class="dv">1</span>, <span class="dv">1</span>] <span class="op">+</span><span class="st"> </span><span class="kw">summary</span>(multiple_regression)<span class="op">$</span>coefficients[<span class="dv">2</span>, </span>
<span id="cb520-2"><a href="regression.html#cb520-2"></a>    <span class="dv">1</span>] <span class="op">*</span><span class="st"> </span><span class="dv">800</span> <span class="op">+</span><span class="st"> </span><span class="kw">summary</span>(multiple_regression)<span class="op">$</span>coefficients[<span class="dv">3</span>, </span>
<span id="cb520-3"><a href="regression.html#cb520-3"></a>    <span class="dv">1</span>] <span class="op">*</span><span class="st"> </span><span class="dv">30</span> <span class="op">+</span><span class="st"> </span><span class="kw">summary</span>(multiple_regression)<span class="op">$</span>coefficients[<span class="dv">4</span>, </span>
<span id="cb520-4"><a href="regression.html#cb520-4"></a>    <span class="dv">1</span>] <span class="op">*</span><span class="st"> </span><span class="dv">5</span></span></code></pre></div>
<pre><code>## [1] 198</code></pre>
<p>The predicted value of the dependent variable is 198 units, i.e., the product will sell 198 units.</p>
<p><strong>Comparing effects</strong></p>
<p>Using the output from the regression model above, it is difficult to compare the effects of the independent variables because they are all measured on different scales (Euros, radio plays, releases). Standardized regression coefficients can be used to judge the relative importance of the predictor variables. Standardization is achieved by multiplying the unstandardized coefficient by the ratio of the standard deviations of the independent and dependent variables:</p>
<p><span class="math display" id="eq:stdcoeff">\[\begin{equation} 
B_{k}=\beta_{k} * \frac{s_{x_k}}{s_y}
\tag{7.18}
\end{equation}\]</span></p>
<p>Hence, the standardized coefficient will tell you by how many standard deviations the outcome will change as a result of a one standard deviation change in the predictor variable. Standardized coefficients can be easily computed using the <code>lm.beta()</code> function from the <code>lm.beta</code> package.</p>
<div class="sourceCode" id="cb522"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb522-1"><a href="regression.html#cb522-1"></a><span class="kw">library</span>(lm.beta)</span>
<span id="cb522-2"><a href="regression.html#cb522-2"></a><span class="kw">lm.beta</span>(multiple_regression)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = sales ~ adspend + airplay + starpower, data = regression)
## 
## Standardized Coefficients::
## (Intercept)     adspend     airplay   starpower 
##        0.00        0.51        0.51        0.19</code></pre>
<p>The results show that for <code>adspend</code> and <code>airplay</code>, a change by one standard deviation will result in a 0.51 standard deviation change in sales, whereas for <code>starpower</code>, a one standard deviation change will only lead to a 0.19 standard deviation change in sales. Hence, while the effects of <code>adspend</code> and <code>airplay</code> are comparable in magnitude, the effect of <code>starpower</code> is less strong.</p>
<p><br></p>
</div>
</div>
<div id="potential-problems" class="section level2">
<h2><span class="header-section-number">7.3</span> Potential problems</h2>
<p>Once you have built and estimated your model it is important to run diagnostics to ensure that the results are accurate. In the following section we will discuss common problems.</p>
<div id="outliers" class="section level3">
<h3><span class="header-section-number">7.3.1</span> Outliers</h3>
<br>
<div align="center">
<iframe width="560" height="315" src="https://www.youtube.com/embed/TphnIgvRUlA" frameborder="0" allowfullscreen>
</iframe>
</div>
<p><br></p>
<p>Outliers are data points that differ vastly from the trend. They can introduce bias into a model due to the fact that they alter the parameter estimates. Consider the example below. A linear regression was performed twice on the same data set, except during the second estimation the two green points were changed to be outliers by being moved to the positions indicated in red. The solid red line is the regression line based on the unaltered data set, while the dotted line was estimated using the altered data set. As you can see the second regression would lead to different conclusions than the first. Therefore it is important to identify outliers and further deal with them.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-40"></span>
<img src="10-Regression_files/figure-html/unnamed-chunk-40-1.png" alt="Effects of outliers" width="672" />
<p class="caption">
Figure 7.1: Effects of outliers
</p>
</div>
<p>One quick way to visually detect outliers is by creating a scatterplot (as above) to see whether anything seems off. Another approach is to inspect the studentized residuals. If there are no outliers in your data, about 95% will be between -2 and 2, as per the assumptions of the normal distribution. Values well outside of this range are unlikely to happen by chance and warrant further inspection. As a rule of thumb, observations whose studentized residuals are greater than 3 in absolute values are potential outliers.</p>
<p>The studentized residuals can be obtained in R with the function <code>rstudent()</code>. We can use this function to create a new variable that contains the studentized residuals e music sales regression from before yields the following residuals:</p>
<div class="sourceCode" id="cb524"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb524-1"><a href="regression.html#cb524-1"></a>regression<span class="op">$</span>stud_resid &lt;-<span class="st"> </span><span class="kw">rstudent</span>(multiple_regression)</span>
<span id="cb524-2"><a href="regression.html#cb524-2"></a><span class="kw">head</span>(regression)</span></code></pre></div>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["sales"],"name":[1],"type":["int"],"align":["right"]},{"label":["adspend"],"name":[2],"type":["dbl"],"align":["right"]},{"label":["airplay"],"name":[3],"type":["int"],"align":["right"]},{"label":["starpower"],"name":[4],"type":["int"],"align":["right"]},{"label":["genre"],"name":[5],"type":["int"],"align":["right"]},{"label":["country"],"name":[6],"type":["int"],"align":["right"]},{"label":["yhat"],"name":[7],"type":["dbl"],"align":["right"]},{"label":["stud_resid"],"name":[8],"type":["dbl"],"align":["right"]}],"data":[{"1":"330","2":"10","3":"43","4":"10","5":"3","6":"1","7":"135","8":"2.20"},{"1":"300","2":"174","3":"40","4":"7","5":"3","6":"1","7":"229","8":"2.15"},{"1":"250","2":"1000","3":"5","4":"7","5":"2","6":"1","7":"273","8":"2.11"},{"1":"120","2":"76","3":"34","4":"6","5":"1","6":"0","7":"248","8":"-0.87"},{"1":"290","2":"1351","3":"37","4":"9","5":"3","6":"0","7":"189","8":"-0.48"},{"1":"60","2":"203","3":"13","4":"8","5":"1","6":"0","7":"189","8":"-1.36"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<p>A good way to visually inspect the studentized residuals is to plot them in a scatterplot and roughly check if most of the observations are within the -3, 3 bounds.</p>
<div class="sourceCode" id="cb525"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb525-1"><a href="regression.html#cb525-1"></a><span class="kw">plot</span>(<span class="dv">1</span><span class="op">:</span><span class="kw">nrow</span>(regression), regression<span class="op">$</span>stud_resid, <span class="dt">ylim =</span> <span class="kw">c</span>(<span class="op">-</span><span class="fl">3.3</span>, </span>
<span id="cb525-2"><a href="regression.html#cb525-2"></a>    <span class="fl">3.3</span>))  <span class="co">#create scatterplot </span></span>
<span id="cb525-3"><a href="regression.html#cb525-3"></a><span class="kw">abline</span>(<span class="dt">h =</span> <span class="kw">c</span>(<span class="op">-</span><span class="dv">3</span>, <span class="dv">3</span>), <span class="dt">col =</span> <span class="st">&quot;red&quot;</span>, <span class="dt">lty =</span> <span class="dv">2</span>)  <span class="co">#add reference lines</span></span></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-42"></span>
<img src="10-Regression_files/figure-html/unnamed-chunk-42-1.png" alt="Plot of the studentized residuals" width="672" />
<p class="caption">
Figure 7.2: Plot of the studentized residuals
</p>
</div>
<p>To identify potentially influential observations in our data set, we can apply a filter to our data:</p>
<div class="sourceCode" id="cb526"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb526-1"><a href="regression.html#cb526-1"></a>outliers &lt;-<span class="st"> </span><span class="kw">subset</span>(regression, <span class="kw">abs</span>(stud_resid) <span class="op">&gt;</span><span class="st"> </span><span class="dv">3</span>)</span>
<span id="cb526-2"><a href="regression.html#cb526-2"></a>outliers</span></code></pre></div>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["sales"],"name":[1],"type":["int"],"align":["right"]},{"label":["adspend"],"name":[2],"type":["dbl"],"align":["right"]},{"label":["airplay"],"name":[3],"type":["int"],"align":["right"]},{"label":["starpower"],"name":[4],"type":["int"],"align":["right"]},{"label":["genre"],"name":[5],"type":["int"],"align":["right"]},{"label":["country"],"name":[6],"type":["int"],"align":["right"]},{"label":["yhat"],"name":[7],"type":["dbl"],"align":["right"]},{"label":["stud_resid"],"name":[8],"type":["dbl"],"align":["right"]}],"data":[{"1":"360","2":"146","3":"42","4":"8","5":"3","6":"0","7":"139","8":"3.2"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<p>After a detailed inspection of the potential outliers, you might decide to delete the affected observations from the data set or not. If an outlier has resulted from an error in data collection, then you might simply remove the observation. However, even though data may have extreme values, they might not be influential to determine a regression line. That means, the results wouldn’t be much different if we either include or exclude them from analysis. This means that the decision of whether to exclude an outlier or not is closely related to the question whether this observation is an influential observation, as will be discussed next.</p>
</div>
<div id="influential-observations" class="section level3">
<h3><span class="header-section-number">7.3.2</span> Influential observations</h3>
<p>Related to the issue of outliers is that of influential observations, meaning observations that exert undue influence on the parameters. It is possible to determine whether or not the results are driven by an influential observation by calculating how far the predicted values for your data would move if the model was fitted without this particular observation. This calculated total distance is called <strong>Cook’s distance</strong>. To identify influential observations, we can inspect the respective plots created from the model output. A rule of thumb to determine whether an observation should be classified as influential or not is to look for observation with a Cook’s distance &gt; 1 (although opinions vary on this). The following plot can be used to see the Cook’s distance associated with each data point:</p>
<div class="sourceCode" id="cb527"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb527-1"><a href="regression.html#cb527-1"></a><span class="kw">plot</span>(multiple_regression, <span class="dv">4</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-44"></span>
<img src="10-Regression_files/figure-html/unnamed-chunk-44-1.png" alt="Cook's distance" width="672" />
<p class="caption">
Figure 4.11: Cook’s distance
</p>
</div>
<p>It is easy to see that none of the Cook’s distance values is close to the critical value of 1. Another useful plot to identify influential observations is plot number 5 from the output:</p>
<div class="sourceCode" id="cb528"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb528-1"><a href="regression.html#cb528-1"></a><span class="kw">plot</span>(multiple_regression, <span class="dv">5</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-45"></span>
<img src="10-Regression_files/figure-html/unnamed-chunk-45-1.png" alt="Residuals vs. Leverage" width="672" />
<p class="caption">
Figure 4.12: Residuals vs. Leverage
</p>
</div>
<p>In this plot, we look for cases outside of a dashed line, which represents <strong>Cook’s distance</strong>. Lines for Cook’s distance thresholds of 0.5 and 1 are included by default. In our example, this line is not even visible, since the Cook’s distance values are far away from the critical values. Generally, you would watch out for outlying values at the upper right corner or at the lower right corner of the plot. Those spots are the places where cases can be influential against a regression line. In our example, there are no influential cases.</p>
<p>To see how influential observations can impact your regression, have a look at <a href="https://en.wikipedia.org/wiki/Anscombe%27s_quartet" target="_blank">this example</a>.</p>
<div class="infobox_orange hint">
<p>To summarize, if you detected outliers in your data, you should test if these observations exert undue influence on your results using the Cook’s distance statistic as described above. If you detect observations which bias your results, you should remove these observations.</p>
</div>
</div>
<div id="non-linearity" class="section level3">
<h3><span class="header-section-number">7.3.3</span> Non-linearity</h3>
<br>
<div align="center">
<iframe width="560" height="315" src="https://www.youtube.com/embed/Fsf0q_6sKKY" frameborder="0" allowfullscreen>
</iframe>
</div>
<p><br></p>
<p>An important underlying assumption for OLS is that of linearity, meaning that the relationship between the dependent and the independent variable can be reasonably approximated in linear terms. One quick way to assess whether a linear relationship can be assumed is to inspect the added variable plots that we already came across earlier:</p>
<div class="sourceCode" id="cb529"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb529-1"><a href="regression.html#cb529-1"></a><span class="kw">library</span>(car)</span>
<span id="cb529-2"><a href="regression.html#cb529-2"></a><span class="kw">avPlots</span>(multiple_regression)</span></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-46"></span>
<img src="10-Regression_files/figure-html/unnamed-chunk-46-1.png" alt="Partial plots" width="672" />
<p class="caption">
Figure 4.13: Partial plots
</p>
</div>
<p>In our example, it appears that linear relationships can be reasonably assumed. Please note, however, that the linear model implies two things:</p>
<ul>
<li>Constant marginal returns</li>
<li>Elasticities increase with X</li>
</ul>
<p>These assumptions may not be justifiable in certain contexts. As an example, consider the effect of marketing expenditures on sales. The linear model assumes that if you change your advertising expenditures from, say 10€ to 11€, this will change sales by the same amount as if you would change your marketing expenditure from, say 100,000€ to 100,001€. This is what we mean by <strong>constant marginal returns</strong> - irrespective of the level of advertising, spending an additional Euro on advertising will change sales by the same amount. Or consider the effect of price on sales. A linear model assumes that changing the price from, say 10€ to 11€, will change the sales by the same amount as increasing the price from, say 20€ to 21€. An elasticity tells you the relative change in the outcome variable (e.g., sales) due to a relative change in the predictor variable. For example, if we change our advertising expenditures by 1%, sales will change by XY%. As we have seen, the linear model assumes constant marginal returns, which implies that the <strong>elasticity increases</strong> with the level of the independent variable. In our example, advertising becomes relatively more effective since as we move to higher levels of advertising expenditures, a relatively smaller change in advertising expenditure will yield the same return.</p>
<p>In marketing applications, it is often more realistic to assume <strong>decreasing marginal returns</strong>, meaning that the return from an increase in advertising is decreasing with increasing levels of advertising (e.g., and increase in advertising expenditures from 10€ to 11€ will lead to larger changes in sales, compared to a change from, say 100,000€ to 100,001€). We will see how to implement such a model further below in the section on extensions of the non-linear model.</p>
<div class="infobox_orange hint">
<p>To summarize, if you find indications that the linear specification might not represent your data well, you should consider a non-linear specification, which we will cover below. One popular and easy way to implement a non-linear specification in marketing applications is the so-called log-log model, where you take the logarithm of the dependent variable and independent variable(s). This type of model allows for decreasing marginal returns and yields constant elasticity, which is more realistic in many marketing settings. Constant elasticity means that a 1% change in the independent variable yields the same <em>relative</em> return for different levels of the independent variable. If you are unsure which model specification represents your data better, you can also compare different model specifications, e.g., by comparing the explained variance of the models (the better fitting model explains more of the variation in your data), and then opt for the specification that fits your data best.</p>
</div>
</div>
<div id="non-constant-error-variance" class="section level3">
<h3><span class="header-section-number">7.3.4</span> Non-constant error variance</h3>
<p>The following video summarizes how to identify non-constant error variance in R</p>
<div align="center">
<iframe width="560" height="315" src="https://www.youtube.com/embed/orrpr8if_Xc" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>
</iframe>
</div>
<p>Another important assumption of the linear model is that the error terms have a constant variance (i.e., homoskedasticity). The following plot from the model output shows the residuals (the vertical distance from an observed value to the predicted values) versus the fitted values (the predicted value from the regression model). If all the points fell exactly on the dashed grey line, it would mean that we have a perfect prediction. The residual variance (i.e., the spread of the values on the y-axis) should be similar across the scale of the fitted values on the x-axis.</p>
<div class="sourceCode" id="cb530"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb530-1"><a href="regression.html#cb530-1"></a><span class="kw">plot</span>(multiple_regression, <span class="dv">1</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-47"></span>
<img src="10-Regression_files/figure-html/unnamed-chunk-47-1.png" alt="Residuals vs. fitted values" width="672" />
<p class="caption">
Figure 6.1: Residuals vs. fitted values
</p>
</div>
<p>In our case, this appears to be the case. You can identify non-constant variances in the errors (i.e., heteroscedasticity) from the presence of a funnel shape in the above plot. When the assumption of constant error variances is not met, this might be due to a misspecification of your model (e.g., the relationship might not be linear). In these cases, it often helps to transform your data (e.g., using log-transformations). The red line also helps you to identify potential misspecification of your model. It is a smoothed curve that passes through the residuals and if it lies close to the gray dashed line (as in our case) it suggest a correct specification. If the line would deviate from the dashed grey line a lot (e.g., a U-shape or inverse U-shape), it would suggest that the linear model specification is not reasonable and you should try different specifications. You can also test for heterogskedasticity in you regression model by using the Breusch-Pagan test, which has the null hypothesis that the error variances are equal (i.e., homoskedasticity) versus the alternative that the error variances are not equal (i.e., heteroskedasticity). The test can be implemented using the <code>bptest()</code> function from the <code>lmtest</code> package.</p>
<div class="sourceCode" id="cb531"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb531-1"><a href="regression.html#cb531-1"></a><span class="kw">library</span>(lmtest)</span>
<span id="cb531-2"><a href="regression.html#cb531-2"></a><span class="kw">bptest</span>(multiple_regression)</span></code></pre></div>
<pre><code>## 
##  studentized Breusch-Pagan test
## 
## data:  multiple_regression
## BP = 6, df = 3, p-value = 0.1</code></pre>
<p>As the <em>p</em>-value is larger than 0.05, we cannot reject the null hypothesis of equal error variances so that the assumption of homoskedasticity is met.</p>
<p>If OLS is performed despite heteroscedasticity, the estimates of the coefficient will still be correct on average. However, the estimator is <em>inefficient</em>, meaning that the standard error is wrong, which will impact the significance tests (i.e., the <em>p</em>-values will be wrong).</p>
<p>Assume that the test would have suggested a violation of the assumption of homoskedasticity - how could you proceed in this case? In the presence of heteroskedasticity, you could rely on robust regression methods, which correct the standard errors. You could implement a robust regression model in R using the <code>coeftest()</code> function from the <code>sandwich</code> package as follows:</p>
<div class="sourceCode" id="cb533"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb533-1"><a href="regression.html#cb533-1"></a><span class="kw">library</span>(sandwich)</span>
<span id="cb533-2"><a href="regression.html#cb533-2"></a><span class="kw">coeftest</span>(multiple_regression, <span class="dt">vcov =</span> <span class="kw">vcovHC</span>(multiple_regression))</span></code></pre></div>
<pre><code>## 
## t test of coefficients:
## 
##              Estimate Std. Error t value             Pr(&gt;|t|)    
## (Intercept) -26.61296   16.17045   -1.65                  0.1    
## adspend       0.08488    0.00693   12.25 &lt; 0.0000000000000002 ***
## airplay       3.36743    0.31510   10.69 &lt; 0.0000000000000002 ***
## starpower    11.08634    2.24743    4.93            0.0000017 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>As you can see, the standard errors (and thus the t-values and p-values) are different compared to the non-robust specification above while the coefficients remain unchanged. However, the difference in this example is not too large since the Breusch-Pagan test suggested the presence of homoskedasticity and we could thus rely on the standard output.</p>
<div class="infobox_orange hint">
<p>To summarize, you can inspect if the assumption of homoskedasticity is met using the residual plot and the Breusch-Pagan test. If the assumption is violated, you should try to transform your data (e.g., using a log-transformation) first and see if this solves the problem. If the problem persists, you can rely on the robust standard errors as it is shown in the example above.</p>
</div>
</div>
<div id="non-normally-distributed-errors" class="section level3">
<h3><span class="header-section-number">7.3.5</span> Non-normally distributed errors</h3>
<div align="center">
<iframe width="560" height="315" src="https://www.youtube.com/embed/oZMET3J6v7Q" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>
</iframe>
</div>
<p>Another assumption of OLS is that the error term is normally distributed. This can be a reasonable assumption for many scenarios, but we still need a way to check if it is actually the case. As we can not directly observe the actual error term, we have to work with the next best thing - the residuals.</p>
<p>A quick way to assess whether a given sample is approximately normally distributed is by using Q-Q plots. These plot the theoretical position of the observations (under the assumption that they are normally distributed) against the actual position. The plot below is created by the model output and shows the residuals in a Q-Q plot. As you can see, most of the points roughly follow the theoretical distribution, as given by the straight line. If most of the points are close to the line, the data is approximately normally distributed.</p>
<div class="sourceCode" id="cb535"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb535-1"><a href="regression.html#cb535-1"></a><span class="kw">plot</span>(multiple_regression, <span class="dv">2</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-50"></span>
<img src="10-Regression_files/figure-html/unnamed-chunk-50-1.png" alt="Q-Q plot" width="672" />
<p class="caption">
Figure 4.14: Q-Q plot
</p>
</div>
<p>Another way to check for normal distribution of the data is to employ statistical tests that test the null hypothesis that the data is normally distributed, such as the Shapiro–Wilk test. We can extract the residuals from our model using the <code>resid()</code> function and apply the <code>shapiro.test()</code> function to it:</p>
<div class="sourceCode" id="cb536"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb536-1"><a href="regression.html#cb536-1"></a><span class="kw">shapiro.test</span>(<span class="kw">resid</span>(multiple_regression))</span></code></pre></div>
<pre><code>## 
##  Shapiro-Wilk normality test
## 
## data:  resid(multiple_regression)
## W = 1, p-value = 0.7</code></pre>
<p>As you can see, we can not reject the H<sub>0</sub> of normally distributed residuals, which means that we can assume the residuals to be approximately normally distributed in our example.</p>
<p>When the assumption of normally distributed errors is not met, this might again be due to a misspecification of your model, in which case it might help to transform your data (e.g., using log-transformations). If transforming your data doesn’t solve the issue, you may use <strong>bootstrapping</strong> to obtain corrected results. Bootstrapping is a so-called resampling method in which we use repeated random sampling with replacement to estimate the sampling distribution based on the sample itself, rather than relying on some assumptions about the shape of the sampling distribution to determine the probability of obtaining a test statistic of a particular magnitude. In other words, the data from our sample are treated as the population from which smaller random samples (so-called bootstrap samples) are repeatedly taken with replacement. The statistic of interest, e.g., the regression coefficients in our example, is calculated in each sample, and by taking many samples, the sampling distribution can be estimated. Similar to the simulations we did in chapter 5, the standard error of the statistic can be estimated using the standard deviation of the sampling distribution. Once we have computed the standard error, we can use it to compute the confidence intervals and significance tests. You can find a description of how to implement this procedure in R e.g., <a href="https://www.statmethods.net/advstats/bootstrapping.html" target="_blank">here</a>).</p>
<p>Essentially, we can use the <code>boot()</code> function contained in the <code>boot</code> package to obtain the bootstrapped results. However, we need to pass this function a statistic to apply the bootstrapping on - in our case, this would be the coefficients from our regression model. To make this easier, we can write a function that returns the coefficients from the model for every bootstrap sample that we take. In the following code block, we specify a function called <code>bs()</code>, which does exactly this (don’t worry if you don’t understand all the details - its basically just another function we can use to automate certain steps in the analysis, only that in this case we have written the function ourselves rather than relying on functions contained in existing packages).</p>
<div class="sourceCode" id="cb538"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb538-1"><a href="regression.html#cb538-1"></a><span class="co"># function to obtain regression coefficients</span></span>
<span id="cb538-2"><a href="regression.html#cb538-2"></a>bs &lt;-<span class="st"> </span><span class="cf">function</span>(formula, data, indices) {</span>
<span id="cb538-3"><a href="regression.html#cb538-3"></a>    d &lt;-<span class="st"> </span>data[indices, ]  <span class="co"># allows boot to select sample</span></span>
<span id="cb538-4"><a href="regression.html#cb538-4"></a>    fit &lt;-<span class="st"> </span><span class="kw">lm</span>(formula, <span class="dt">data =</span> d)</span>
<span id="cb538-5"><a href="regression.html#cb538-5"></a>    <span class="kw">return</span>(<span class="kw">coef</span>(fit))</span>
<span id="cb538-6"><a href="regression.html#cb538-6"></a>}</span></code></pre></div>
<p>Now that we have specified this function, we can use it within the <code>boot</code> function to obtained the bootstrapped results for the regression coefficients. To do this, we first load the <code>boot</code> package and use the <code>boot</code> function by specifying the following arguments:</p>
<ul>
<li><strong>data</strong>: the data set we use (the regression data set in our example)</li>
<li><strong>statistic</strong>: the statistic(s) we would like to bootstrap (in our example, we use the function we have specified above to obtain the regression coefficients)</li>
<li><strong>R</strong>: the number of bootstrap samples to use (we will use 2000 samples)</li>
<li><strong>formula</strong>: the regression equation from which we obtain the coefficients for the bootstrapping procedure</li>
</ul>
<p>We create an object called <code>boot_out</code>, which contains the output from the bootstrapping.</p>
<div class="sourceCode" id="cb539"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb539-1"><a href="regression.html#cb539-1"></a><span class="co"># If the residuals do not follow a normal</span></span>
<span id="cb539-2"><a href="regression.html#cb539-2"></a><span class="co"># distribution, transform the data or use</span></span>
<span id="cb539-3"><a href="regression.html#cb539-3"></a><span class="co"># bootstrapping</span></span>
<span id="cb539-4"><a href="regression.html#cb539-4"></a><span class="kw">library</span>(boot)</span>
<span id="cb539-5"><a href="regression.html#cb539-5"></a><span class="co"># bootstrapping with 2000 replications</span></span>
<span id="cb539-6"><a href="regression.html#cb539-6"></a>boot_out &lt;-<span class="st"> </span><span class="kw">boot</span>(<span class="dt">data =</span> regression, <span class="dt">statistic =</span> bs, </span>
<span id="cb539-7"><a href="regression.html#cb539-7"></a>    <span class="dt">R =</span> <span class="dv">2000</span>, <span class="dt">formula =</span> sales <span class="op">~</span><span class="st"> </span>adspend <span class="op">+</span><span class="st"> </span>airplay <span class="op">+</span><span class="st"> </span></span>
<span id="cb539-8"><a href="regression.html#cb539-8"></a><span class="st">        </span>starpower)</span></code></pre></div>
<p>In a next step, let’s extract the 95% confidence intervals for the regression coefficients. The intercept is the first element in the <code>boot_out</code> object and we can use the <code>boot.ci()</code> function and use argument <code>index=1</code> to obtain the bootstrapped confidence interval for the intercept. The <code>type</code> argument specifies the type of confidence interval we would like to obtain (in this case, we use bias corrected and accelerated, i.e., bca):</p>
<div class="sourceCode" id="cb540"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb540-1"><a href="regression.html#cb540-1"></a><span class="co"># get 95% confidence intervals</span></span>
<span id="cb540-2"><a href="regression.html#cb540-2"></a><span class="kw">boot.ci</span>(boot_out, <span class="dt">type =</span> <span class="st">&quot;bca&quot;</span>, <span class="dt">index =</span> <span class="dv">1</span>)  <span class="co"># intercept</span></span></code></pre></div>
<pre><code>## BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS
## Based on 2000 bootstrap replicates
## 
## CALL : 
## boot.ci(boot.out = boot_out, type = &quot;bca&quot;, index = 1)
## 
## Intervals : 
## Level       BCa          
## 95%   (-57.3,   7.8 )  
## Calculations and Intervals on Original Scale</code></pre>
<p>We can obtain the confidence intervals for the remaining coefficients in the same way:</p>
<div class="sourceCode" id="cb542"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb542-1"><a href="regression.html#cb542-1"></a><span class="co"># get 95% confidence intervals</span></span>
<span id="cb542-2"><a href="regression.html#cb542-2"></a><span class="kw">boot.ci</span>(boot_out, <span class="dt">type =</span> <span class="st">&quot;bca&quot;</span>, <span class="dt">index =</span> <span class="dv">2</span>)  <span class="co"># adspend</span></span></code></pre></div>
<pre><code>## BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS
## Based on 2000 bootstrap replicates
## 
## CALL : 
## boot.ci(boot.out = boot_out, type = &quot;bca&quot;, index = 2)
## 
## Intervals : 
## Level       BCa          
## 95%   ( 0.071,  0.098 )  
## Calculations and Intervals on Original Scale</code></pre>
<div class="sourceCode" id="cb544"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb544-1"><a href="regression.html#cb544-1"></a><span class="kw">boot.ci</span>(boot_out, <span class="dt">type =</span> <span class="st">&quot;bca&quot;</span>, <span class="dt">index =</span> <span class="dv">3</span>)  <span class="co"># airplay</span></span></code></pre></div>
<pre><code>## BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS
## Based on 2000 bootstrap replicates
## 
## CALL : 
## boot.ci(boot.out = boot_out, type = &quot;bca&quot;, index = 3)
## 
## Intervals : 
## Level       BCa          
## 95%   ( 2.7,  3.9 )  
## Calculations and Intervals on Original Scale</code></pre>
<div class="sourceCode" id="cb546"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb546-1"><a href="regression.html#cb546-1"></a><span class="kw">boot.ci</span>(boot_out, <span class="dt">type =</span> <span class="st">&quot;bca&quot;</span>, <span class="dt">index =</span> <span class="dv">4</span>)  <span class="co"># starpower</span></span></code></pre></div>
<pre><code>## BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS
## Based on 2000 bootstrap replicates
## 
## CALL : 
## boot.ci(boot.out = boot_out, type = &quot;bca&quot;, index = 4)
## 
## Intervals : 
## Level       BCa          
## 95%   ( 6.5, 15.3 )  
## Calculations and Intervals on Original Scale</code></pre>
<p>As usual, we can judge the significance of a coefficient by inspecting whether the null hypothesis (0 in this case) is contained in the intervals. As can be seen, zero is not included in any of the intervals leading us to conclude that all of the predictor variables have a significant effect on the outcome variable.</p>
<p>We could also compare the bootstrapped confidence intervals to the once we obtained from the model without bootstrapping.</p>
<div class="sourceCode" id="cb548"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb548-1"><a href="regression.html#cb548-1"></a><span class="co"># get 95% confidence intervals for standard model</span></span>
<span id="cb548-2"><a href="regression.html#cb548-2"></a><span class="kw">confint</span>(multiple_regression)</span></code></pre></div>
<pre><code>##               2.5 % 97.5 %
## (Intercept) -60.830  7.604
## adspend       0.071  0.099
## airplay       2.820  3.915
## starpower     6.279 15.894</code></pre>
<p>As you can see, the bootstrapped confidence interval is very similar to the once obtained without bootstrapping, which is not unexpected since in our example, the tests indicated that our assumptions about the distribution of errors is actually met so that we wouldn’t have needed to apply the bootstrapping. You could also inspect the distribution of the obtained regression coefficients from the 2000 bootstrap samples using the <code>plot()</code> function and passing it the respective index. Inspecting the plots reveals that for all coefficients (with the exception of the intercept) zero is not contained in the range of plausible values, indicating the the coefficients are significant at the 5% level.</p>
<div class="sourceCode" id="cb550"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb550-1"><a href="regression.html#cb550-1"></a><span class="kw">plot</span>(boot_out, <span class="dt">index =</span> <span class="dv">1</span>)  <span class="co"># intercept</span></span></code></pre></div>
<p><img src="10-Regression_files/figure-html/unnamed-chunk-57-1.png" width="672" /></p>
<div class="sourceCode" id="cb551"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb551-1"><a href="regression.html#cb551-1"></a><span class="kw">plot</span>(boot_out, <span class="dt">index =</span> <span class="dv">2</span>)  <span class="co"># adspend</span></span></code></pre></div>
<p><img src="10-Regression_files/figure-html/unnamed-chunk-57-2.png" width="672" /></p>
<div class="sourceCode" id="cb552"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb552-1"><a href="regression.html#cb552-1"></a><span class="kw">plot</span>(boot_out, <span class="dt">index =</span> <span class="dv">3</span>)  <span class="co"># airplay</span></span></code></pre></div>
<p><img src="10-Regression_files/figure-html/unnamed-chunk-57-3.png" width="672" /></p>
<div class="sourceCode" id="cb553"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb553-1"><a href="regression.html#cb553-1"></a><span class="kw">plot</span>(boot_out, <span class="dt">index =</span> <span class="dv">4</span>)  <span class="co"># starpower</span></span></code></pre></div>
<p><img src="10-Regression_files/figure-html/unnamed-chunk-57-4.png" width="672" /></p>
<div class="infobox_orange hint">
<p>To summarize, you can inspect if the assumption of normally distributed errors is violated by visually examining the QQ-plot and using the Shapiro-Wilk test. If the results suggest a non-normal distribution of the errors, you should first try to transform your data (e.g., by using a log-transformation). If this doesn’t solve the issue, you should apply the bootstrapping procedure as shown above to obtain a robust test of the significance of the regression coefficients.</p>
</div>
</div>
<div id="correlation-of-errors" class="section level3">
<h3><span class="header-section-number">7.3.6</span> Correlation of errors</h3>
<p>The assumption of independent errors implies that for any two observations the residual terms should be uncorrelated. This is also known as a <em>lack of autocorrelation</em>. In theory, this could be tested with the Durbin-Watson test, which checks whether adjacent residuals are correlated. However, be aware that the test is sensitive to the order of your data. Hence, it only makes sense if there is a natural order in the data (e.g., time-series data) when the presence of dependent errors indicates autocorrelation. <strong>Since there is no natural order in our data, we don’t need to apply this test</strong>.</p>
<p>If you are confronted with data that has a natural order, you can performed the test using the command <code>durbinWatsonTest()</code>, which takes the object that the <code>lm()</code> function generates as an argument. The test statistic varies between 0 and 4, with values close to 2 being desirable. As a rule of thumb values below 1 and above 3 are causes for concern.</p>
</div>
<div id="collinearity" class="section level3">
<h3><span class="header-section-number">7.3.7</span> Collinearity</h3>
<div align="center">
<iframe width="560" height="315" src="https://www.youtube.com/embed/9TV2nqsBwMk" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>
</iframe>
</div>
<p>Linear dependence of regressors, also known as multicollinearity, is when there is a strong linear relationship between the independent variables. Some correlation will always be present, but severe correlation can make proper estimation impossible. When present, it affects the model in several ways:</p>
<ul>
<li>Limits the size of R<sup>2</sup>: when two variables are highly correlated, the amount of unique explained variance is low; therefore the incremental change in R<sup>2</sup> by including an additional predictor is larger if the predictor is uncorrelated with the other predictors.</li>
<li>Increases the standard errors of the coefficients, making them less trustworthy.</li>
<li>Uncertainty about the importance of predictors: if two predictors explain similar variance in the outcome, we cannot know which of these variables is important.</li>
</ul>
<p>A quick way to find obvious multicollinearity is to examine the correlation matrix of the data. Any value &gt; 0.8 - 0.9 should be cause for concern. You can, for example, create a correlation matrix using the <code>rcorr()</code> function from the <code>Hmisc</code> package.</p>
<div class="sourceCode" id="cb554"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb554-1"><a href="regression.html#cb554-1"></a><span class="kw">library</span>(<span class="st">&quot;Hmisc&quot;</span>)</span>
<span id="cb554-2"><a href="regression.html#cb554-2"></a><span class="kw">rcorr</span>(<span class="kw">as.matrix</span>(regression[, <span class="kw">c</span>(<span class="st">&quot;adspend&quot;</span>, <span class="st">&quot;airplay&quot;</span>, </span>
<span id="cb554-3"><a href="regression.html#cb554-3"></a>    <span class="st">&quot;starpower&quot;</span>)]))</span></code></pre></div>
<pre><code>##           adspend airplay starpower
## adspend      1.00    0.10      0.08
## airplay      0.10    1.00      0.18
## starpower    0.08    0.18      1.00
## 
## n= 200 
## 
## 
## P
##           adspend airplay starpower
## adspend           0.1511  0.2557   
## airplay   0.1511          0.0099   
## starpower 0.2557  0.0099</code></pre>
<p>The bivariate correlations can also be show in a plot:</p>
<div class="sourceCode" id="cb556"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb556-1"><a href="regression.html#cb556-1"></a><span class="kw">plot</span>(regression[, <span class="kw">c</span>(<span class="st">&quot;adspend&quot;</span>, <span class="st">&quot;airplay&quot;</span>, <span class="st">&quot;starpower&quot;</span>)])</span></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-59"></span>
<img src="10-Regression_files/figure-html/unnamed-chunk-59-1.png" alt="Bivariate correlation plots" width="672" />
<p class="caption">
Figure 7.3: Bivariate correlation plots
</p>
</div>
<p>However, this only spots bivariate multicollinearity. Variance inflation factors can be used to spot more subtle multicollinearity arising from multivariate relationships. It is calculated by regressing X<sub>i</sub> on all other X and using the resulting R<sup>2</sup> to calculate</p>
<p><span class="math display" id="eq:VIF">\[\begin{equation} 
\begin{split}
\frac{1}{1 - R_i^2}
\end{split}
\tag{7.19}
\end{equation}\]</span></p>
<p>VIF values of over 4 are certainly cause for concern and values over 2 should be further investigated. If the average VIF is over 1 the regression may be biased. The VIF for all variables can easily be calculated in R with the <code>vif()</code> function contained in the <code>car</code> package.</p>
<div class="sourceCode" id="cb557"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb557-1"><a href="regression.html#cb557-1"></a><span class="kw">library</span>(car)</span>
<span id="cb557-2"><a href="regression.html#cb557-2"></a><span class="kw">vif</span>(multiple_regression)</span></code></pre></div>
<pre><code>##   adspend   airplay starpower 
##         1         1         1</code></pre>
<p>As you can see the values are well below the cutoff, indicating that we do not have to worry about multicollinearity in our example. If multicollinearity turns out to be an issue in your analysis, there are at least two ways to proceed. First, you could eliminate one of the predictors, e.g., by using variable selection procedures which will be covered below. Second, you could combine predictors that are highly correlated using statistical methods aiming at reducing the dimensionality of the data based on the correlation matrix, such as the Pricipal Component Analysis (PCA), which will be covered in the next chapter.</p>
<div class="infobox_orange hint">
<p>To summarize, you can inspect if the assumption of multicollinearity is violated by inspecting the variance inflation factor associated with the regression coefficients. Values over 4 are a cause for concern. In case multicollinearity turns out to be an issue, you can address it by 1) eliminating one of the regressors (e.g., using variable selection procedures) or combining variables that are highly correlated in one factor (e.g., using Principal Component Analysis).</p>
</div>
</div>
<div id="omitted-variables" class="section level3">
<h3><span class="header-section-number">7.3.8</span> Omitted Variables</h3>
<div align="center">
<iframe width="560" height="315" src="https://www.youtube.com/embed/sVPu3f0log4" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>
</iframe>
</div>
<p>If the goal of your analysis is to explain the effect of one variable on an outcome (rather than just predicting an outcome), one main concern that you need to be aware of is related to omitted variables. This issue relates back to the choice of research design. If you are interested in causal inference and you did not obtain your data from a randomized trial, the issue of omitted variables bias is of great importance. If your goal is to make predictions, you don’t need to worry about this too much - in this case other potential problems such as overfitting (see below) should receive more attention.</p>
<p>What do we mean by “omitted variables”? If a variable that influences the outcome is left out of the model (i.e., it is “omitted”), a bias in other variables’ coefficients might be introduced. Specifically, the other coefficients will be biased if the omitted variable influences the outcome <em>and</em> the independent variable(s) in your model. Intuitively, the variables left in the model “pick up” the effect of the omitted variable to the degree that they are related. Let’s illustrate this with an example.</p>
<p>Consider the following data set containing information on the number of streams that a sample of artists receive on a streaming service in one month.</p>
<div class="sourceCode" id="cb559"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb559-1"><a href="regression.html#cb559-1"></a><span class="kw">head</span>(streaming_data)</span></code></pre></div>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["popularity"],"name":[1],"type":["dbl"],"align":["right"]},{"label":["playlists"],"name":[2],"type":["dbl"],"align":["right"]},{"label":["streams"],"name":[3],"type":["dbl"],"align":["right"]}],"data":[{"1":"2.88","2":"8","3":"1160"},{"1":"7.88","2":"99","3":"1538"},{"1":"4.09","2":"31","3":"1232"},{"1":"8.83","2":"58","3":"1464"},{"1":"9.40","2":"5","3":"1442"},{"1":"0.46","2":"4","3":"1159"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<p>The data set contains three variables:</p>
<ul>
<li><strong>popularity</strong>: The average popularity rating of an artist measured on a scale from 0-10</li>
<li><strong>playlists</strong>: The number of playlists the artist is listen on</li>
<li><strong>streams</strong>: The number of streams an artist generates during the observation month (in thousands)</li>
</ul>
<p>Say, as a marketing manager we are interested in estimating the effect of the number of playlists on the number of streams. If we estimate a model to explain the number of streams as a function of only the number of playlists, the results would look as follows:</p>
<div class="sourceCode" id="cb560"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb560-1"><a href="regression.html#cb560-1"></a>stream_model_<span class="dv">1</span> &lt;-<span class="st"> </span><span class="kw">lm</span>(streams <span class="op">~</span><span class="st"> </span>playlists, <span class="dt">data =</span> streaming_data)</span>
<span id="cb560-2"><a href="regression.html#cb560-2"></a><span class="kw">summary</span>(stream_model_<span class="dv">1</span>)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = streams ~ playlists, data = streaming_data)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -394.7  -87.9   -5.6   92.9  334.5 
## 
## Coefficients:
##             Estimate Std. Error t value            Pr(&gt;|t|)    
## (Intercept) 1248.065     15.814    78.9 &lt;0.0000000000000002 ***
## playlists      3.032      0.198    15.3 &lt;0.0000000000000002 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 135 on 170 degrees of freedom
## Multiple R-squared:  0.581,  Adjusted R-squared:  0.578 
## F-statistic:  235 on 1 and 170 DF,  p-value: &lt;0.0000000000000002</code></pre>
<p>As you can see, the results suggest that being listed on one more playlist leads to 3,032 more streams on average (recall that the dependent variable is given in thousands in this case, so we need multiply the coefficient by 1,000 to obtain the effect). Now let’s see what happens when we add the popularity of an artist as an additional predictor:</p>
<div class="sourceCode" id="cb562"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb562-1"><a href="regression.html#cb562-1"></a>stream_model_<span class="dv">2</span> &lt;-<span class="st"> </span><span class="kw">lm</span>(streams <span class="op">~</span><span class="st"> </span>playlists <span class="op">+</span><span class="st"> </span>popularity, </span>
<span id="cb562-2"><a href="regression.html#cb562-2"></a>    <span class="dt">data =</span> streaming_data)</span>
<span id="cb562-3"><a href="regression.html#cb562-3"></a><span class="kw">summary</span>(stream_model_<span class="dv">2</span>)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = streams ~ playlists + popularity, data = streaming_data)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -278.94  -62.28    7.23   72.35  246.51 
## 
## Coefficients:
##             Estimate Std. Error t value            Pr(&gt;|t|)    
## (Intercept) 1120.721     17.124    65.5 &lt;0.0000000000000002 ***
## playlists      1.923      0.185    10.4 &lt;0.0000000000000002 ***
## popularity    37.782      3.545    10.7 &lt;0.0000000000000002 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 105 on 169 degrees of freedom
## Multiple R-squared:  0.749,  Adjusted R-squared:  0.746 
## F-statistic:  252 on 2 and 169 DF,  p-value: &lt;0.0000000000000002</code></pre>
<p>What happens to the coefficient of <code>playlists</code>? As you can see, the magnitude of the coefficient decreased substantially. Because the popularity of an artist influences both the number of playlists (more popular artists are listed on a larger number of playlists) <em>and</em> the number of streams (more popular artists receive more streams), the coefficient will be biased upwards. In this case, the popularity of an artists is said to be an <strong>unobserved confounder</strong> if it is not included in the model and the playlist variable is referred to as an endogenous predictor (i.e., the assumption of exogeneity is violated). As you could see, this unobserved confounder would lead us to overestimate the effect of playlists on the number of streams. It is therefore crucially important that you carefully consider what other factors could explain the dependent variable besides your main independent variable of interest. This is also the reason why it is much more difficult to estimate causal effects from observational data compared to randomized experiments, where you could, e.g., randomly assign artists to be included on playlists or not. But in real life, it is often not feasible to run field experiments, e.g., because we may not have control over which artists get included on a playlists and which artists don’t.</p>
<div class="infobox_orange hint">
<p>To summarize, if your goal is to identify a causal effect of one variable on another variable using observational (non-experimental) data, you need to carefully think about which potentially omitting variable may influence both the dependent and independent variable in your model. Unfortunately, there is no test that would tell you if you have indeed included all variables in your model so that you need to put forth arguments why you think that unobserved confounders are not a reason for concern in your analysis.</p>
</div>
</div>
<div id="overfitting" class="section level3">
<h3><span class="header-section-number">7.3.9</span> Overfitting</h3>
<div align="center">
<iframe width="560" height="315" src="https://www.youtube.com/embed/G1hDFXZa9CQ" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>
</iframe>
</div>
<p>If the goal of your analysis is to predict an outcome, rather than explaining a causal effect of one variable on another variable, the issue of overfitting is a major concern. Overfitting basically means that your model is tuned so much to the specific data set that you have used to estimate the model that it won’t perform well when you would like to use the model to predict observations outside of your sample. So far, we have considered ‘in-sample’ statistics (e.g., R2) to judge the model fit. However, when the model building purpose is forecasting, we should test the predictive performance of the model on new data (i.e., ‘out-of-sample’ prediction). We can easily achieve this by splitting the sample in two parts:</p>
<ul>
<li><strong>Training data set</strong>: used to estimate the model parameters</li>
<li><strong>Test data set (‘hold-out set’)</strong>: predict values based training data</li>
</ul>
<p>By inspecting how well the model based on the training data is able to predict the observations in the test data, we can judge the predictive ability of a model on new data. This is also referred to as the out-of-sample predictive accuracy. You can easily test the out-of-sample predictive accuracy of your model by splitting the data set in two parts - the training data and the test data. In the following code, we randomly 2/3 of the observations to estimate the model and retain the remaining 1/3 of observations to test how well we can predict the outcome based on our model.</p>
<div class="sourceCode" id="cb564"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb564-1"><a href="regression.html#cb564-1"></a><span class="co"># randomly split into training and test data:</span></span>
<span id="cb564-2"><a href="regression.html#cb564-2"></a><span class="kw">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb564-3"><a href="regression.html#cb564-3"></a>n &lt;-<span class="st"> </span><span class="kw">nrow</span>(regression)</span>
<span id="cb564-4"><a href="regression.html#cb564-4"></a>train &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="dv">1</span><span class="op">:</span>n, <span class="kw">round</span>(n <span class="op">*</span><span class="st"> </span><span class="dv">2</span><span class="op">/</span><span class="dv">3</span>))</span>
<span id="cb564-5"><a href="regression.html#cb564-5"></a>test &lt;-<span class="st"> </span>(<span class="dv">1</span><span class="op">:</span>n)[<span class="op">-</span>train]</span></code></pre></div>
<p>Now we have created two data sets - the training data set and the test data set. Next, let’s estimate the model using the training data and inspect the results</p>
<div class="sourceCode" id="cb565"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb565-1"><a href="regression.html#cb565-1"></a><span class="co"># estimate linear model based on training data</span></span>
<span id="cb565-2"><a href="regression.html#cb565-2"></a>multiple_train &lt;-<span class="st"> </span><span class="kw">lm</span>(sales <span class="op">~</span><span class="st"> </span>adspend <span class="op">+</span><span class="st"> </span>airplay <span class="op">+</span><span class="st"> </span>starpower, </span>
<span id="cb565-3"><a href="regression.html#cb565-3"></a>    <span class="dt">data =</span> regression, <span class="dt">subset =</span> train)</span>
<span id="cb565-4"><a href="regression.html#cb565-4"></a><span class="kw">summary</span>(multiple_train)  <span class="co">#summary of results</span></span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = sales ~ adspend + airplay + starpower, data = regression, 
##     subset = train)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -109.89  -30.21   -0.16   28.19  146.84 
## 
## Coefficients:
##              Estimate Std. Error t value            Pr(&gt;|t|)    
## (Intercept) -36.52425   20.15595   -1.81              0.0723 .  
## adspend       0.09015    0.00868   10.38 &lt;0.0000000000000002 ***
## airplay       3.49391    0.35649    9.80 &lt;0.0000000000000002 ***
## starpower    11.22697    2.93189    3.83              0.0002 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 47 on 129 degrees of freedom
## Multiple R-squared:   0.7,   Adjusted R-squared:  0.693 
## F-statistic:  100 on 3 and 129 DF,  p-value: &lt;0.0000000000000002</code></pre>
<p>As you can see, the model results are similar to the results from the model from the beginning, which is not surprising since it is based on the same data with the only difference that we discarded 1/3 of the observations for validation purposes. In a next step, we can use the <code>predict()</code> function and the argument <code>newdata</code> to predict observations in the test data set based on our estimated model. To test the our-of-sample predictive accuracy of the model, we can then compute the squared correlation coefficient between the predicted and observed values, which will give us the out-of-sample model fit (i.e., the R2).</p>
<div class="sourceCode" id="cb567"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb567-1"><a href="regression.html#cb567-1"></a><span class="co"># using coefficients to predict test data</span></span>
<span id="cb567-2"><a href="regression.html#cb567-2"></a>pred_lm &lt;-<span class="st"> </span><span class="kw">predict</span>(multiple_train, <span class="dt">newdata =</span> regression[test, </span>
<span id="cb567-3"><a href="regression.html#cb567-3"></a>    ])</span>
<span id="cb567-4"><a href="regression.html#cb567-4"></a><span class="kw">cor</span>(regression[test, <span class="st">&quot;sales&quot;</span>], pred_lm)<span class="op">^</span><span class="dv">2</span>  <span class="co"># R^2 for test data</span></span></code></pre></div>
<pre><code>## [1] 0.57</code></pre>
<p>As you can see, the R2 is about 0.57, suggesting that 57% of the variation in the test data set can be explained by our model. Note that this share is somewhat lower compared to the within-sample fit that you can see in the model above (i.e., R2 = 0.7), which is not unexpected since the test data were not used to estimate the model. The value of 0.57 suggests that the model generalizes to other data sets quite well. We could also visualize the out-of-sample model fit as follows:</p>
<div class="sourceCode" id="cb569"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb569-1"><a href="regression.html#cb569-1"></a><span class="co"># plot predicted vs. observed values for test data</span></span>
<span id="cb569-2"><a href="regression.html#cb569-2"></a><span class="kw">plot</span>(regression[test, <span class="st">&quot;sales&quot;</span>], pred_lm, <span class="dt">xlab =</span> <span class="st">&quot;y measured&quot;</span>, </span>
<span id="cb569-3"><a href="regression.html#cb569-3"></a>    <span class="dt">ylab =</span> <span class="st">&quot;y predicted&quot;</span>, <span class="dt">cex.lab =</span> <span class="fl">1.3</span>)</span>
<span id="cb569-4"><a href="regression.html#cb569-4"></a><span class="kw">abline</span>(<span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">1</span>))</span></code></pre></div>
<p><img src="10-Regression_files/figure-html/unnamed-chunk-68-1.png" width="672" /></p>
<div class="infobox_orange hint">
<p>To summarize, overfitting is a concern in predictive models and it means that your model is so highly tuned to the data set you used to estimate the model that it does not generalize well to new data. To make sure that overfitting is not a reason for concern, you should test the out-of-sample predictive ability of your model using the process explained above.</p>
</div>
</div>
<div id="variable-selection" class="section level3">
<h3><span class="header-section-number">7.3.10</span> Variable selection</h3>
<div align="center">
<iframe width="560" height="315" src="https://www.youtube.com/embed/SODPyrVruB8" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>
</iframe>
</div>
<p>A <em>parsimonious</em> model is a model that accomplishes a desired level of explanation or prediction with as few predictor variables as possible. Parsimonious models are desired because smaller models are easier to interpret and redundant or unnecessary variables should be left out (also to combat overfitting). Note that this is especially true for predictive models. If our goal was to explain a relationship between two variables and the variable we formulated a hypothesis for turns out to be insignificant, this result would also be interesting and, hence, you should also report the effect even though it may be insignificant.</p>
<p>Let’s use an example to see how we could identify variables that do not add any explanatory power to the model and should thus be excluded from the model. First, we will add a random variable to our existing regression data set. We can use the <code>rnorm()</code> function to generate random observations from a normal distribution.</p>
<div class="sourceCode" id="cb570"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb570-1"><a href="regression.html#cb570-1"></a><span class="kw">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb570-2"><a href="regression.html#cb570-2"></a><span class="co"># Add another random variable</span></span>
<span id="cb570-3"><a href="regression.html#cb570-3"></a>regression<span class="op">$</span>var_test &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="kw">nrow</span>(regression), <span class="dv">0</span>, <span class="dv">1</span>)</span></code></pre></div>
<p>The model selection process should tell us that we should favor a model without this predictor since it does not add any explanatory power. In the following code, we specify our model by gradually adding one predictor after the other and then we will use the <code>anova()</code> function to test the differences between the models.</p>
<div class="sourceCode" id="cb571"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb571-1"><a href="regression.html#cb571-1"></a><span class="co"># Model comparison with anova</span></span>
<span id="cb571-2"><a href="regression.html#cb571-2"></a>lm0 &lt;-<span class="st"> </span><span class="kw">lm</span>(sales <span class="op">~</span><span class="st"> </span><span class="dv">1</span>, <span class="dt">data =</span> regression)</span>
<span id="cb571-3"><a href="regression.html#cb571-3"></a>lm1 &lt;-<span class="st"> </span><span class="kw">lm</span>(sales <span class="op">~</span><span class="st"> </span>adspend, <span class="dt">data =</span> regression)</span>
<span id="cb571-4"><a href="regression.html#cb571-4"></a>lm2 &lt;-<span class="st"> </span><span class="kw">lm</span>(sales <span class="op">~</span><span class="st"> </span>adspend <span class="op">+</span><span class="st"> </span>airplay, <span class="dt">data =</span> regression)</span>
<span id="cb571-5"><a href="regression.html#cb571-5"></a>lm3 &lt;-<span class="st"> </span><span class="kw">lm</span>(sales <span class="op">~</span><span class="st"> </span>adspend <span class="op">+</span><span class="st"> </span>airplay <span class="op">+</span><span class="st"> </span>starpower, <span class="dt">data =</span> regression)</span>
<span id="cb571-6"><a href="regression.html#cb571-6"></a>lm4 &lt;-<span class="st"> </span><span class="kw">lm</span>(sales <span class="op">~</span><span class="st"> </span>adspend <span class="op">+</span><span class="st"> </span>airplay <span class="op">+</span><span class="st"> </span>starpower <span class="op">+</span><span class="st"> </span>var_test, </span>
<span id="cb571-7"><a href="regression.html#cb571-7"></a>    <span class="dt">data =</span> regression)</span>
<span id="cb571-8"><a href="regression.html#cb571-8"></a><span class="kw">anova</span>(lm0, lm1, lm2, lm3, lm4)</span></code></pre></div>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["Res.Df"],"name":[1],"type":["dbl"],"align":["right"]},{"label":["RSS"],"name":[2],"type":["dbl"],"align":["right"]},{"label":["Df"],"name":[3],"type":["dbl"],"align":["right"]},{"label":["Sum of Sq"],"name":[4],"type":["dbl"],"align":["right"]},{"label":["F"],"name":[5],"type":["dbl"],"align":["right"]},{"label":["Pr(>F)"],"name":[6],"type":["dbl"],"align":["right"]}],"data":[{"1":"199","2":"1295952","3":"NA","4":"NA","5":"NA","6":"NA"},{"1":"198","2":"862264","3":"1","4":"433688","5":"195.9","6":"0.00000000000000000000000000000028"},{"1":"197","2":"480428","3":"1","4":"381836","5":"172.5","6":"0.00000000000000000000000000012130"},{"1":"196","2":"434575","3":"1","4":"45853","5":"20.7","6":"0.00000936444748619836851190323390"},{"1":"195","2":"431640","3":"1","4":"2935","5":"1.3","6":"0.25095438927991031707875890788273"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<p>The output shows that the last model, in which we add the random variable as an additional predictor, does not significantly improve the model compared to the previous model (the results from the F-test is insignificant). Hence, we should use model “lm3” in this case.</p>
<p>Alternatively, we could also select the variables to be included in our model using a stepwise procedure with the <code>step()</code> function. To do this, we pass the most complete model from above, i.e., ‘lm4’ to the function and inspect the results:</p>
<div class="sourceCode" id="cb572"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb572-1"><a href="regression.html#cb572-1"></a><span class="kw">options</span>(<span class="dt">digits =</span> <span class="dv">8</span>)</span>
<span id="cb572-2"><a href="regression.html#cb572-2"></a><span class="co"># Stepwise variable selection Automatic model</span></span>
<span id="cb572-3"><a href="regression.html#cb572-3"></a><span class="co"># selection with step</span></span>
<span id="cb572-4"><a href="regression.html#cb572-4"></a>model_lmstep &lt;-<span class="st"> </span><span class="kw">step</span>(lm4)</span></code></pre></div>
<pre><code>## Start:  AIC=1545.41
## sales ~ adspend + airplay + starpower + var_test
## 
##             Df Sum of Sq    RSS     AIC
## - var_test   1      2935 434575 1544.76
## &lt;none&gt;                   431640 1545.41
## - starpower  1     41322 472962 1561.69
## - airplay    1    318263 749902 1653.88
## - adspend    1    335612 767252 1658.45
## 
## Step:  AIC=1544.76
## sales ~ adspend + airplay + starpower
## 
##             Df Sum of Sq    RSS     AIC
## &lt;none&gt;                   434575 1544.76
## - starpower  1     45853 480428 1562.82
## - airplay    1    325860 760434 1654.67
## - adspend    1    333332 767907 1656.62</code></pre>
<div class="sourceCode" id="cb574"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb574-1"><a href="regression.html#cb574-1"></a>model_lmstep</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = sales ~ adspend + airplay + starpower, data = regression)
## 
## Coefficients:
## (Intercept)      adspend      airplay    starpower  
##  -26.612958     0.084885     3.367425    11.086335</code></pre>
<p>In this case, the model selection is based on the information criterion <em>AIC</em>, which is defined as <span class="math inline">\(AIC=-2*maxLL+2k\)</span>, where LL refers to the log-likelihood and <span class="math inline">\(k\)</span> denotes the number of parameters in the model. Maximizing the log-likelihood-function corresponds to a minimization of the residual sum of squares RSS (i.e., OLS estimator). Information criteria based on the maximized log-likelihood (e.g., AIC) provide an estimate of model parsimony, i.e., resolve the trade-off between model fit and model complexity, to achieve the best predictive ability (similar to adjusted R2). They include a penalty for model complexity and penalize overly complex models (i.e., <span class="math inline">\(k\)</span> in case of the AIC), where complexity refers to the number of parameters in the model. The model with lowest AIC value is the most parsimonious. The information criteria and LL statistics should not be interpreted in absolute terms, but rather in comparison to nested model specifications.</p>
<p>The output above indicates that removing the <code>var_test</code> variable would change the AIC from 1545.41 to 1544.76 (i.e., lead to a more parsimonious model). For all the other predictor variables, the results indicate that removing these variables would actually lead to a higher AIC statistic (i.e., leading to a less parsimonious model). Similar to the conclusion for the comparison above, this model would correctly suggest to drop the <code>var_test</code> variable and retain all the other variables in the model.</p>
<div class="infobox_orange hint">
<p>To summarize, to obtain a parsimonious model, you should test if any of the model variables could be left out without decreasing the fit of your model using the procedures explained above.</p>
</div>
</div>
</div>
<div id="categorical-predictors" class="section level2">
<h2><span class="header-section-number">7.4</span> Categorical predictors</h2>
<div align="center">
<iframe width="560" height="315" src="https://www.youtube.com/embed/Zttj2HWFL2M" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>
</iframe>
</div>
<div id="two-categories" class="section level3">
<h3><span class="header-section-number">7.4.1</span> Two categories</h3>
<p>Suppose, you wish to investigate the effect of the variable “country” on sales, which is a categorical variable that can only take two levels (i.e., 0 = local artist, 1 = international artist). Categorical variables with two levels are also called binary predictors. It is straightforward to include these variables in your model as “dummy” variables. Dummy variables are factor variables that can only take two values. For our “country” variable, we can create a new predictor variable that takes the form:</p>
<p><span class="math display" id="eq:dummycoding">\[\begin{equation} 
x_4 =
  \begin{cases}
    1       &amp; \quad \text{if } i \text{th artist is international}\\
    0  &amp; \quad \text{if } i \text{th artist is local}
  \end{cases}
\tag{7.20}
\end{equation}\]</span></p>
<p>This new variable is then added to our regression equation from before, so that the equation becomes</p>
<p><span class="math display">\[\begin{align}
Sales =\beta_0 &amp;+\beta_1*adspend\\
      &amp;+\beta_2*airplay\\
      &amp;+\beta_3*starpower\\ 
      &amp;+\beta_4*international+\epsilon
\end{align}\]</span></p>
<p>where “international” represents the new dummy variable and <span class="math inline">\(\beta_4\)</span> is the coefficient associated with this variable. Estimating the model is straightforward - you just need to include the variable as an additional predictor variable. Note that the variable needs to be specified as a factor variable before including it in your model. If you haven’t converted it to a factor variable before, you could also use the wrapper function <code>as.factor()</code> within the equation.</p>
<div class="sourceCode" id="cb576"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb576-1"><a href="regression.html#cb576-1"></a>multiple_regression_bin &lt;-<span class="st"> </span><span class="kw">lm</span>(sales <span class="op">~</span><span class="st"> </span>adspend <span class="op">+</span><span class="st"> </span>airplay <span class="op">+</span><span class="st"> </span></span>
<span id="cb576-2"><a href="regression.html#cb576-2"></a><span class="st">    </span>starpower <span class="op">+</span><span class="st"> </span>country, <span class="dt">data =</span> regression)  <span class="co">#estimate linear model</span></span>
<span id="cb576-3"><a href="regression.html#cb576-3"></a><span class="kw">summary</span>(multiple_regression_bin)  <span class="co">#summary of results</span></span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = sales ~ adspend + airplay + starpower + country, 
##     data = regression)
## 
## Residuals:
##       Min        1Q    Median        3Q       Max 
## -109.1987  -24.2970   -1.8219   29.1854  156.3111 
## 
## Coefficients:
##                         Estimate  Std. Error t value              Pr(&gt;|t|)    
## (Intercept)          -16.4006000  16.3953963 -1.0003                0.3184    
## adspend                0.0814637   0.0065285 12.4781 &lt; 0.00000000000000022 ***
## airplay                3.0376582   0.2680852 11.3309 &lt; 0.00000000000000022 ***
## starpower             10.0809967   2.2954638  4.3917          0.0000184294 ***
## countryinternational  45.6727416   8.6911720  5.2551          0.0000003862 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 44.183 on 195 degrees of freedom
## Multiple R-squared:  0.70627,    Adjusted R-squared:  0.70024 
## F-statistic: 117.22 on 4 and 195 DF,  p-value: &lt; 0.000000000000000222</code></pre>
<p>You can see that we now have an additional coefficient in the regression output, which tells us the effect of the binary predictor. The dummy variable can generally be interpreted as the average difference in the dependent variable between the two groups (similar to a t-test), conditional on the other variables you have included in your model. In this case, the coefficient tells you the difference in sales between international and local artists, and whether this difference is significant. Specifically, it means that international artists on average sell 45.67 units more than local artists, and this difference is significant (i.e., p &lt; 0.05).</p>
</div>
<div id="more-than-two-categories" class="section level3">
<h3><span class="header-section-number">7.4.2</span> More than two categories</h3>
<p>Predictors with more than two categories, like our “genre”" variable, can also be included in your model. However, in this case one dummy variable cannot represent all possible values, since there are three genres (i.e., 1 = Rock, 2 = Pop, 3 = Electronic). Thus, we need to create additional dummy variables. For example, for our “genre” variable, we create two dummy variables as follows:</p>
<p><span class="math display" id="eq:dummycoding1">\[\begin{equation} 
x_5 =
  \begin{cases}
    1       &amp; \quad \text{if } i \text{th  product is from Pop genre}\\
    0  &amp; \quad \text{if } i \text{th product is from Rock genre}
  \end{cases}
\tag{7.21}
\end{equation}\]</span></p>
<p><span class="math display" id="eq:dummycoding2">\[\begin{equation} 
x_6 =
  \begin{cases}
    1       &amp; \quad \text{if } i \text{th  product is from Electronic genre}\\
    0  &amp; \quad \text{if } i \text{th product is from Rock genre}
  \end{cases}
\tag{7.22}
\end{equation}\]</span></p>
<p>We would then add these variables as additional predictors in the regression equation and obtain the following model</p>
<p><span class="math display">\[\begin{align}
Sales =\beta_0 &amp;+\beta_1*adspend\\
      &amp;+\beta_2*airplay\\
      &amp;+\beta_3*starpower\\ 
      &amp;+\beta_4*international\\
      &amp;+\beta_5*Pop\\
      &amp;+\beta_6*Electronic+\epsilon
\end{align}\]</span></p>
<p>where “Pop” and “Rock” represent our new dummy variables, and <span class="math inline">\(\beta_5\)</span> and <span class="math inline">\(\beta_6\)</span> represent the associated regression coefficients.</p>
<p>The interpretation of the coefficients is as follows: <span class="math inline">\(\beta_5\)</span> is the difference in average sales between the genres “Rock” and “Pop”, while <span class="math inline">\(\beta_6\)</span> is the difference in average sales between the genres “Rock” and “Electro”. Note that the level for which no dummy variable is created is also referred to as the <em>baseline</em>. In our case, “Rock” would be the baseline genre. This means that there will always be one fewer dummy variable than the number of levels.</p>
<p>You don’t have to create the dummy variables manually as R will do this automatically when you add the variable to your equation:</p>
<div class="sourceCode" id="cb578"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb578-1"><a href="regression.html#cb578-1"></a>multiple_regression &lt;-<span class="st"> </span><span class="kw">lm</span>(sales <span class="op">~</span><span class="st"> </span>adspend <span class="op">+</span><span class="st"> </span>airplay <span class="op">+</span><span class="st"> </span></span>
<span id="cb578-2"><a href="regression.html#cb578-2"></a><span class="st">    </span>starpower <span class="op">+</span><span class="st"> </span>country <span class="op">+</span><span class="st"> </span>genre, <span class="dt">data =</span> regression)  <span class="co">#estimate linear model</span></span>
<span id="cb578-3"><a href="regression.html#cb578-3"></a><span class="kw">summary</span>(multiple_regression)  <span class="co">#summary of results</span></span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = sales ~ adspend + airplay + starpower + country + 
##     genre, data = regression)
## 
## Residuals:
##       Min        1Q    Median        3Q       Max 
## -116.1848  -26.5420    0.0528   27.9780  154.5628 
## 
## Coefficients:
##                         Estimate  Std. Error t value              Pr(&gt;|t|)    
## (Intercept)          -30.6790082  16.5998893 -1.8481             0.0661114 .  
## adspend                0.0723302   0.0065726 11.0048 &lt; 0.00000000000000022 ***
## airplay                2.7141773   0.2682446 10.1183 &lt; 0.00000000000000022 ***
## starpower             10.4962794   2.1937951  4.7845           0.000003398 ***
## countryinternational  40.8798829   8.4086835  4.8616           0.000002407 ***
## genrepop              47.6963967  10.4871650  4.5481           0.000009550 ***
## genreelectronic       27.6203374   8.1722318  3.3798             0.0008778 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 42.181 on 193 degrees of freedom
## Multiple R-squared:  0.73502,    Adjusted R-squared:  0.72678 
## F-statistic: 89.226 on 6 and 193 DF,  p-value: &lt; 0.000000000000000222</code></pre>
<p>How can we interpret the coefficients? It is estimated based on our model that products from the “Pop” genre will on average sell 47.69 units more than products from the “Rock” genre, and that products from the “Electronic” genre will sell on average 27.62 units more than the products from the “Rock” genre. The p-value of both variables is smaller than 0.05, suggesting that there is statistical evidence for a real difference in sales between the genres.</p>
<p>The level of the baseline category is arbitrary. As you have seen, R simply selects the first level as the baseline. If you would like to use a different baseline category, you can use the <code>relevel()</code> function and set the reference category using the <code>ref</code> argument. The following would estimate the same model using the second category as the baseline:</p>
<div class="sourceCode" id="cb580"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb580-1"><a href="regression.html#cb580-1"></a>multiple_regression &lt;-<span class="st"> </span><span class="kw">lm</span>(sales <span class="op">~</span><span class="st"> </span>adspend <span class="op">+</span><span class="st"> </span>airplay <span class="op">+</span><span class="st"> </span></span>
<span id="cb580-2"><a href="regression.html#cb580-2"></a><span class="st">    </span>starpower <span class="op">+</span><span class="st"> </span>country <span class="op">+</span><span class="st"> </span><span class="kw">relevel</span>(genre, <span class="dt">ref =</span> <span class="dv">2</span>), </span>
<span id="cb580-3"><a href="regression.html#cb580-3"></a>    <span class="dt">data =</span> regression)  <span class="co">#estimate linear model</span></span>
<span id="cb580-4"><a href="regression.html#cb580-4"></a><span class="kw">summary</span>(multiple_regression)  <span class="co">#summary of results</span></span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = sales ~ adspend + airplay + starpower + country + 
##     relevel(genre, ref = 2), data = regression)
## 
## Residuals:
##       Min        1Q    Median        3Q       Max 
## -116.1848  -26.5420    0.0528   27.9780  154.5628 
## 
## Coefficients:
##                                      Estimate  Std. Error t value
## (Intercept)                        17.0173885  18.1970368  0.9352
## adspend                             0.0723302   0.0065726 11.0048
## airplay                             2.7141773   0.2682446 10.1183
## starpower                          10.4962794   2.1937951  4.7845
## countryinternational               40.8798829   8.4086835  4.8616
## relevel(genre, ref = 2)rock       -47.6963967  10.4871650 -4.5481
## relevel(genre, ref = 2)electronic -20.0760593   7.9874725 -2.5134
##                                                Pr(&gt;|t|)    
## (Intercept)                                     0.35087    
## adspend                           &lt; 0.00000000000000022 ***
## airplay                           &lt; 0.00000000000000022 ***
## starpower                                   0.000003398 ***
## countryinternational                        0.000002407 ***
## relevel(genre, ref = 2)rock                 0.000009550 ***
## relevel(genre, ref = 2)electronic               0.01277 *  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 42.181 on 193 degrees of freedom
## Multiple R-squared:  0.73502,    Adjusted R-squared:  0.72678 
## F-statistic: 89.226 on 6 and 193 DF,  p-value: &lt; 0.000000000000000222</code></pre>
<p>Note that while your choice of the baseline category impacts the coefficients and the significance level, the prediction for each group will be the same regardless of this choice.</p>
</div>
</div>
<div id="extensions-of-the-linear-model" class="section level2">
<h2><span class="header-section-number">7.5</span> Extensions of the linear model</h2>
<p>The standard linear regression model provides results that are easy to interpret and is useful to address many real-world problems. However, it makes rather restrictive assumptions that might be violated in many cases. Notably, it assumes that the relationships between the response and predictor variable is <em>additive</em> and <em>linear</em>. The additive assumption states that the effect of an independent variable on the dependent variable is independent of the values of the other independent variables included in the model. The linear assumption means that the effect of a one-unit change in the independent variable on the dependent variable is the same, regardless of the values of the value of the independent variable. This is also referred to as constant <em>marginal returns</em>. For example, an increase in ad-spend from 10€ to 11€ yields the same increase in sales as an increase from 100,000€ to 100,001€. This section presents alternative model specifications if the assumptions do not hold.</p>
<div id="interaction-effects" class="section level3">
<h3><span class="header-section-number">7.5.1</span> Interaction effects</h3>
<div align="center">
<iframe width="560" height="315" src="https://www.youtube.com/embed/AP2vZ7V-qrw" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>
</iframe>
</div>
<p>Regarding the additive assumption, it might be argued that the effect of some variables are not fully independent of the values of other variables. In our example, one could argue that the effect of advertising depends on the type of artist. For example, for local artist advertising might be more effective. We can investigate if this is the case using a grouped scatterplot:</p>
<div class="sourceCode" id="cb582"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb582-1"><a href="regression.html#cb582-1"></a><span class="kw">ggplot</span>(regression, <span class="kw">aes</span>(adspend, sales, <span class="dt">colour =</span> <span class="kw">as.factor</span>(country))) <span class="op">+</span><span class="st"> </span></span>
<span id="cb582-2"><a href="regression.html#cb582-2"></a><span class="st">    </span><span class="kw">geom_point</span>() <span class="op">+</span><span class="st"> </span><span class="kw">geom_smooth</span>(<span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="dt">alpha =</span> <span class="fl">0.1</span>) <span class="op">+</span><span class="st"> </span></span>
<span id="cb582-3"><a href="regression.html#cb582-3"></a><span class="st">    </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&quot;Advertising expenditures (EUR)&quot;</span>, <span class="dt">y =</span> <span class="st">&quot;Number of sales&quot;</span>, </span>
<span id="cb582-4"><a href="regression.html#cb582-4"></a>        <span class="dt">colour =</span> <span class="st">&quot;country&quot;</span>) <span class="op">+</span><span class="st"> </span><span class="kw">theme_bw</span>()</span></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-76"></span>
<img src="10-Regression_files/figure-html/unnamed-chunk-76-1.png" alt="Effect of advertising by group" width="672" />
<p class="caption">
Figure 7.4: Effect of advertising by group
</p>
</div>
<p>The scatterplot indeed suggests that there is a difference in advertising effectiveness between local and international artists. You can see this from the two different regression lines. We can incorporate this interaction effect by including an interaction term in the regression equation as follows:</p>
<p><span class="math display">\[\begin{align}
Sales =\beta_0 &amp;+\beta_1*adspend\\
      &amp;+\beta_2*airplay\\
      &amp;+\beta_3*starpower\\ 
      &amp;+\beta_4*international\\
      &amp;+\beta_5*(adspend*international)\\
      &amp;+\epsilon
\end{align}\]</span></p>
<p>You can see that the effect of advertising now depends on the type of artist. Hence, the additive assumption is removed. Note that if you decide to include an interaction effect, you should always include the main effects of the variables that are included in the interaction (even if the associated p-values do not suggest significant effects). It is easy to include an interaction effect in you model by adding an additional variable that has the format ```var1:var2````. In our example, this could be achieved using the following specification:</p>
<div class="sourceCode" id="cb583"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb583-1"><a href="regression.html#cb583-1"></a>multiple_regression &lt;-<span class="st"> </span><span class="kw">lm</span>(sales <span class="op">~</span><span class="st"> </span>adspend <span class="op">+</span><span class="st"> </span>airplay <span class="op">+</span><span class="st"> </span></span>
<span id="cb583-2"><a href="regression.html#cb583-2"></a><span class="st">    </span>starpower <span class="op">+</span><span class="st"> </span>country <span class="op">+</span><span class="st"> </span>adspend<span class="op">:</span>country, <span class="dt">data =</span> regression)  <span class="co">#estimate linear model</span></span>
<span id="cb583-3"><a href="regression.html#cb583-3"></a><span class="kw">summary</span>(multiple_regression)  <span class="co">#summary of results</span></span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = sales ~ adspend + airplay + starpower + country + 
##     adspend:country, data = regression)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -105.431  -26.259   -2.717   29.353  161.525 
## 
## Coefficients:
##                                 Estimate  Std. Error t value
## (Intercept)                  -14.1169481  16.2937062 -0.8664
## adspend                        0.0884725   0.0072955 12.1270
## airplay                        2.9574474   0.2685930 11.0109
## starpower                      9.4373011   2.2969371  4.1086
## countryinternational          71.5753078  15.1287682  4.7311
## adspend:countryinternational  -0.0347189   0.0166667 -2.0831
##                                           Pr(&gt;|t|)    
## (Intercept)                                0.38734    
## adspend                      &lt; 0.00000000000000022 ***
## airplay                      &lt; 0.00000000000000022 ***
## starpower                              0.000058669 ***
## countryinternational                   0.000004293 ***
## adspend:countryinternational               0.03855 *  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 43.809 on 194 degrees of freedom
## Multiple R-squared:  0.71269,    Adjusted R-squared:  0.70529 
## F-statistic: 96.247 on 5 and 194 DF,  p-value: &lt; 0.000000000000000222</code></pre>
<p>How can we interpret the coefficient? The <code>adspend</code> main effect tells you the effect of advertising for the reference group that has the factor level zero. In our example, it is the advertising effect for local artist. This means that for local artists, spending an additional 1,000 Euros on advertising will result in approximately 89 additional unit sales. The interaction effect tells you by how much the effect differs for the other group (i.e., international artists) and whether this difference is significant. In our example, it means that the effect for international artists can be computed as: 0.0885 - 0.0347 = 0.0538. This means that for international artists, spending an additional 1,000 Euros on advertising will result in approximately 54 additional unit sales. Since the interaction effect is significant (p &lt; 0.05) we can conclude that advertising is less effective for international artists.</p>
<p>The above example showed the interaction between a categorical variable (i.e., “country”) and a continuous variable (i.e., “adspend”). However, interaction effects can be defined for different combinations of variable types. For example, you might just as well specify an interaction between two continuous variables. In our example, you might suspect that there are synergy effects between advertising expenditures and radio airplay. It could be that advertising is more effective when an artist receives a large number of radio plays. In this case, we would specify our model as:</p>
<p><span class="math display">\[\begin{align}
Sales =\beta_0 &amp;+\beta_1*adspend\\
      &amp;+\beta_2*airplay\\
      &amp;+\beta_3*starpower\\ 
      &amp;+\beta_4*(adspend*airplay)\\
      &amp;+\epsilon
\end{align}\]</span></p>
<p>In this case, we can interpret <span class="math inline">\(\beta_4\)</span> as the increase in the effectiveness of advertising for a one unit increase in radio airplay (or vice versa). This can be translated to R using:</p>
<div class="sourceCode" id="cb585"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb585-1"><a href="regression.html#cb585-1"></a>multiple_regression &lt;-<span class="st"> </span><span class="kw">lm</span>(sales <span class="op">~</span><span class="st"> </span>adspend <span class="op">+</span><span class="st"> </span>airplay <span class="op">+</span><span class="st"> </span></span>
<span id="cb585-2"><a href="regression.html#cb585-2"></a><span class="st">    </span>starpower <span class="op">+</span><span class="st"> </span>adspend<span class="op">:</span>airplay, <span class="dt">data =</span> regression)  <span class="co">#estimate linear model</span></span>
<span id="cb585-3"><a href="regression.html#cb585-3"></a><span class="kw">summary</span>(multiple_regression)  <span class="co">#summary of results</span></span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = sales ~ adspend + airplay + starpower + adspend:airplay, 
##     data = regression)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -128.301  -28.587   -2.152   28.041  141.591 
## 
## Coefficients:
##                     Estimate   Std. Error t value             Pr(&gt;|t|)    
## (Intercept)     -34.52047735  19.31153974 -1.7876               0.0754 .  
## adspend           0.10438741   0.02200443  4.7439 0.000004043410591583 ***
## airplay           3.68042650   0.43539815  8.4530 0.000000000000006556 ***
## starpower        10.89024162   2.44767207  4.4492 0.000014470121505845 ***
## adspend:airplay  -0.00064885   0.00069488 -0.9338               0.3516    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 47.103 on 195 degrees of freedom
## Multiple R-squared:  0.66616,    Adjusted R-squared:  0.65931 
## F-statistic: 97.278 on 4 and 195 DF,  p-value: &lt; 0.000000000000000222</code></pre>
<p>However, since the p-value of the interaction is larger than 0.05, there is little statistical evidence for an interaction between the two variables.</p>
</div>
<div id="non-linear-relationships" class="section level3">
<h3><span class="header-section-number">7.5.2</span> Non-linear relationships</h3>
<div id="multiplicative-model" class="section level4">
<h4><span class="header-section-number">7.5.2.1</span> Multiplicative model</h4>
<div align="center">
<iframe width="560" height="315" src="https://www.youtube.com/embed/SizQ5BgnraY" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>
</iframe>
</div>
<p>In our example above, it appeared that linear relationships could be reasonably assumed. In many practical applications, however, this might not be the case. Let’s review the implications of a linear specification again:</p>
<ul>
<li>Constant marginal returns (e.g., an increase in ad-spend from 10€ to 11€ yields the same increase in sales as an increase from 100,000€ to 100,001€)</li>
<li>Elasticities increase with X (e.g., advertising becomes relatively more effective; i.e., a relatively smaller change in advertising expenditure will yield the same return)</li>
</ul>
<p>In many marketing contexts, these might not be reasonable assumptions. Consider the case of advertising. It is unlikely that the return on advertising will not depend on the level of advertising expenditures. It is rather likely that saturation occurs at some level, meaning that the return from an additional Euro spend on advertising is decreasing with the level of advertising expenditures (i.e., decreasing marginal returns). In other words, at some point the advertising campaign has achieved a certain level of penetration and an additional Euro spend on advertising won’t yield the same return as in the beginning.</p>
<p>Let’s use an example data set, containing the advertising expenditures of a company and the sales (in thousand units).</p>
<div class="sourceCode" id="cb587"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb587-1"><a href="regression.html#cb587-1"></a>non_linear_reg &lt;-<span class="st"> </span><span class="kw">read.table</span>(<span class="st">&quot;https://raw.githubusercontent.com/IMSMWU/Teaching/master/MRDA2017/non_linear.dat&quot;</span>, </span>
<span id="cb587-2"><a href="regression.html#cb587-2"></a>    <span class="dt">sep =</span> <span class="st">&quot;</span><span class="ch">\t</span><span class="st">&quot;</span>, <span class="dt">header =</span> <span class="ot">TRUE</span>)  <span class="co">#read in data</span></span>
<span id="cb587-3"><a href="regression.html#cb587-3"></a><span class="kw">head</span>(non_linear_reg)</span></code></pre></div>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["sales"],"name":[1],"type":["dbl"],"align":["right"]},{"label":["advertising"],"name":[2],"type":["dbl"],"align":["right"]}],"data":[{"1":"16.2","2":"13817.4"},{"1":"17.1","2":"5074.6"},{"1":"15.3","2":"3501.3"},{"1":"27.3","2":"23662.2"},{"1":"13.6","2":"9977.2"},{"1":"18.8","2":"11972.5"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<p>Now we inspect if a linear specification is appropriate by looking at the scatterplot:</p>
<div class="sourceCode" id="cb588"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb588-1"><a href="regression.html#cb588-1"></a><span class="kw">ggplot</span>(<span class="dt">data =</span> non_linear_reg, <span class="kw">aes</span>(<span class="dt">x =</span> advertising, </span>
<span id="cb588-2"><a href="regression.html#cb588-2"></a>    <span class="dt">y =</span> sales)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_point</span>(<span class="dt">shape =</span> <span class="dv">1</span>) <span class="op">+</span><span class="st"> </span><span class="kw">geom_smooth</span>(<span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>, </span>
<span id="cb588-3"><a href="regression.html#cb588-3"></a>    <span class="dt">fill =</span> <span class="st">&quot;blue&quot;</span>, <span class="dt">alpha =</span> <span class="fl">0.1</span>) <span class="op">+</span><span class="st"> </span><span class="kw">theme_bw</span>()</span></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-80"></span>
<img src="10-Regression_files/figure-html/unnamed-chunk-80-1.png" alt="Non-linear relationship" width="672" />
<p class="caption">
Figure 7.5: Non-linear relationship
</p>
</div>
<p>It appears that a linear model might <strong>not</strong> represent the data well. It rather appears that the effect of an additional Euro spend on advertising is decreasing with increasing levels of advertising expenditures. Thus, we have decreasing marginal returns. We could put this to a test and estimate a linear model:</p>
<div class="sourceCode" id="cb589"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb589-1"><a href="regression.html#cb589-1"></a>linear_reg &lt;-<span class="st"> </span><span class="kw">lm</span>(sales <span class="op">~</span><span class="st"> </span>advertising, <span class="dt">data =</span> non_linear_reg)</span>
<span id="cb589-2"><a href="regression.html#cb589-2"></a><span class="kw">summary</span>(linear_reg)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = sales ~ advertising, data = non_linear_reg)
## 
## Residuals:
##       Min        1Q    Median        3Q       Max 
## -10.47733  -2.38863  -0.35584   2.18843  16.74527 
## 
## Coefficients:
##                Estimate  Std. Error t value              Pr(&gt;|t|)    
## (Intercept) 9.957521554 0.225115083  44.233 &lt; 0.00000000000000022 ***
## advertising 0.000502446 0.000015611  32.186 &lt; 0.00000000000000022 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 3.5931 on 998 degrees of freedom
## Multiple R-squared:  0.50932,    Adjusted R-squared:  0.50883 
## F-statistic: 1035.9 on 1 and 998 DF,  p-value: &lt; 0.000000000000000222</code></pre>
<p>Advertising appears to be positively related to sales with an additional Euro that is spent on advertising resulting in 0.0005 additional sales. The R<sup>2</sup> statistic suggests that approximately 51% of the total variation can be explained by the model</p>
<p>To test if the linear specification is appropriate, let’s inspect some of the plots that are generated by R. We start by inspecting the residuals plot.</p>
<div class="sourceCode" id="cb591"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb591-1"><a href="regression.html#cb591-1"></a><span class="kw">plot</span>(linear_reg, <span class="dv">1</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-82"></span>
<img src="10-Regression_files/figure-html/unnamed-chunk-82-1.png" alt="Residuals vs. Fitted" width="672" />
<p class="caption">
Figure 7.6: Residuals vs. Fitted
</p>
</div>
<p>The plot suggests that the assumption of homoscedasticity is violated (i.e., the spread of values on the y-axis is different for different levels of the fitted values). In addition, the red line deviates from the dashed grey line, suggesting that the relationship might not be linear. Finally, the Q-Q plot of the residuals suggests that the residuals are not normally distributed.</p>
<div class="sourceCode" id="cb592"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb592-1"><a href="regression.html#cb592-1"></a><span class="kw">plot</span>(linear_reg, <span class="dv">2</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-83"></span>
<img src="10-Regression_files/figure-html/unnamed-chunk-83-1.png" alt="Q-Q plot" width="672" />
<p class="caption">
Figure 7.7: Q-Q plot
</p>
</div>
<p>To sum up, a linear specification might not be the best model for this data set.</p>
<p>In this case, a multiplicative model might be a better representation of the data. The multiplicative model has the following formal representation:</p>
<p><span class="math display" id="eq:multiplicative">\[\begin{equation} 
Y =\beta_0 *X_1^{\beta_1}*X_2^{\beta_2}*...*X_J^{\beta_J}*\epsilon
\tag{7.23}
\end{equation}\]</span></p>
<p>This functional form can be linearized by taking the logarithm of both sides of the equation:</p>
<p><span class="math display" id="eq:multiplicativetransformed">\[\begin{equation} 
log(Y) =log(\beta_0) + \beta_1*log(X_1) + \beta_2*log(X_2) + ...+ \beta_J*log(X_J) + log(\epsilon)
\tag{7.24}
\end{equation}\]</span></p>
<p>This means that taking logarithms of both sides of the equation makes linear estimation possible. The above transformation follows from two logarithm rules that we apply here:</p>
<ol style="list-style-type: decimal">
<li>the product rule states that <span class="math inline">\(log(xy)=log(x)+log(y)\)</span>; thus, when taking the logarithm of the right hand side of the multiplicative model, we can write <span class="math inline">\(log(X_1) + log(X_2)... log(X_J)\)</span> instead of <span class="math inline">\(log(X_1*X_2*...X_J)\)</span>, and</li>
<li>the power rule states that <span class="math inline">\(log(x^y) = ylog(x)\)</span>; thus, we can write <span class="math inline">\(\beta*log(X)\)</span> instead of <span class="math inline">\(X^{\beta}\)</span></li>
</ol>
<p>Let’s test how the scatterplot would look like if we use the logarithm of our variables using the <code>log()</code> function instead of the original values.</p>
<div class="sourceCode" id="cb593"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb593-1"><a href="regression.html#cb593-1"></a><span class="kw">ggplot</span>(<span class="dt">data =</span> non_linear_reg, <span class="kw">aes</span>(<span class="dt">x =</span> <span class="kw">log</span>(advertising), </span>
<span id="cb593-2"><a href="regression.html#cb593-2"></a>    <span class="dt">y =</span> <span class="kw">log</span>(sales))) <span class="op">+</span><span class="st"> </span><span class="kw">geom_point</span>(<span class="dt">shape =</span> <span class="dv">1</span>) <span class="op">+</span><span class="st"> </span><span class="kw">geom_smooth</span>(<span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>, </span>
<span id="cb593-3"><a href="regression.html#cb593-3"></a>    <span class="dt">fill =</span> <span class="st">&quot;blue&quot;</span>, <span class="dt">alpha =</span> <span class="fl">0.1</span>) <span class="op">+</span><span class="st"> </span><span class="kw">theme_bw</span>()</span></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-84"></span>
<img src="10-Regression_files/figure-html/unnamed-chunk-84-1.png" alt="Linearized effect" width="672" />
<p class="caption">
Figure 7.8: Linearized effect
</p>
</div>
<p>It appears that now, with the log-transformed variables, a linear specification is a much better representation of the data. Hence, we can log-transform our variables and estimate the following equation:</p>
<p><span class="math display" id="eq:multiplicativetransformed1">\[\begin{equation} 
log(sales) = log(\beta_0) + \beta_1*log(advertising) + log(\epsilon)
\tag{7.25}
\end{equation}\]</span></p>
<p>This can be easily implemented in R by transforming the variables using the <code>log()</code> function:</p>
<div class="sourceCode" id="cb594"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb594-1"><a href="regression.html#cb594-1"></a>log_reg &lt;-<span class="st"> </span><span class="kw">lm</span>(<span class="kw">log</span>(sales) <span class="op">~</span><span class="st"> </span><span class="kw">log</span>(advertising), <span class="dt">data =</span> non_linear_reg)</span>
<span id="cb594-2"><a href="regression.html#cb594-2"></a><span class="kw">summary</span>(log_reg)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = log(sales) ~ log(advertising), data = non_linear_reg)
## 
## Residuals:
##       Min        1Q    Median        3Q       Max 
## -0.665943 -0.127226  0.002988  0.134372  0.639636 
## 
## Coefficients:
##                    Estimate Std. Error t value            Pr(&gt;|t|)    
## (Intercept)      -0.0149269  0.0597141  -0.250              0.8027    
## log(advertising)  0.3007687  0.0065095  46.205 &lt;0.0000000000000002 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.20181 on 998 degrees of freedom
## Multiple R-squared:  0.68144,    Adjusted R-squared:  0.68113 
## F-statistic: 2134.9 on 1 and 998 DF,  p-value: &lt; 0.000000000000000222</code></pre>
<p>Note that this specification implies decreasing marginal returns (i.e., the returns of advertising are decreasing with the level of advertising), which appear to be more consistent with the data. The specification is also consistent with proportional changes in advertising being associated with proportional changes in sales (i.e., advertising does not become more effective with increasing levels). This has important implications on the interpretation of the coefficients. In our example, you would interpret the coefficient as follows: <strong>A 1% increase in advertising leads to a 0.3% increase in sales</strong>. Hence, the interpretation is in proportional terms and no longer in units. This means that the coefficients in a log-log model can be directly interpreted as elasticities, which also makes communication easier. We can generally also inspect the R<sup>2</sup> statistic to see that the model fit has increased compared to the linear specification (i.e., R<sup>2</sup> has increased to 0.681 from 0.509). However, please note that the variables are now measured on a different scale, which means that the model fit in theory is not directly comparable. Also, we could use the residuals plot to confirm that the revised specification is more appropriate:</p>
<div class="sourceCode" id="cb596"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb596-1"><a href="regression.html#cb596-1"></a><span class="kw">plot</span>(log_reg, <span class="dv">1</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-86-1"></span>
<img src="10-Regression_files/figure-html/unnamed-chunk-86-1.png" alt="Residuals plot" width="672" />
<p class="caption">
Figure 7.9: Residuals plot
</p>
</div>
<div class="sourceCode" id="cb597"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb597-1"><a href="regression.html#cb597-1"></a><span class="kw">plot</span>(log_reg, <span class="dv">2</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-86-2"></span>
<img src="10-Regression_files/figure-html/unnamed-chunk-86-2.png" alt="Q-Q plot" width="672" />
<p class="caption">
Figure 7.10: Q-Q plot
</p>
</div>
<p>Finally, we can plot the predicted values against the observed values to see that the results from the log-log model (red) provide a better prediction than the results from the linear model (blue).</p>
<div class="sourceCode" id="cb598"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb598-1"><a href="regression.html#cb598-1"></a>non_linear_reg<span class="op">$</span>pred_lin_reg &lt;-<span class="st"> </span><span class="kw">predict</span>(linear_reg)</span>
<span id="cb598-2"><a href="regression.html#cb598-2"></a>non_linear_reg<span class="op">$</span>pred_log_reg &lt;-<span class="st"> </span><span class="kw">predict</span>(log_reg)</span>
<span id="cb598-3"><a href="regression.html#cb598-3"></a><span class="kw">ggplot</span>(<span class="dt">data =</span> non_linear_reg) <span class="op">+</span><span class="st"> </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(<span class="dt">x =</span> advertising, </span>
<span id="cb598-4"><a href="regression.html#cb598-4"></a>    <span class="dt">y =</span> sales), <span class="dt">shape =</span> <span class="dv">1</span>) <span class="op">+</span><span class="st"> </span><span class="kw">geom_line</span>(<span class="dt">data =</span> non_linear_reg, </span>
<span id="cb598-5"><a href="regression.html#cb598-5"></a>    <span class="kw">aes</span>(<span class="dt">x =</span> advertising, <span class="dt">y =</span> pred_lin_reg), <span class="dt">color =</span> <span class="st">&quot;blue&quot;</span>, </span>
<span id="cb598-6"><a href="regression.html#cb598-6"></a>    <span class="dt">size =</span> <span class="fl">1.05</span>) <span class="op">+</span><span class="st"> </span><span class="kw">geom_line</span>(<span class="dt">data =</span> non_linear_reg, </span>
<span id="cb598-7"><a href="regression.html#cb598-7"></a>    <span class="kw">aes</span>(<span class="dt">x =</span> advertising, <span class="dt">y =</span> <span class="kw">exp</span>(pred_log_reg)), <span class="dt">color =</span> <span class="st">&quot;red&quot;</span>, </span>
<span id="cb598-8"><a href="regression.html#cb598-8"></a>    <span class="dt">size =</span> <span class="fl">1.05</span>) <span class="op">+</span><span class="st"> </span><span class="kw">theme_bw</span>()</span></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-87"></span>
<img src="10-Regression_files/figure-html/unnamed-chunk-87-1.png" alt="Comparison if model fit" width="672" />
<p class="caption">
Figure 7.11: Comparison if model fit
</p>
</div>
</div>
<div id="quadratic-model" class="section level4">
<h4><span class="header-section-number">7.5.2.2</span> Quadratic model</h4>
<div align="center">
<iframe width="560" height="315" src="https://www.youtube.com/embed/05OItolOlLw" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>
</iframe>
</div>
<p>Another way of modeling non-linearities is including a squared term if there are decreasing or increasing effects. In fact, we can model non-constant slopes as long as the form is a linear combination of exponentials (i.e. squared, cubed, …) of the explanatory variables. Usually we do not expect <em>many</em> inflection points so squared or third power terms suffice. Note that the degree of the polynomial has to be equal to the number of inflection points.</p>
<p>When using squared terms we can model diminishing and eventually negative returns. Think about advertisement spending. If a brand is not well known, spending on ads will increase brand awareness and have a large effect on sales. In a regression model this translates to a steep slope for spending at the origin (i.e. for lower spending). However, as more and more people will already know the brand we expect that an additional Euro spent on advertisement will have less and less of an effect the more the company spends. We say that the returns are diminishing. Eventually, if they keep putting more and more ads out, people get annoyed and some will stop buying from the company. In that case the return might even get negative. To model such a situation we need a linear as well as a squared term in the regression.</p>
<p><code>lm(...)</code> can take squared (or any power) terms as input by adding <code>I(X^2)</code> as explanatory variable. In the example below we see a clear quadratic relationship with an inflection point at around 70. If we try to model this using the level of the covariates without the quadratic term we do not get a very good fit.</p>
<div class="sourceCode" id="cb599"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb599-1"><a href="regression.html#cb599-1"></a><span class="kw">set.seed</span>(<span class="dv">1234</span>)</span>
<span id="cb599-2"><a href="regression.html#cb599-2"></a>X &lt;-<span class="st"> </span><span class="kw">as.integer</span>(<span class="kw">runif</span>(<span class="dv">1000</span>, <span class="dv">0</span>, <span class="dv">12000</span>))</span>
<span id="cb599-3"><a href="regression.html#cb599-3"></a>Y &lt;-<span class="st"> </span><span class="dv">80000</span> <span class="op">+</span><span class="st"> </span><span class="dv">140</span> <span class="op">*</span><span class="st"> </span>X <span class="op">-</span><span class="st"> </span><span class="fl">0.01</span> <span class="op">*</span><span class="st"> </span>(X<span class="op">^</span><span class="dv">2</span>) <span class="op">+</span><span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">1000</span>, <span class="dv">0</span>, </span>
<span id="cb599-4"><a href="regression.html#cb599-4"></a>    <span class="dv">35000</span>)</span>
<span id="cb599-5"><a href="regression.html#cb599-5"></a>modLinear &lt;-<span class="st"> </span><span class="kw">lm</span>(Y<span class="op">/</span><span class="dv">100000</span> <span class="op">~</span><span class="st"> </span>X)</span>
<span id="cb599-6"><a href="regression.html#cb599-6"></a>sales_quad &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">sales =</span> Y<span class="op">/</span><span class="dv">100000</span>, <span class="dt">advertising =</span> X <span class="op">*</span><span class="st"> </span></span>
<span id="cb599-7"><a href="regression.html#cb599-7"></a><span class="st">    </span><span class="fl">0.01</span>, <span class="dt">Prediction =</span> <span class="kw">fitted</span>(modLinear))</span>
<span id="cb599-8"><a href="regression.html#cb599-8"></a><span class="kw">ggplot</span>(sales_quad) <span class="op">+</span><span class="st"> </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(<span class="dt">x =</span> advertising, </span>
<span id="cb599-9"><a href="regression.html#cb599-9"></a>    <span class="dt">y =</span> sales, <span class="dt">color =</span> <span class="st">&quot;Data&quot;</span>)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">x =</span> advertising, </span>
<span id="cb599-10"><a href="regression.html#cb599-10"></a>    <span class="dt">y =</span> Prediction, <span class="dt">color =</span> <span class="st">&quot;Prediction&quot;</span>)) <span class="op">+</span><span class="st"> </span><span class="kw">theme_bw</span>() <span class="op">+</span><span class="st"> </span></span>
<span id="cb599-11"><a href="regression.html#cb599-11"></a><span class="st">    </span><span class="kw">ggtitle</span>(<span class="st">&quot;Linear Predictor&quot;</span>) <span class="op">+</span><span class="st"> </span><span class="kw">theme</span>(<span class="dt">legend.title =</span> <span class="kw">element_blank</span>())</span></code></pre></div>
<p><img src="10-Regression_files/figure-html/unnamed-chunk-88-1.png" width="672" /></p>
<p>The graph above clearly shows that advertising spending of between 0 and 50 increases sales. However, the marginal increase (i.e. the slope of the data curve) is decreasing. Around 70 there is an inflection point. After that point additional ad-spending actually decreases sales (e.g. people get annoyed).
Notice that the prediction line is straight, that is, the marginal increase of sales due to additional spending on advertising is the same for any amount of spending. This shows the danger of basing business decisions on wrongly specified models. But even in the area in which the sign of the prediction is correct, we are quite far off. Lets take a look at the top 5 sales values and the corresponding predictions:</p>
<div class="sourceCode" id="cb600"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb600-1"><a href="regression.html#cb600-1"></a>top5 &lt;-<span class="st"> </span><span class="kw">which</span>(sales_quad<span class="op">$</span>sales <span class="op">%in%</span><span class="st"> </span><span class="kw">head</span>(<span class="kw">sort</span>(sales_quad<span class="op">$</span>sales, </span>
<span id="cb600-2"><a href="regression.html#cb600-2"></a>    <span class="dt">decreasing =</span> <span class="ot">TRUE</span>), <span class="dv">5</span>))</span>
<span id="cb600-3"><a href="regression.html#cb600-3"></a>dplyr<span class="op">::</span><span class="kw">arrange</span>(sales_quad[top5, ], <span class="kw">desc</span>(sales_quad[top5, </span>
<span id="cb600-4"><a href="regression.html#cb600-4"></a>    <span class="dv">1</span>]))</span></code></pre></div>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["sales"],"name":[1],"type":["dbl"],"align":["right"]},{"label":["advertising"],"name":[2],"type":["dbl"],"align":["right"]},{"label":["Prediction"],"name":[3],"type":["dbl"],"align":["right"]}],"data":[{"1":"6.5310412","2":"71.83","3":"4.5906120"},{"1":"6.3341788","2":"80.98","3":"4.7559082"},{"1":"6.3257561","2":"62.82","3":"4.4278448"},{"1":"6.3222325","2":"61.73","3":"4.4081538"},{"1":"6.3102048","2":"67.95","3":"4.5205191"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<p>By including a quadratic term we can fit the data very well. This is still a linear model since the outcome variable is still explained by a linear combination of regressors even though one of the regressors is now just a non-linear function of the same variable (i.e. the squared value).</p>
<div class="sourceCode" id="cb601"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb601-1"><a href="regression.html#cb601-1"></a>quad_mod &lt;-<span class="st"> </span><span class="kw">lm</span>(sales <span class="op">~</span><span class="st"> </span>advertising <span class="op">+</span><span class="st"> </span><span class="kw">I</span>(advertising<span class="op">^</span><span class="dv">2</span>), </span>
<span id="cb601-2"><a href="regression.html#cb601-2"></a>    <span class="dt">data =</span> sales_quad)</span>
<span id="cb601-3"><a href="regression.html#cb601-3"></a><span class="kw">summary</span>(quad_mod)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = sales ~ advertising + I(advertising^2), data = sales_quad)
## 
## Residuals:
##       Min        1Q    Median        3Q       Max 
## -1.021614 -0.220140  0.003639  0.223573  0.956181 
## 
## Coefficients:
##                       Estimate    Std. Error  t value              Pr(&gt;|t|)    
## (Intercept)       0.8162719014  0.0317011155   25.749 &lt; 0.00000000000000022 ***
## advertising       0.1396425755  0.0011994504  116.422 &lt; 0.00000000000000022 ***
## I(advertising^2) -0.0009997164  0.0000095479 -104.706 &lt; 0.00000000000000022 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.33218 on 997 degrees of freedom
## Multiple R-squared:  0.93596,    Adjusted R-squared:  0.93583 
## F-statistic: 7285.7 on 2 and 997 DF,  p-value: &lt; 0.000000000000000222</code></pre>
<div class="sourceCode" id="cb603"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb603-1"><a href="regression.html#cb603-1"></a><span class="kw">confint</span>(quad_mod)</span></code></pre></div>
<pre><code>##                          2.5 %         97.5 %
## (Intercept)       0.7540633367  0.87848046615
## advertising       0.1372888385  0.14199631240
## I(advertising^2) -0.0010184526 -0.00098098016</code></pre>
<div class="sourceCode" id="cb605"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb605-1"><a href="regression.html#cb605-1"></a>sales_quad<span class="op">$</span>Prediction &lt;-<span class="st"> </span><span class="kw">predict</span>(quad_mod)</span>
<span id="cb605-2"><a href="regression.html#cb605-2"></a><span class="kw">ggplot</span>(<span class="dt">data =</span> sales_quad, <span class="kw">aes</span>(<span class="dt">x =</span> Prediction, <span class="dt">y =</span> sales)) <span class="op">+</span><span class="st"> </span></span>
<span id="cb605-3"><a href="regression.html#cb605-3"></a><span class="st">    </span><span class="kw">geom_point</span>(<span class="dt">shape =</span> <span class="dv">1</span>) <span class="op">+</span><span class="st"> </span><span class="kw">geom_smooth</span>(<span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>, </span>
<span id="cb605-4"><a href="regression.html#cb605-4"></a>    <span class="dt">fill =</span> <span class="st">&quot;blue&quot;</span>, <span class="dt">alpha =</span> <span class="fl">0.1</span>) <span class="op">+</span><span class="st"> </span><span class="kw">theme_bw</span>()</span></code></pre></div>
<pre><code>## `geom_smooth()` using formula &#39;y ~ x&#39;</code></pre>
<p><img src="10-Regression_files/figure-html/unnamed-chunk-90-1.png" width="672" /></p>
<div class="sourceCode" id="cb607"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb607-1"><a href="regression.html#cb607-1"></a><span class="kw">plot</span>(quad_mod, <span class="dv">1</span>)</span></code></pre></div>
<p><img src="10-Regression_files/figure-html/unnamed-chunk-90-2.png" width="672" /></p>
<div class="sourceCode" id="cb608"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb608-1"><a href="regression.html#cb608-1"></a><span class="kw">plot</span>(quad_mod, <span class="dv">2</span>)</span></code></pre></div>
<p><img src="10-Regression_files/figure-html/unnamed-chunk-90-3.png" width="672" /></p>
<div class="sourceCode" id="cb609"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb609-1"><a href="regression.html#cb609-1"></a><span class="kw">shapiro.test</span>(<span class="kw">resid</span>(quad_mod))</span></code></pre></div>
<pre><code>## 
##  Shapiro-Wilk normality test
## 
## data:  resid(quad_mod)
## W = 0.997653, p-value = 0.16559</code></pre>
<div class="sourceCode" id="cb611"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb611-1"><a href="regression.html#cb611-1"></a>sales_quad<span class="op">$</span>pred_lin_reg &lt;-<span class="st"> </span><span class="kw">predict</span>(modLinear)</span>
<span id="cb611-2"><a href="regression.html#cb611-2"></a><span class="kw">ggplot</span>(<span class="dt">data =</span> sales_quad) <span class="op">+</span><span class="st"> </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(<span class="dt">x =</span> advertising, </span>
<span id="cb611-3"><a href="regression.html#cb611-3"></a>    <span class="dt">y =</span> sales), <span class="dt">shape =</span> <span class="dv">1</span>) <span class="op">+</span><span class="st"> </span><span class="kw">geom_line</span>(<span class="dt">data =</span> sales_quad, </span>
<span id="cb611-4"><a href="regression.html#cb611-4"></a>    <span class="kw">aes</span>(<span class="dt">x =</span> advertising, <span class="dt">y =</span> pred_lin_reg), <span class="dt">color =</span> <span class="st">&quot;blue&quot;</span>, </span>
<span id="cb611-5"><a href="regression.html#cb611-5"></a>    <span class="dt">size =</span> <span class="fl">1.05</span>) <span class="op">+</span><span class="st"> </span><span class="kw">geom_line</span>(<span class="dt">data =</span> sales_quad, <span class="kw">aes</span>(<span class="dt">x =</span> advertising, </span>
<span id="cb611-6"><a href="regression.html#cb611-6"></a>    <span class="dt">y =</span> Prediction), <span class="dt">color =</span> <span class="st">&quot;red&quot;</span>, <span class="dt">size =</span> <span class="fl">1.05</span>) <span class="op">+</span><span class="st"> </span></span>
<span id="cb611-7"><a href="regression.html#cb611-7"></a><span class="st">    </span><span class="kw">theme_bw</span>() <span class="op">+</span><span class="st"> </span><span class="kw">xlab</span>(<span class="st">&quot;Advertising (thsd. Euro)&quot;</span>) <span class="op">+</span><span class="st"> </span></span>
<span id="cb611-8"><a href="regression.html#cb611-8"></a><span class="st">    </span><span class="kw">ylab</span>(<span class="st">&quot;Sales (million units)&quot;</span>)</span></code></pre></div>
<p><img src="10-Regression_files/figure-html/unnamed-chunk-90-4.png" width="672" /></p>
<p>Now the prediction of the model is very close to the actual data and we could base our production decisions on that model.</p>
<div class="sourceCode" id="cb612"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb612-1"><a href="regression.html#cb612-1"></a>top5 &lt;-<span class="st"> </span><span class="kw">which</span>(sales_quad<span class="op">$</span>sales <span class="op">%in%</span><span class="st"> </span><span class="kw">head</span>(<span class="kw">sort</span>(sales_quad<span class="op">$</span>sales, </span>
<span id="cb612-2"><a href="regression.html#cb612-2"></a>    <span class="dt">decreasing =</span> <span class="ot">TRUE</span>), <span class="dv">5</span>))</span>
<span id="cb612-3"><a href="regression.html#cb612-3"></a>dplyr<span class="op">::</span><span class="kw">arrange</span>(sales_quad[top5, ], <span class="kw">desc</span>(sales_quad[top5, </span>
<span id="cb612-4"><a href="regression.html#cb612-4"></a>    <span class="dv">1</span>]))</span></code></pre></div>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["sales"],"name":[1],"type":["dbl"],"align":["right"]},{"label":["advertising"],"name":[2],"type":["dbl"],"align":["right"]},{"label":["Prediction"],"name":[3],"type":["dbl"],"align":["right"]},{"label":["pred_lin_reg"],"name":[4],"type":["dbl"],"align":["right"]}],"data":[{"1":"6.5310412","2":"71.83","3":"5.6887126","4":"4.5906120"},{"1":"6.3341788","2":"80.98","3":"5.5686272","4":"4.7559082"},{"1":"6.3257561","2":"62.82","3":"5.6433854","4":"4.4278448"},{"1":"6.3222325","2":"61.73","3":"5.6268960","4":"4.4081538"},{"1":"6.3102048","2":"67.95","3":"5.6890920","4":"4.5205191"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<p>When interpreting the coefficients of the predictor in this model we have to be careful. Since we included the squared term, the slope is now different at each level of production (this can be seen in the graph above). That is, we do not have a single coefficient to interpret as the slope anymore. This can easily be shown by calculating the derivative of the model with respect to production.</p>
<p><span class="math display">\[
\text{Sales} = \alpha + \beta_1 \text{ Advertising} + \beta_2 \text{ Advertising}^2 + \varepsilon\\
{\delta \text{ Sales} \over \delta \text{ Advertising}} = \beta_1 + 2 \beta_2 \text{ Advertising} \equiv \text{Slope}
\]</span></p>
<p>Intuitively, this means that the change of sales due to an additional Euro spent on advertising depends on the current level of advertising. <span class="math inline">\(\alpha\)</span>, the intercept can still be interpreted as the expected value of sales given that we do not advertise at all (set advertising to 0 in the model). The sign of the squared term (<span class="math inline">\(\beta_2\)</span>) can be used to determine the curvature of the function. If the sign is positive, the function is convex (curvature is upwards), if it is negative it is concave curvature is downwards). We can interpret <span class="math inline">\(\beta_1\)</span> and <span class="math inline">\(\beta_2\)</span> separately in terms of their influence on the <em>slope</em>. By setting advertising to <span class="math inline">\(0\)</span> we observe that <span class="math inline">\(\beta_1\)</span> is the slope at the origin. By taking the derivative of the slope with respect to advertising we see that the change of the slope due to additional spending on advertising is two times <span class="math inline">\(\beta_2\)</span>.</p>
<p><span class="math display">\[
{\delta Slope \over \delta Advertising} = 2\beta_2
\]</span></p>
<p>At the maximum predicted value the slope is close to <span class="math inline">\(0\)</span> (theoretically it is equal to <span class="math inline">\(0\)</span> but this would require decimals and we can only sell whole pieces). Above we only calculated the prediction for the observed data, so let’s first predict the profit for all possible values between <span class="math inline">\(1\)</span> and <span class="math inline">\(200\)</span> to get the optimal production level according to our model.</p>
<div class="sourceCode" id="cb613"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb613-1"><a href="regression.html#cb613-1"></a>predictionAll &lt;-<span class="st"> </span><span class="kw">predict</span>(quad_mod, <span class="dt">newdata =</span> <span class="kw">data.frame</span>(<span class="dt">advertising =</span> <span class="dv">1</span><span class="op">:</span><span class="dv">200</span>))</span>
<span id="cb613-2"><a href="regression.html#cb613-2"></a>(optimalAdvertising &lt;-<span class="st"> </span><span class="kw">as.integer</span>(<span class="kw">which.max</span>(predictionAll)))</span></code></pre></div>
<pre><code>## [1] 70</code></pre>
<div class="sourceCode" id="cb615"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb615-1"><a href="regression.html#cb615-1"></a><span class="co"># Slope at optimum:</span></span>
<span id="cb615-2"><a href="regression.html#cb615-2"></a><span class="kw">coef</span>(quad_mod)[[<span class="st">&quot;advertising&quot;</span>]] <span class="op">+</span><span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span><span class="kw">coef</span>(quad_mod)[[<span class="st">&quot;I(advertising^2)&quot;</span>]] <span class="op">*</span><span class="st"> </span></span>
<span id="cb615-3"><a href="regression.html#cb615-3"></a><span class="st">    </span>optimalAdvertising</span></code></pre></div>
<pre><code>## [1] -0.00031771725</code></pre>
<p>For all other levels of advertising we insert the pieces produced into the formula to obtain the slope at that point. In the following example you can choose the level of advertising.</p>
<p>
<iframe src="https://learn.wu.ac.at/shiny/imsm/sqrag/" style="border: medium none; width: 100%; height: 650px; transform: scale(1.0);">
</iframe>
</p>

</div>
</div>
</div>
<div id="logistic-regression" class="section level2">
<h2><span class="header-section-number">7.6</span> Logistic regression</h2>
<br>
<div align="center">
<iframe width="560" height="315" src="https://www.youtube.com/embed/J7T7_ulyQ0I" frameborder="0" allowfullscreen>
</iframe>
</div>
<p><br></p>
<div id="motivation-and-intuition" class="section level3">
<h3><span class="header-section-number">7.6.1</span> Motivation and intuition</h3>
<p>In the last section we saw how to predict continuous outcomes (sales, height, etc.) via linear regression models. Another interesting case is that of binary outcomes, i.e. when the variable we want to model can only take two values (yes or no, group 1 or group 2, dead or alive, etc.). To this end we would like to estimate how our predictor variables change the probability of a value being 0 or 1. In this case we can technically still use a linear model (e.g. OLS). However, its predictions will most likely not be particularly useful. A more useful method is the logistic regression. In particular we are going to have a look at the logit model. In the following dataset we are trying to predict whether a song will be a top-10 hit on a popular music streaming platform. In a first step we are going to use only the danceability index as a predictor. Later we are going to add more independent variables.</p>
<div class="sourceCode" id="cb617"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb617-1"><a href="regression.html#cb617-1"></a><span class="kw">library</span>(ggplot2)</span>
<span id="cb617-2"><a href="regression.html#cb617-2"></a><span class="kw">library</span>(gridExtra)</span>
<span id="cb617-3"><a href="regression.html#cb617-3"></a></span>
<span id="cb617-4"><a href="regression.html#cb617-4"></a>chart_data &lt;-<span class="st"> </span><span class="kw">read.delim2</span>(<span class="st">&quot;https://raw.githubusercontent.com/IMSMWU/MRDA2018/master/data/chart_data_logistic.dat&quot;</span>,<span class="dt">header=</span>T, <span class="dt">sep =</span> <span class="st">&quot;</span><span class="ch">\t</span><span class="st">&quot;</span>,<span class="dt">stringsAsFactors =</span> F, <span class="dt">dec =</span> <span class="st">&quot;.&quot;</span>)</span>
<span id="cb617-5"><a href="regression.html#cb617-5"></a><span class="co">#Create a new dummy variable &quot;top10&quot;, which is 1 if a song made it to the top10 and 0 else:</span></span>
<span id="cb617-6"><a href="regression.html#cb617-6"></a>chart_data<span class="op">$</span>top10 &lt;-<span class="st"> </span><span class="kw">ifelse</span>(chart_data<span class="op">$</span>rank<span class="op">&lt;</span><span class="dv">11</span>,<span class="dv">1</span>,<span class="dv">0</span>)</span>
<span id="cb617-7"><a href="regression.html#cb617-7"></a></span>
<span id="cb617-8"><a href="regression.html#cb617-8"></a><span class="co"># Inspect data</span></span>
<span id="cb617-9"><a href="regression.html#cb617-9"></a><span class="kw">head</span>(chart_data)</span></code></pre></div>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["artistName"],"name":[1],"type":["chr"],"align":["left"]},{"label":["trackID"],"name":[2],"type":["chr"],"align":["left"]},{"label":["trackName"],"name":[3],"type":["chr"],"align":["left"]},{"label":["rank"],"name":[4],"type":["int"],"align":["right"]},{"label":["streams"],"name":[5],"type":["int"],"align":["right"]},{"label":["frequency"],"name":[6],"type":["int"],"align":["right"]},{"label":["danceability"],"name":[7],"type":["dbl"],"align":["right"]},{"label":["energy"],"name":[8],"type":["dbl"],"align":["right"]},{"label":["key"],"name":[9],"type":["int"],"align":["right"]},{"label":["loudness"],"name":[10],"type":["dbl"],"align":["right"]},{"label":["speechiness"],"name":[11],"type":["dbl"],"align":["right"]},{"label":["acousticness"],"name":[12],"type":["dbl"],"align":["right"]},{"label":["instrumentalness"],"name":[13],"type":["dbl"],"align":["right"]},{"label":["liveness"],"name":[14],"type":["dbl"],"align":["right"]},{"label":["valence"],"name":[15],"type":["dbl"],"align":["right"]},{"label":["tempo"],"name":[16],"type":["dbl"],"align":["right"]},{"label":["duration_ms"],"name":[17],"type":["int"],"align":["right"]},{"label":["time_signature"],"name":[18],"type":["int"],"align":["right"]},{"label":["isrc"],"name":[19],"type":["chr"],"align":["left"]},{"label":["spotifyArtistID"],"name":[20],"type":["chr"],"align":["left"]},{"label":["releaseDate"],"name":[21],"type":["chr"],"align":["left"]},{"label":["daysSinceRelease"],"name":[22],"type":["int"],"align":["right"]},{"label":["spotifyFollowers"],"name":[23],"type":["int"],"align":["right"]},{"label":["mbid"],"name":[24],"type":["chr"],"align":["left"]},{"label":["artistCountry"],"name":[25],"type":["chr"],"align":["left"]},{"label":["indicator"],"name":[26],"type":["int"],"align":["right"]},{"label":["top10"],"name":[27],"type":["dbl"],"align":["right"]}],"data":[{"1":"dj mustard","2":"01gNiOqg8u7vT90uVgOVmz","3":"Whole Lotta Lovin'","4":"120","5":"917710","6":"3","7":"0.438","8":"0.399","9":"4","10":"-8.752","11":"0.0623","12":"0.1540","13":"0.00000845","14":"0.0646","15":"0.382","16":"160.159","17":"299160","18":"5","19":"QMJMT1500808","20":"0YinUQ50QDB7ZxSCLyQ40k","21":"08.01.2016","22":"450","23":"139718","24":"0612bcce-e351-40be-b3d7-2bb5e1c23479","25":"US","26":"1","27":"0"},{"1":"bing crosby","2":"01h424WG38dgY34vkI3Yd0","3":"White Christmas","4":"70","5":"1865526","6":"9","7":"0.225","8":"0.248","9":"9","10":"-15.871","11":"0.0337","12":"0.9120","13":"0.00014300","14":"0.4040","15":"0.185","16":"96.013","17":"183613","18":"4","19":"USMC14750470","20":"6ZjFtWeHP9XN7FeKSUe80S","21":"27.08.2007","22":"1000","23":"123135","24":"2437980f-513a-44fc-80f1-b90d9d7fcf8f","25":"US","26":"1","27":"0"},{"1":"post malone","2":"02opp1cycqiFNDpLd2o1J3","3":"Big Lie","4":"129","5":"1480436","6":"1","7":"0.325","8":"0.689","9":"6","10":"-4.951","11":"0.2430","12":"0.1970","13":"0.00000000","14":"0.0722","15":"0.225","16":"77.917","17":"207680","18":"4","19":"USUM71614468","20":"246dkjvS1zLTtiykXe5h60","21":"09.12.2016","22":"114","23":"629600","24":"b1e26560-60e5-4236-bbdb-9aa5a8d5ee19","25":"0","26":"1","27":"0"},{"1":"chris brown","2":"02yRHV9Cgk8CUS2fx9lKVC","3":"Anyway","4":"130","5":"894216","6":"1","7":"0.469","8":"0.664","9":"7","10":"-7.160","11":"0.1210","12":"0.0566","13":"0.00000158","14":"0.4820","15":"0.267","16":"124.746","17":"211413","18":"4","19":"USRC11502943","20":"7bXgB6jMjp9ATFy66eO08Z","21":"11.12.2015","22":"478","23":"4077185","24":"c234fa42-e6a6-443e-937e-2f4b073538a3","25":"US","26":"1","27":"0"},{"1":"5 seconds of summer","2":"0375PEO6HIwCHx5Y2sowQm","3":"Waste The Night","4":"182","5":"642784","6":"1","7":"0.286","8":"0.907","9":"8","10":"-4.741","11":"0.1130","12":"0.0144","13":"0.00000000","14":"0.2680","15":"0.271","16":"75.640","17":"266640","18":"4","19":"GBUM71505159","20":"5Rl15oVamLq7FbSb0NNBNy","21":"23.10.2015","22":"527","23":"2221348","24":"830e5c4e-6b7d-431d-86ab-00c751281dc5","25":"AU","26":"1","27":"0"},{"1":"rihanna","2":"046irIGshCqu24AjmEWZtr","3":"Same Olâ\\200\\231 Mistakes","4":"163","5":"809256","6":"2","7":"0.447","8":"0.795","9":"8","10":"-5.435","11":"0.0443","12":"0.2110","13":"0.00169000","14":"0.0725","15":"0.504","16":"151.277","17":"397093","18":"4","19":"QM5FT1600108","20":"5pKCCKE2ajJHZ9KAiaK11H","21":"29.01.2016","22":"429","23":"9687258","24":"73e5e69d-3554-40d8-8516-00cb38737a1c","25":"0","26":"1","27":"0"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<div class="sourceCode" id="cb618"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb618-1"><a href="regression.html#cb618-1"></a><span class="kw">str</span>(chart_data)</span></code></pre></div>
<pre><code>## &#39;data.frame&#39;:    422 obs. of  27 variables:
##  $ artistName      : chr  &quot;dj mustard&quot; &quot;bing crosby&quot; &quot;post malone&quot; &quot;chris brown&quot; ...
##  $ trackID         : chr  &quot;01gNiOqg8u7vT90uVgOVmz&quot; &quot;01h424WG38dgY34vkI3Yd0&quot; &quot;02opp1cycqiFNDpLd2o1J3&quot; &quot;02yRHV9Cgk8CUS2fx9lKVC&quot; ...
##  $ trackName       : chr  &quot;Whole Lotta Lovin&#39;&quot; &quot;White Christmas&quot; &quot;Big Lie&quot; &quot;Anyway&quot; ...
##  $ rank            : int  120 70 129 130 182 163 12 86 67 77 ...
##  $ streams         : int  917710 1865526 1480436 894216 642784 809256 3490456 1737890 1914768 1056689 ...
##  $ frequency       : int  3 9 1 1 1 2 2 12 17 11 ...
##  $ danceability    : num  0.438 0.225 0.325 0.469 0.286 0.447 0.337 0.595 0.472 0.32 ...
##  $ energy          : num  0.399 0.248 0.689 0.664 0.907 0.795 0.615 0.662 0.746 0.752 ...
##  $ key             : int  4 9 6 7 8 8 9 11 6 6 ...
##  $ loudness        : num  -8.75 -15.87 -4.95 -7.16 -4.74 ...
##  $ speechiness     : num  0.0623 0.0337 0.243 0.121 0.113 0.0443 0.0937 0.0362 0.119 0.056 ...
##  $ acousticness    : num  0.154 0.912 0.197 0.0566 0.0144 0.211 0.0426 0.0178 0.072 0.289 ...
##  $ instrumentalness: num  0.00000845 0.000143 0 0.00000158 0 0.00169 0.0000167 0 0 0.000101 ...
##  $ liveness        : num  0.0646 0.404 0.0722 0.482 0.268 0.0725 0.193 0.0804 0.116 0.102 ...
##  $ valence         : num  0.382 0.185 0.225 0.267 0.271 0.504 0.0729 0.415 0.442 0.398 ...
##  $ tempo           : num  160.2 96 77.9 124.7 75.6 ...
##  $ duration_ms     : int  299160 183613 207680 211413 266640 397093 199973 218447 196040 263893 ...
##  $ time_signature  : int  5 4 4 4 4 4 4 4 4 4 ...
##  $ isrc            : chr  &quot;QMJMT1500808&quot; &quot;USMC14750470&quot; &quot;USUM71614468&quot; &quot;USRC11502943&quot; ...
##  $ spotifyArtistID : chr  &quot;0YinUQ50QDB7ZxSCLyQ40k&quot; &quot;6ZjFtWeHP9XN7FeKSUe80S&quot; &quot;246dkjvS1zLTtiykXe5h60&quot; &quot;7bXgB6jMjp9ATFy66eO08Z&quot; ...
##  $ releaseDate     : chr  &quot;08.01.2016&quot; &quot;27.08.2007&quot; &quot;09.12.2016&quot; &quot;11.12.2015&quot; ...
##  $ daysSinceRelease: int  450 1000 114 478 527 429 506 132 291 556 ...
##  $ spotifyFollowers: int  139718 123135 629600 4077185 2221348 9687258 8713999 39723 4422933 3462797 ...
##  $ mbid            : chr  &quot;0612bcce-e351-40be-b3d7-2bb5e1c23479&quot; &quot;2437980f-513a-44fc-80f1-b90d9d7fcf8f&quot; &quot;b1e26560-60e5-4236-bbdb-9aa5a8d5ee19&quot; &quot;c234fa42-e6a6-443e-937e-2f4b073538a3&quot; ...
##  $ artistCountry   : chr  &quot;US&quot; &quot;US&quot; &quot;0&quot; &quot;US&quot; ...
##  $ indicator       : int  1 1 1 1 1 1 1 1 1 1 ...
##  $ top10           : num  0 0 0 0 0 0 0 0 0 0 ...</code></pre>
<p>Below are two attempts to model the data. The left assumes a linear probability model (calculated with the same methods that we used in the last chapter), while the right model is a <strong>logistic regression model</strong>. As you can see, the linear probability model produces probabilities that are above 1 and below 0, which are not valid probabilities, while the logistic model stays between 0 and 1. Notice that songs with a higher danceability index (on the right of the x-axis) seem to cluster more at <span class="math inline">\(1\)</span> and those with a lower more at <span class="math inline">\(0\)</span> so we expect a positive influence of danceability on the probability of a song to become a top-10 hit.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-4"></span>
<img src="11-Logistic_Regression_files/figure-html/unnamed-chunk-4-1.png" alt="The same binary data explained by two models; A linear probability model (on the left) and a logistic regression model (on the right)" width="672" />
<p class="caption">
Figure 1.3: The same binary data explained by two models; A linear probability model (on the left) and a logistic regression model (on the right)
</p>
</div>
<p>A key insight at this point is that the connection between <span class="math inline">\(\mathbf{X}\)</span> and <span class="math inline">\(Y\)</span> is <strong>non-linear</strong> in the logistic regression model. As we can see in the plot, the probability of success is most strongly affected by danceability around values of <span class="math inline">\(0.5\)</span>, while higher and lower values have a smaller marginal effect. This obviously also has consequences for the interpretation of the coefficients later on.</p>
</div>
<div id="technical-details-of-the-model" class="section level3">
<h3><span class="header-section-number">7.6.2</span> Technical details of the model</h3>
<p>As the name suggests, the logistic function is an important component of the logistic regression model. It has the following form:</p>
<p><span class="math display">\[
f(\mathbf{X}) = \frac{1}{1 + e^{-\mathbf{X}}}
\]</span>
This function transforms all real numbers into the range between 0 and 1. We need this to model probabilities, as probabilities can only be between 0 and 1.</p>
<p><img src="11-Logistic_Regression_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<p>The logistic function on its own is not very useful yet, as we want to be able to determine how predictors influence the probability of a value to be equal to 1. To this end we replace the <span class="math inline">\(\mathbf{X}\)</span> in the function above with our familiar linear specification, i.e.</p>
<p><span class="math display">\[
\mathbf{X} = \beta_0 + \beta_1 * x_{1,i} + \beta_2 * x_{2,i} + ... +\beta_m * x_{m,i}\\
f(\mathbf{X}) = P(y_i = 1) = \frac{1}{1 + e^{-(\beta_0 + \beta_1 * x_{1,i} + \beta_2 * x_{2,i} + ... +\beta_m * x_{m,i})}}
\]</span></p>
<p>In our case we only have <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>, the coefficient associated with danceability.</p>
<p>In general we now have a mathematical relationship between our predictor variables <span class="math inline">\((x_1, ..., x_m)\)</span> and the probability of <span class="math inline">\(y_i\)</span> being equal to one. The last step is to estimate the parameters of this model <span class="math inline">\((\beta_0, \beta_1, ..., \beta_m)\)</span> to determine the magnitude of the effects.</p>
</div>
<div id="estimation-in-r" class="section level3">
<h3><span class="header-section-number">7.6.3</span> Estimation in R</h3>
<p>We are now going to show how to perform logistic regression in R. Instead of <code>lm()</code> we now use <code>glm(Y~X, family=binomial(link = 'logit'))</code> to use the logit model. We can still use the <code>summary()</code> command to inspect the output of the model.</p>
<div class="sourceCode" id="cb620"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb620-1"><a href="regression.html#cb620-1"></a><span class="co">#Run the glm</span></span>
<span id="cb620-2"><a href="regression.html#cb620-2"></a>logit_model &lt;-<span class="st"> </span><span class="kw">glm</span>(top10 <span class="op">~</span><span class="st"> </span>danceability,<span class="dt">family=</span><span class="kw">binomial</span>(<span class="dt">link=</span><span class="st">&#39;logit&#39;</span>),<span class="dt">data=</span>chart_data)</span>
<span id="cb620-3"><a href="regression.html#cb620-3"></a><span class="co">#Inspect model summary</span></span>
<span id="cb620-4"><a href="regression.html#cb620-4"></a><span class="kw">summary</span>(logit_model )</span></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = top10 ~ danceability, family = binomial(link = &quot;logit&quot;), 
##     data = chart_data)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.8852  -0.5011  -0.2385   0.2932   2.8196  
## 
## Coefficients:
##              Estimate Std. Error z value            Pr(&gt;|z|)    
## (Intercept)  -10.0414     0.8963  -11.20 &lt;0.0000000000000002 ***
## danceability  17.0939     1.6016   10.67 &lt;0.0000000000000002 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 539.05  on 421  degrees of freedom
## Residual deviance: 258.49  on 420  degrees of freedom
## AIC: 262.49
## 
## Number of Fisher Scoring iterations: 6</code></pre>
<p>Noticeably this output does not include an <span class="math inline">\(R^2\)</span> value to asses model fit. Multiple “Pseudo <span class="math inline">\(R^2\)</span>s”, similar to the one used in OLS, have been developed. There are packages that return the <span class="math inline">\(R^2\)</span> given a logit model (see <code>rcompanion</code> or <code>pscl</code>). The calculation by hand is also fairly simple. We define the function <code>logisticPseudoR2s()</code> that takes a logit model as an input and returns three popular pseudo <span class="math inline">\(R^2\)</span> values.</p>
<div class="sourceCode" id="cb622"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb622-1"><a href="regression.html#cb622-1"></a>logisticPseudoR2s &lt;-<span class="st"> </span><span class="cf">function</span>(LogModel) {</span>
<span id="cb622-2"><a href="regression.html#cb622-2"></a>  dev &lt;-<span class="st"> </span>LogModel<span class="op">$</span>deviance </span>
<span id="cb622-3"><a href="regression.html#cb622-3"></a>  nullDev &lt;-<span class="st"> </span>LogModel<span class="op">$</span>null.deviance </span>
<span id="cb622-4"><a href="regression.html#cb622-4"></a>  modelN &lt;-<span class="st"> </span><span class="kw">length</span>(LogModel<span class="op">$</span>fitted.values)</span>
<span id="cb622-5"><a href="regression.html#cb622-5"></a>  R.l &lt;-<span class="st">  </span><span class="dv">1</span> <span class="op">-</span><span class="st">  </span>dev <span class="op">/</span><span class="st"> </span>nullDev</span>
<span id="cb622-6"><a href="regression.html#cb622-6"></a>  R.cs &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">-</span><span class="st"> </span><span class="kw">exp</span> ( <span class="op">-</span>(nullDev <span class="op">-</span><span class="st"> </span>dev) <span class="op">/</span><span class="st"> </span>modelN)</span>
<span id="cb622-7"><a href="regression.html#cb622-7"></a>  R.n &lt;-<span class="st"> </span>R.cs <span class="op">/</span><span class="st"> </span>( <span class="dv">1</span> <span class="op">-</span><span class="st"> </span>( <span class="kw">exp</span> (<span class="op">-</span>(nullDev <span class="op">/</span><span class="st"> </span>modelN))))</span>
<span id="cb622-8"><a href="regression.html#cb622-8"></a>  <span class="kw">cat</span>(<span class="st">&quot;Pseudo R^2 for logistic regression</span><span class="ch">\n</span><span class="st">&quot;</span>)</span>
<span id="cb622-9"><a href="regression.html#cb622-9"></a>  <span class="kw">cat</span>(<span class="st">&quot;Hosmer and Lemeshow R^2  &quot;</span>, <span class="kw">round</span>(R.l, <span class="dv">3</span>), <span class="st">&quot;</span><span class="ch">\n</span><span class="st">&quot;</span>)</span>
<span id="cb622-10"><a href="regression.html#cb622-10"></a>  <span class="kw">cat</span>(<span class="st">&quot;Cox and Snell R^2        &quot;</span>, <span class="kw">round</span>(R.cs, <span class="dv">3</span>), <span class="st">&quot;</span><span class="ch">\n</span><span class="st">&quot;</span>)</span>
<span id="cb622-11"><a href="regression.html#cb622-11"></a>  <span class="kw">cat</span>(<span class="st">&quot;Nagelkerke R^2           &quot;</span>, <span class="kw">round</span>(R.n, <span class="dv">3</span>),    <span class="st">&quot;</span><span class="ch">\n</span><span class="st">&quot;</span>)</span>
<span id="cb622-12"><a href="regression.html#cb622-12"></a>}</span>
<span id="cb622-13"><a href="regression.html#cb622-13"></a><span class="co">#Inspect Pseudo R2s</span></span>
<span id="cb622-14"><a href="regression.html#cb622-14"></a><span class="kw">logisticPseudoR2s</span>(logit_model )</span></code></pre></div>
<pre><code>## Pseudo R^2 for logistic regression
## Hosmer and Lemeshow R^2   0.52 
## Cox and Snell R^2         0.486 
## Nagelkerke R^2            0.673</code></pre>
<p>The coefficients of the model give the change in the <a href="https://en.wikipedia.org/wiki/Odds#Statistical_usage">log odds</a> of the dependent variable due to a unit change in the regressor. This makes the exact interpretation of the coefficients difficult, but we can still interpret the signs and the p-values which will tell us if a variable has a significant positive or negative impact on the probability of the dependent variable being <span class="math inline">\(1\)</span>. In order to get the odds ratios we can simply take the exponent of the coefficients.</p>
<div class="sourceCode" id="cb624"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb624-1"><a href="regression.html#cb624-1"></a><span class="kw">exp</span>(<span class="kw">coef</span>(logit_model ))</span></code></pre></div>
<pre><code>##          (Intercept)         danceability 
##        0.00004355897 26532731.71142345294</code></pre>
<p>Notice that the coefficient is extremely large. That is (partly) due to the fact that the danceability variable is constrained to values between <span class="math inline">\(0\)</span> and <span class="math inline">\(1\)</span> and the coefficients are for a unit change. We can make the “unit-change” interpretation more meaningful by multiplying the danceability index by <span class="math inline">\(100\)</span>. This linear transformation does not affect the model fit or the p-values.</p>
<div class="sourceCode" id="cb626"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb626-1"><a href="regression.html#cb626-1"></a><span class="co">#Re-scale independet variable</span></span>
<span id="cb626-2"><a href="regression.html#cb626-2"></a>chart_data<span class="op">$</span>danceability_<span class="dv">100</span> &lt;-<span class="st"> </span>chart_data<span class="op">$</span>danceability<span class="op">*</span><span class="dv">100</span> </span>
<span id="cb626-3"><a href="regression.html#cb626-3"></a><span class="co">#Run the regression model</span></span>
<span id="cb626-4"><a href="regression.html#cb626-4"></a>logit_model &lt;-<span class="st"> </span><span class="kw">glm</span>(top10 <span class="op">~</span><span class="st"> </span>danceability_<span class="dv">100</span>,<span class="dt">family=</span><span class="kw">binomial</span>(<span class="dt">link=</span><span class="st">&#39;logit&#39;</span>),<span class="dt">data=</span>chart_data)</span>
<span id="cb626-5"><a href="regression.html#cb626-5"></a><span class="co">#Inspect model summary</span></span>
<span id="cb626-6"><a href="regression.html#cb626-6"></a><span class="kw">summary</span>(logit_model )</span></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = top10 ~ danceability_100, family = binomial(link = &quot;logit&quot;), 
##     data = chart_data)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.8852  -0.5011  -0.2385   0.2932   2.8196  
## 
## Coefficients:
##                   Estimate Std. Error z value            Pr(&gt;|z|)    
## (Intercept)      -10.04139    0.89629  -11.20 &lt;0.0000000000000002 ***
## danceability_100   0.17094    0.01602   10.67 &lt;0.0000000000000002 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 539.05  on 421  degrees of freedom
## Residual deviance: 258.49  on 420  degrees of freedom
## AIC: 262.49
## 
## Number of Fisher Scoring iterations: 6</code></pre>
<div class="sourceCode" id="cb628"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb628-1"><a href="regression.html#cb628-1"></a><span class="co">#Inspect Pseudo R2s</span></span>
<span id="cb628-2"><a href="regression.html#cb628-2"></a><span class="kw">logisticPseudoR2s</span>(logit_model )</span></code></pre></div>
<pre><code>## Pseudo R^2 for logistic regression
## Hosmer and Lemeshow R^2   0.52 
## Cox and Snell R^2         0.486 
## Nagelkerke R^2            0.673</code></pre>
<div class="sourceCode" id="cb630"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb630-1"><a href="regression.html#cb630-1"></a><span class="co">#Convert coefficients to odds ratios</span></span>
<span id="cb630-2"><a href="regression.html#cb630-2"></a><span class="kw">exp</span>(<span class="kw">coef</span>(logit_model ))</span></code></pre></div>
<pre><code>##      (Intercept) danceability_100 
##    0.00004355897    1.18641825295</code></pre>
<p>We observe that danceability positively affects the likelihood of becoming at top-10 hit. To get the confidence intervals for the coefficients we can use the same function as with OLS</p>
<div class="sourceCode" id="cb632"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb632-1"><a href="regression.html#cb632-1"></a><span class="kw">confint</span>(logit_model)</span></code></pre></div>
<pre><code>##                        2.5 %     97.5 %
## (Intercept)      -11.9208213 -8.3954496
## danceability_100   0.1415602  0.2045529</code></pre>
<p>In order to get a rough idea about the magnitude of the effects we can calculate the partial effects at the mean of the data (that is the effect for the average observation). Alternatively, we can calculate the mean of the effects (that is the average of the individual effects). Both can be done with the <code>logitmfx(...)</code> function from the <code>mfx</code> package. If we set <code>logitmfx(logit_model, data = my_data, atmean = FALSE)</code> we calculate the latter. Setting <code>atmean = TRUE</code> will calculate the former. However, in general we are most interested in the sign and significance of the coefficient.</p>
<div class="sourceCode" id="cb634"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb634-1"><a href="regression.html#cb634-1"></a><span class="kw">library</span>(mfx)</span>
<span id="cb634-2"><a href="regression.html#cb634-2"></a><span class="co"># Average partial effect</span></span>
<span id="cb634-3"><a href="regression.html#cb634-3"></a><span class="kw">logitmfx</span>(logit_model, <span class="dt">data =</span> chart_data, <span class="dt">atmean =</span> <span class="ot">FALSE</span>)</span></code></pre></div>
<pre><code>## Call:
## logitmfx(formula = logit_model, data = chart_data, atmean = FALSE)
## 
## Marginal Effects:
##                      dF/dx Std. Err.      z        P&gt;|z|    
## danceability_100 0.0157310 0.0029761 5.2857 0.0000001252 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>This now gives the average partial effects in percentage points. An additional point on the danceability scale (from <span class="math inline">\(1\)</span> to <span class="math inline">\(100\)</span>), on average, makes it <span class="math inline">\(1.57%\)</span> more likely for a song to become at top-10 hit.</p>
<p>To get the effect of an additional point at a specific value, we can calculate the odds ratio by predicting the probability at a value and at the value <span class="math inline">\(+1\)</span>. For example if we are interested in how much more likely a song with 51 compared to 50 danceability is to become a hit we can simply calculate the following</p>
<div class="sourceCode" id="cb636"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb636-1"><a href="regression.html#cb636-1"></a><span class="co">#Probability of a top 10 hit with a danceability of 50</span></span>
<span id="cb636-2"><a href="regression.html#cb636-2"></a>prob_<span class="dv">50</span> &lt;-<span class="st"> </span><span class="kw">exp</span>(<span class="op">-</span>(<span class="op">-</span><span class="kw">summary</span>(logit_model)<span class="op">$</span>coefficients[<span class="dv">1</span>,<span class="dv">1</span>]<span class="op">-</span><span class="kw">summary</span>(logit_model)<span class="op">$</span>coefficients[<span class="dv">2</span>,<span class="dv">1</span>]<span class="op">*</span><span class="dv">50</span> ))</span>
<span id="cb636-3"><a href="regression.html#cb636-3"></a>prob_<span class="dv">50</span></span></code></pre></div>
<pre><code>## [1] 0.224372</code></pre>
<div class="sourceCode" id="cb638"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb638-1"><a href="regression.html#cb638-1"></a><span class="co">#Probability of a top 10 hit with a danceability of 51</span></span>
<span id="cb638-2"><a href="regression.html#cb638-2"></a>prob_<span class="dv">51</span> &lt;-<span class="st"> </span><span class="kw">exp</span>(<span class="op">-</span>(<span class="op">-</span><span class="kw">summary</span>(logit_model)<span class="op">$</span>coefficients[<span class="dv">1</span>,<span class="dv">1</span>]<span class="op">-</span><span class="kw">summary</span>(logit_model)<span class="op">$</span>coefficients[<span class="dv">2</span>,<span class="dv">1</span>]<span class="op">*</span><span class="dv">51</span> ))</span>
<span id="cb638-3"><a href="regression.html#cb638-3"></a>prob_<span class="dv">51</span></span></code></pre></div>
<pre><code>## [1] 0.266199</code></pre>
<div class="sourceCode" id="cb640"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb640-1"><a href="regression.html#cb640-1"></a><span class="co">#Odds ratio</span></span>
<span id="cb640-2"><a href="regression.html#cb640-2"></a>prob_<span class="dv">51</span><span class="op">/</span>prob_<span class="dv">50</span></span></code></pre></div>
<pre><code>## [1] 1.186418</code></pre>
<p>So the odds are 20% higher at 51 than at 50.</p>
<div id="logistic-model-with-multiple-predictors" class="section level4">
<h4><span class="header-section-number">7.6.3.1</span> Logistic model with multiple predictors</h4>
<p>Of course we can also use multiple predictors in logistic regression as shown in the formula above. We might want to add spotify followers (in million) and weeks since the release of the song.</p>
<div class="sourceCode" id="cb642"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb642-1"><a href="regression.html#cb642-1"></a>chart_data<span class="op">$</span>spotify_followers_m &lt;-<span class="st"> </span>chart_data<span class="op">$</span>spotifyFollowers<span class="op">/</span><span class="dv">1000000</span></span>
<span id="cb642-2"><a href="regression.html#cb642-2"></a>chart_data<span class="op">$</span>weeks_since_release &lt;-<span class="st"> </span>chart_data<span class="op">$</span>daysSinceRelease<span class="op">/</span><span class="dv">7</span></span></code></pre></div>
<p>Again, the familiar formula interface can be used with the <code>glm()</code> function. All the model summaries shown above still work with multiple predictors.</p>
<div class="sourceCode" id="cb643"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb643-1"><a href="regression.html#cb643-1"></a>multiple_logit_model &lt;-<span class="st"> </span><span class="kw">glm</span>(top10 <span class="op">~</span><span class="st"> </span>danceability_<span class="dv">100</span> <span class="op">+</span><span class="st"> </span>spotify_followers_m <span class="op">+</span><span class="st"> </span>weeks_since_release,<span class="dt">family=</span><span class="kw">binomial</span>(<span class="dt">link=</span><span class="st">&#39;logit&#39;</span>),<span class="dt">data=</span>chart_data)</span>
<span id="cb643-2"><a href="regression.html#cb643-2"></a><span class="kw">summary</span>(multiple_logit_model)</span></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = top10 ~ danceability_100 + spotify_followers_m + 
##     weeks_since_release, family = binomial(link = &quot;logit&quot;), data = chart_data)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.8861  -0.4390  -0.2083   0.2311   2.8015  
## 
## Coefficients:
##                      Estimate Std. Error z value             Pr(&gt;|z|)    
## (Intercept)         -9.603762   0.990481  -9.696 &lt; 0.0000000000000002 ***
## danceability_100     0.166236   0.016358  10.162 &lt; 0.0000000000000002 ***
## spotify_followers_m  0.197717   0.060030   3.294             0.000989 ***
## weeks_since_release -0.012976   0.004956  -2.619             0.008832 ** 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 534.91  on 416  degrees of freedom
## Residual deviance: 239.15  on 413  degrees of freedom
##   (5 observations deleted due to missingness)
## AIC: 247.15
## 
## Number of Fisher Scoring iterations: 6</code></pre>
<div class="sourceCode" id="cb645"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb645-1"><a href="regression.html#cb645-1"></a><span class="kw">logisticPseudoR2s</span>(multiple_logit_model)</span></code></pre></div>
<pre><code>## Pseudo R^2 for logistic regression
## Hosmer and Lemeshow R^2   0.553 
## Cox and Snell R^2         0.508 
## Nagelkerke R^2            0.703</code></pre>
<div class="sourceCode" id="cb647"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb647-1"><a href="regression.html#cb647-1"></a><span class="kw">exp</span>(<span class="kw">coef</span>(multiple_logit_model))</span></code></pre></div>
<pre><code>##         (Intercept)    danceability_100 spotify_followers_m weeks_since_release 
##        0.0000674744        1.1808513243        1.2186174345        0.9871076460</code></pre>
<div class="sourceCode" id="cb649"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb649-1"><a href="regression.html#cb649-1"></a><span class="kw">confint</span>(multiple_logit_model)</span></code></pre></div>
<pre><code>##                            2.5 %       97.5 %
## (Intercept)         -11.67983072 -7.782122558
## danceability_100      0.13625795  0.200625438
## spotify_followers_m   0.08079476  0.317115293
## weeks_since_release  -0.02307859 -0.003566462</code></pre>
</div>
<div id="model-selection" class="section level4">
<h4><span class="header-section-number">7.6.3.2</span> Model selection</h4>
<p>The question remains, whether a variable <em>should</em> be added to the model. We will present two methods for model selection for logistic regression. The first is based on the <em>Akaike Information Criterium</em> (AIC). It is reported with the summary output for logit models. The value of the AIC is <strong>relative</strong>, meaning that it has no interpretation by itself. However, it can be used to compare and select models. The model with the lowest AIC value is the one that should be chosen. Note that the AIC does not indicate how well the model fits the data, but is merely used to compare models.</p>
<p>For example, consider the following model, where we exclude the <code>followers</code> covariate. Seeing as it was able to contribute significantly to the explanatory power of the model, the AIC increases, indicating that the model including <code>followers</code> is better suited to explain the data. We always want the lowest possible AIC.</p>
<div class="sourceCode" id="cb651"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb651-1"><a href="regression.html#cb651-1"></a>multiple_logit_model2 &lt;-<span class="st"> </span><span class="kw">glm</span>(top10 <span class="op">~</span><span class="st"> </span>danceability_<span class="dv">100</span> <span class="op">+</span><span class="st"> </span>weeks_since_release,<span class="dt">family=</span><span class="kw">binomial</span>(<span class="dt">link=</span><span class="st">&#39;logit&#39;</span>),<span class="dt">data=</span>chart_data)</span>
<span id="cb651-2"><a href="regression.html#cb651-2"></a></span>
<span id="cb651-3"><a href="regression.html#cb651-3"></a><span class="kw">summary</span>(multiple_logit_model2)</span></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = top10 ~ danceability_100 + weeks_since_release, 
##     family = binomial(link = &quot;logit&quot;), data = chart_data)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.9578  -0.4721  -0.2189   0.2562   2.8759  
## 
## Coefficients:
##                      Estimate Std. Error z value            Pr(&gt;|z|)    
## (Intercept)         -8.980225   0.930654  -9.649 &lt;0.0000000000000002 ***
## danceability_100     0.166498   0.016107  10.337 &lt;0.0000000000000002 ***
## weeks_since_release -0.012805   0.004836  -2.648              0.0081 ** 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 534.91  on 416  degrees of freedom
## Residual deviance: 250.12  on 414  degrees of freedom
##   (5 observations deleted due to missingness)
## AIC: 256.12
## 
## Number of Fisher Scoring iterations: 6</code></pre>
<p>As a second measure for variable selection, you can use the pseudo <span class="math inline">\(R^2\)</span>s as shown above. The fit is distinctly worse according to all three values presented here, when excluding the Spotify followers.</p>
<div class="sourceCode" id="cb653"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb653-1"><a href="regression.html#cb653-1"></a><span class="kw">logisticPseudoR2s</span>(multiple_logit_model2)</span></code></pre></div>
<pre><code>## Pseudo R^2 for logistic regression
## Hosmer and Lemeshow R^2   0.532 
## Cox and Snell R^2         0.495 
## Nagelkerke R^2            0.685</code></pre>
</div>
<div id="predictions" class="section level4">
<h4><span class="header-section-number">7.6.3.3</span> Predictions</h4>
<p>We can predict the probability given an observation using the <code>predict(my_logit, newdata = ..., type = "response")</code> function. Replace <code>...</code> with the observed values for which you would like to predict the outcome variable.</p>
<div class="sourceCode" id="cb655"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb655-1"><a href="regression.html#cb655-1"></a><span class="co"># Prediction for one observation</span></span>
<span id="cb655-2"><a href="regression.html#cb655-2"></a><span class="kw">predict</span>(multiple_logit_model, <span class="dt">newdata =</span> <span class="kw">data.frame</span>(<span class="dt">danceability_100=</span><span class="dv">50</span>, <span class="dt">spotify_followers_m=</span><span class="dv">10</span>, <span class="dt">weeks_since_release=</span><span class="dv">1</span>), <span class="dt">type =</span> <span class="st">&quot;response&quot;</span>)</span></code></pre></div>
<pre><code>##         1 
## 0.6619986</code></pre>
<p>The prediction indicates that a song with danceability of <span class="math inline">\(50\)</span> from an artist with <span class="math inline">\(10M\)</span> Spotify followers has a <span class="math inline">\(66%\)</span> chance of being in the top-10, 1 week after its release.</p>
</div>
<div id="perfect-prediction-logit" class="section level4">
<h4><span class="header-section-number">7.6.3.4</span> Perfect Prediction Logit</h4>
<p>Perfect prediction occurs whenever a linear function of <span class="math inline">\(X\)</span> can perfectly separate the <span class="math inline">\(1\)</span>s from the <span class="math inline">\(0\)</span>s in the dependent variable. This is problematic when estimating a logit model as it will result in biased estimators (also check to p-values in the example!). R will return the following message if this occurs:</p>
<p><code>glm.fit: fitted probabilities numerically 0 or 1 occurred</code></p>
<p>Given this error, one should not use the output of the <code>glm(...)</code> function for the analysis. There are <a href="https://stats.stackexchange.com/a/68917">various ways</a> to deal with this problem, one of which is to use Firth’s bias-reduced penalized-likelihood logistic regression with the <code>logistf(Y~X)</code> function in the <code>logistf</code> package.</p>
<div id="example" class="section level5">
<h5><span class="header-section-number">7.6.3.4.1</span> Example</h5>
<p>In this example data <span class="math inline">\(Y = 0\)</span> if <span class="math inline">\(x_1 &lt;0\)</span> and <span class="math inline">\(Y=1\)</span> if <span class="math inline">\(x_1&gt;0\)</span> and we thus have perfect prediction. As we can see the output of the regular logit model is not interpretable. The standard errors are huge compared to the coefficients and thus the p-values are <span class="math inline">\(1\)</span> despite <span class="math inline">\(x_1\)</span> being a predictor of <span class="math inline">\(Y\)</span>. Thus, we turn to the penalized-likelihood version. This model correctly indicates that <span class="math inline">\(x_1\)</span> is in fact a predictor for <span class="math inline">\(Y\)</span> as the coefficient is significant.</p>
<div class="sourceCode" id="cb657"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb657-1"><a href="regression.html#cb657-1"></a>Y &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb657-2"><a href="regression.html#cb657-2"></a>X &lt;-<span class="st"> </span><span class="kw">cbind</span>(<span class="kw">c</span>(<span class="op">-</span><span class="dv">1</span>,<span class="op">-</span><span class="dv">2</span>,<span class="op">-</span><span class="dv">3</span>,<span class="op">-</span><span class="dv">3</span>,<span class="dv">5</span>,<span class="dv">6</span>,<span class="dv">10</span>,<span class="dv">11</span>),<span class="kw">c</span>(<span class="dv">3</span>,<span class="dv">2</span>,<span class="op">-</span><span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">4</span>,<span class="dv">1</span>,<span class="dv">0</span>))</span>
<span id="cb657-3"><a href="regression.html#cb657-3"></a></span>
<span id="cb657-4"><a href="regression.html#cb657-4"></a><span class="co"># Perfect prediction with regular logit</span></span>
<span id="cb657-5"><a href="regression.html#cb657-5"></a><span class="kw">summary</span>(<span class="kw">glm</span>(Y<span class="op">~</span>X, <span class="dt">family=</span><span class="kw">binomial</span>(<span class="dt">link=</span><span class="st">&quot;logit&quot;</span>)))</span></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = Y ~ X, family = binomial(link = &quot;logit&quot;))
## 
## Deviance Residuals: 
##             1              2              3              4              5  
## -0.0000102197  -0.0000012300  -0.0000033675  -0.0000033675   0.0000105893  
##             6              7              8  
##  0.0000060786   0.0000000211   0.0000000211  
## 
## Coefficients:
##               Estimate Std. Error z value Pr(&gt;|z|)
## (Intercept)     -6.943 113859.814       0        1
## X1               7.359  15925.251       0        1
## X2              -3.125  43853.489       0        1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 11.09035488895912  on 7  degrees of freedom
## Residual deviance:  0.00000000027772  on 5  degrees of freedom
## AIC: 6
## 
## Number of Fisher Scoring iterations: 24</code></pre>
<div class="sourceCode" id="cb659"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb659-1"><a href="regression.html#cb659-1"></a><span class="kw">library</span>(logistf)</span>
<span id="cb659-2"><a href="regression.html#cb659-2"></a><span class="co"># Perfect prediction with penalized-likelihood logit</span></span>
<span id="cb659-3"><a href="regression.html#cb659-3"></a><span class="kw">summary</span>(<span class="kw">logistf</span>(Y<span class="op">~</span>X))</span></code></pre></div>
<pre><code>## logistf(formula = Y ~ X)
## 
## Model fitted by Penalized ML
## Coefficients:
##                    coef  se(coef)  lower 0.95 upper 0.95      Chisq          p
## (Intercept) -0.98871431 1.4283595 -10.2169313   1.884501 0.59231445 0.44152553
## X1           0.33195157 0.2142516   0.0417035   1.463409 5.31583569 0.02113246
## X2           0.08250307 0.6085055  -2.1788866   3.379327 0.01980379 0.88808646
##             method
## (Intercept)      2
## X1               2
## X2               2
## 
## Method: 1-Wald, 2-Profile penalized log-likelihood, 3-None
## 
## Likelihood ratio test=5.800986 on 2 df, p=0.05499609, n=8
## Wald test = 2.706445 on 2 df, p = 0.2584062</code></pre>
</div>
</div>
</div>
</div>
<div id="learning-check-5" class="section level2 unnumbered">
<h2>Learning check</h2>
<p><strong>(LC7.1) What is a correlation coefficient?</strong></p>
<ul>
<li><input type="checkbox" disabled="" />
It describes the difference in means of two variables</li>
<li><input type="checkbox" disabled="" />
It describes the causal relation between two variables</li>
<li><input type="checkbox" disabled="" />
It is the standardized covariance</li>
<li><input type="checkbox" disabled="" />
It describes the degree to which the variation in one variable is related to the variation in another variable</li>
<li><input type="checkbox" disabled="" />
None of the above</li>
</ul>
<p><strong>(LC7.2) Which line through a scatterplot produces the best fit in a linear regression model?</strong></p>
<ul>
<li><input type="checkbox" disabled="" />
The line associated with the steepest slope parameter</li>
<li><input type="checkbox" disabled="" />
The line that minimizes the sum of the squared deviations of the predicted values (regression line) from the observed values</li>
<li><input type="checkbox" disabled="" />
The line that minimizes the sum of the squared residuals</li>
<li><input type="checkbox" disabled="" />
The line that maximizes the sum of the squared residuals</li>
<li><input type="checkbox" disabled="" />
None of the above</li>
</ul>
<p><strong>(LC7.3) What is the interpretation of the regression coefficient (<span class="math inline">\(\beta_1\)</span>=0.05) in a regression model where log(sales) (i.e., log-transformed units) is the dependent variable and log(advertising) (i.e., the log-transformed advertising expenditures in Euro) is the independent variable (i.e., <span class="math inline">\(log(sales)=13.4+0.05∗log(advertising)\)</span>)?</strong></p>
<ul>
<li><input type="checkbox" disabled="" />
An increase in advertising by 1€ leads to an increase in sales by 0.5 units</li>
<li><input type="checkbox" disabled="" />
A 1% increase in advertising leads to a 0.05% increase in sales</li>
<li><input type="checkbox" disabled="" />
A 1% increase in advertising leads to a 5% decrease in sales</li>
<li><input type="checkbox" disabled="" />
An increase in advertising by 1€ leads to an increase in sales by 0.005 units</li>
<li><input type="checkbox" disabled="" />
None of the above</li>
</ul>
<p><strong>(LC7.4) Which of the following statements about the adjusted R-squared is TRUE?</strong></p>
<ul>
<li><input type="checkbox" disabled="" />
It is always larger than the regular <span class="math inline">\(R^{2}\)</span></li>
<li><input type="checkbox" disabled="" />
It increases with every additional variable</li>
<li><input type="checkbox" disabled="" />
It increases only with additional variables that add more explanatory power than pure chance</li>
<li><input type="checkbox" disabled="" />
It contains a “penalty” for including unnecessary variables</li>
<li><input type="checkbox" disabled="" />
None of the above</li>
</ul>
<p><strong>(LC7.5) What does the term overfitting refer to?</strong></p>
<ul>
<li><input type="checkbox" disabled="" />
A regression model that has too many predictor variables</li>
<li><input type="checkbox" disabled="" />
A regression model that fits to a specific data set so poorly, that it will not generalize to other samples</li>
<li><input type="checkbox" disabled="" />
A regression model that fits to a specific data set so well, that it will only predict well within the sample but not generalize to other samples</li>
<li><input type="checkbox" disabled="" />
A regression model that fits to a specific data set so well, that it will generalize to other samples particularly well</li>
<li><input type="checkbox" disabled="" />
None of the above</li>
</ul>
<p><strong>(LC7.6) What are assumptions of the linear regression model?</strong></p>
<ul>
<li><input type="checkbox" disabled="" />
Endogeneity</li>
<li><input type="checkbox" disabled="" />
Independent errors</li>
<li><input type="checkbox" disabled="" />
Heteroscedasticity</li>
<li><input type="checkbox" disabled="" />
Linear dependence of regressors</li>
<li><input type="checkbox" disabled="" />
None of the above</li>
</ul>
<p><strong>(LC7.7) What does the problem of heteroscedasticity in a regression model refer to?</strong></p>
<ul>
<li><input type="checkbox" disabled="" />
The variance of the error term is not constant</li>
<li><input type="checkbox" disabled="" />
A strong linear relationship between the independent variables</li>
<li><input type="checkbox" disabled="" />
The variance of the error term is constant</li>
<li><input type="checkbox" disabled="" />
A correlation between the error term and the independent variables</li>
<li><input type="checkbox" disabled="" />
None of the above</li>
</ul>
<p><strong>(LC7.8) What are properties of the multiplicative regression model (i.e., log-log specification)?</strong></p>
<ul>
<li><input type="checkbox" disabled="" />
Constant marginal returns</li>
<li><input type="checkbox" disabled="" />
Decreasing marginal returns</li>
<li><input type="checkbox" disabled="" />
Constant elasticity</li>
<li><input type="checkbox" disabled="" />
Increasing marginal returns</li>
<li><input type="checkbox" disabled="" />
None of the above</li>
</ul>
<p><strong>(LC7.9) When do you use a logistic regression model?</strong></p>
<ul>
<li><input type="checkbox" disabled="" />
When the dependent variable is continuous</li>
<li><input type="checkbox" disabled="" />
When the independent and dependent variables are binary</li>
<li><input type="checkbox" disabled="" />
When the dependent variable is binary</li>
<li><input type="checkbox" disabled="" />
None of the above</li>
</ul>
<p><strong>(LC7.10) What is the correct way to implement a linear regression model in R? (x = independent variable, y = dependent variable)?</strong></p>
<ul>
<li><input type="checkbox" disabled="" />
<code>lm(y~x, data=data)</code></li>
<li><input type="checkbox" disabled="" />
<code>lm(x~y + error, data=data)</code></li>
<li><input type="checkbox" disabled="" />
<code>lm(x~y, data=data)</code></li>
<li><input type="checkbox" disabled="" />
<code>lm(y~x + error, data=data)</code></li>
<li><input type="checkbox" disabled="" />
None of the above</li>
</ul>
</div>
<div id="references-4" class="section level2 unnumbered">
<h2>References</h2>
<ul>
<li>Field, A., Miles J., &amp; Field, Z. (2012): Discovering Statistics Using R. Sage Publications (<strong>chapters 6, 7, 8</strong>).</li>
<li>James, G., Witten, D., Hastie, T., &amp; Tibshirani, R. (2013): An Introduction to Statistical Learning with Applications in R, Springer (<strong>chapter 3</strong>)</li>
</ul>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="hypothesis-testing.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="exploratory-factor-analysis.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
