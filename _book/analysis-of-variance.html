<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Marketing Research Design &amp; Analysis 2017</title>
  <meta name="description" content="An Introduction to Statistics Using R">
  <meta name="generator" content="bookdown 0.7 and GitBook 2.6.7">

  <meta property="og:title" content="Marketing Research Design &amp; Analysis 2017" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="An Introduction to Statistics Using R" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Marketing Research Design &amp; Analysis 2017" />
  
  <meta name="twitter:description" content="An Introduction to Statistics Using R" />
  




  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="hypothesis-testing.html">
<link rel="next" href="regression.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<link href="libs/pagedtable-1.1/css/pagedtable.css" rel="stylesheet" />
<script src="libs/pagedtable-1.1/js/pagedtable.js"></script>
<script src="libs/htmlwidgets-1.2/htmlwidgets.js"></script>
<script src="libs/plotly-binding-4.7.1/plotly.js"></script>
<script src="libs/typedarray-0.1/typedarray.min.js"></script>
<link href="libs/crosstalk-1.0.0/css/crosstalk.css" rel="stylesheet" />
<script src="libs/crosstalk-1.0.0/js/crosstalk.min.js"></script>
<link href="libs/plotlyjs-1.29.2/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotlyjs-1.29.2/plotly-latest.min.js"></script>


<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><strong><a href="./">MRDA 2017</a></strong></li>
<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Welcome!</a></li>
<li class="chapter" data-level="" data-path="course-materials.html"><a href="course-materials.html"><i class="fa fa-check"></i>Course materials</a><ul>
<li class="chapter" data-level="" data-path="course-materials.html"><a href="course-materials.html#main-reference"><i class="fa fa-check"></i>Main reference</a></li>
<li class="chapter" data-level="" data-path="course-materials.html"><a href="course-materials.html#further-readings"><i class="fa fa-check"></i>Further readings</a></li>
<li class="chapter" data-level="" data-path="course-materials.html"><a href="course-materials.html#datacamp"><i class="fa fa-check"></i>DataCamp</a></li>
<li class="chapter" data-level="" data-path="course-materials.html"><a href="course-materials.html#other-web-resources"><i class="fa fa-check"></i>Other web-resources</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="getting-started.html"><a href="getting-started.html"><i class="fa fa-check"></i><b>1</b> Getting started</a><ul>
<li class="chapter" data-level="1.1" data-path="getting-started.html"><a href="getting-started.html#why-r"><i class="fa fa-check"></i><b>1.1</b> Why R?</a></li>
<li class="chapter" data-level="1.2" data-path="getting-started.html"><a href="getting-started.html#how-to-download-and-install-r-and-rstudio"><i class="fa fa-check"></i><b>1.2</b> How to download and install R and RStudio</a></li>
<li class="chapter" data-level="1.3" data-path="getting-started.html"><a href="getting-started.html#getting-help"><i class="fa fa-check"></i><b>1.3</b> Getting help</a></li>
<li class="chapter" data-level="1.4" data-path="getting-started.html"><a href="getting-started.html#functions"><i class="fa fa-check"></i><b>1.4</b> Functions</a></li>
<li class="chapter" data-level="1.5" data-path="getting-started.html"><a href="getting-started.html#packages"><i class="fa fa-check"></i><b>1.5</b> Packages</a></li>
<li class="chapter" data-level="1.6" data-path="getting-started.html"><a href="getting-started.html#a-typical-r-session"><i class="fa fa-check"></i><b>1.6</b> A typical R session</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="basic-data-handling.html"><a href="basic-data-handling.html"><i class="fa fa-check"></i><b>2</b> Basic data handling</a><ul>
<li class="chapter" data-level="2.1" data-path="basic-data-handling.html"><a href="basic-data-handling.html#creating-objects"><i class="fa fa-check"></i><b>2.1</b> Creating objects</a></li>
<li class="chapter" data-level="2.2" data-path="basic-data-handling.html"><a href="basic-data-handling.html#data-types"><i class="fa fa-check"></i><b>2.2</b> Data types</a></li>
<li class="chapter" data-level="2.3" data-path="basic-data-handling.html"><a href="basic-data-handling.html#data-structures"><i class="fa fa-check"></i><b>2.3</b> Data structures</a><ul>
<li class="chapter" data-level="2.3.1" data-path="basic-data-handling.html"><a href="basic-data-handling.html#accessing-data-in-data-frames"><i class="fa fa-check"></i><b>2.3.1</b> Accessing data in data frames</a></li>
<li class="chapter" data-level="2.3.2" data-path="basic-data-handling.html"><a href="basic-data-handling.html#inspecting-the-content-of-a-data-frame"><i class="fa fa-check"></i><b>2.3.2</b> Inspecting the content of a data frame</a></li>
<li class="chapter" data-level="2.3.3" data-path="basic-data-handling.html"><a href="basic-data-handling.html#append-and-delete-variables-tofrom-data-frames"><i class="fa fa-check"></i><b>2.3.3</b> Append and delete variables to/from data frames</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="advanced-data-handling.html"><a href="advanced-data-handling.html"><i class="fa fa-check"></i><b>3</b> Advanced data handling</a><ul>
<li class="chapter" data-level="3.1" data-path="advanced-data-handling.html"><a href="advanced-data-handling.html#the-dplyr-package"><i class="fa fa-check"></i><b>3.1</b> The dplyr package</a><ul>
<li class="chapter" data-level="3.1.1" data-path="advanced-data-handling.html"><a href="advanced-data-handling.html#filter-rows"><i class="fa fa-check"></i><b>3.1.1</b> Filter rows</a></li>
<li class="chapter" data-level="3.1.2" data-path="advanced-data-handling.html"><a href="advanced-data-handling.html#select-columns"><i class="fa fa-check"></i><b>3.1.2</b> Select columns</a></li>
<li class="chapter" data-level="3.1.3" data-path="advanced-data-handling.html"><a href="advanced-data-handling.html#rename-columns"><i class="fa fa-check"></i><b>3.1.3</b> Rename columns</a></li>
<li class="chapter" data-level="3.1.4" data-path="advanced-data-handling.html"><a href="advanced-data-handling.html#arrange-rows"><i class="fa fa-check"></i><b>3.1.4</b> Arrange rows</a></li>
<li class="chapter" data-level="3.1.5" data-path="advanced-data-handling.html"><a href="advanced-data-handling.html#adding-and-changing-variables"><i class="fa fa-check"></i><b>3.1.5</b> Adding and changing variables</a></li>
<li class="chapter" data-level="3.1.6" data-path="advanced-data-handling.html"><a href="advanced-data-handling.html#changing-variables"><i class="fa fa-check"></i><b>3.1.6</b> Changing variables</a></li>
<li class="chapter" data-level="3.1.7" data-path="advanced-data-handling.html"><a href="advanced-data-handling.html#pipes"><i class="fa fa-check"></i><b>3.1.7</b> Pipes</a></li>
<li class="chapter" data-level="3.1.8" data-path="advanced-data-handling.html"><a href="advanced-data-handling.html#creating-custom-summaries"><i class="fa fa-check"></i><b>3.1.8</b> Creating custom summaries</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="advanced-data-handling.html"><a href="advanced-data-handling.html#dealing-with-strings"><i class="fa fa-check"></i><b>3.2</b> Dealing with strings</a><ul>
<li class="chapter" data-level="3.2.1" data-path="advanced-data-handling.html"><a href="advanced-data-handling.html#the-stringr-package"><i class="fa fa-check"></i><b>3.2.1</b> The <code>stringr</code> package</a></li>
<li class="chapter" data-level="3.2.2" data-path="advanced-data-handling.html"><a href="advanced-data-handling.html#a-crash-course-in-regex"><i class="fa fa-check"></i><b>3.2.2</b> A crash course in regex</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="advanced-data-handling.html"><a href="advanced-data-handling.html#case-study"><i class="fa fa-check"></i><b>3.3</b> Case study</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="data-import-and-export.html"><a href="data-import-and-export.html"><i class="fa fa-check"></i><b>4</b> Data import and export</a><ul>
<li class="chapter" data-level="4.1" data-path="data-import-and-export.html"><a href="data-import-and-export.html#getting-data-for-this-course"><i class="fa fa-check"></i><b>4.1</b> Getting data for this course</a><ul>
<li class="chapter" data-level="4.1.1" data-path="data-import-and-export.html"><a href="data-import-and-export.html#directly-import-datasets-from-github-recommended"><i class="fa fa-check"></i><b>4.1.1</b> Directly import datasets from GitHub (recommended)</a></li>
<li class="chapter" data-level="4.1.2" data-path="data-import-and-export.html"><a href="data-import-and-export.html#download-and-import-datasets-from-learnwu"><i class="fa fa-check"></i><b>4.1.2</b> Download and import datasets from “Learn@WU”</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="data-import-and-export.html"><a href="data-import-and-export.html#import-data-created-by-other-software-packages"><i class="fa fa-check"></i><b>4.2</b> Import data created by other software packages</a></li>
<li class="chapter" data-level="4.3" data-path="data-import-and-export.html"><a href="data-import-and-export.html#export-data"><i class="fa fa-check"></i><b>4.3</b> Export data</a></li>
<li class="chapter" data-level="4.4" data-path="data-import-and-export.html"><a href="data-import-and-export.html#import-data-from-the-web"><i class="fa fa-check"></i><b>4.4</b> Import data from the Web</a><ul>
<li class="chapter" data-level="4.4.1" data-path="data-import-and-export.html"><a href="data-import-and-export.html#scraping-data-from-websites"><i class="fa fa-check"></i><b>4.4.1</b> Scraping data from websites</a></li>
<li class="chapter" data-level="4.4.2" data-path="data-import-and-export.html"><a href="data-import-and-export.html#scraping-data-from-apis"><i class="fa fa-check"></i><b>4.4.2</b> Scraping data from APIs</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="random-variables.html"><a href="random-variables.html"><i class="fa fa-check"></i><b>5</b> Random Variables</a><ul>
<li class="chapter" data-level="5.0.1" data-path="random-variables.html"><a href="random-variables.html#why-random-variables"><i class="fa fa-check"></i><b>5.0.1</b> Why Random Variables?</a></li>
<li class="chapter" data-level="5.0.2" data-path="random-variables.html"><a href="random-variables.html#tossing-coins"><i class="fa fa-check"></i><b>5.0.2</b> Tossing coins</a></li>
<li class="chapter" data-level="5.0.3" data-path="random-variables.html"><a href="random-variables.html#sum-of-two-dice"><i class="fa fa-check"></i><b>5.0.3</b> Sum of two dice</a></li>
<li class="chapter" data-level="5.0.4" data-path="random-variables.html"><a href="random-variables.html#discrete-random-variables"><i class="fa fa-check"></i><b>5.0.4</b> Discrete Random Variables</a></li>
<li class="chapter" data-level="5.0.5" data-path="random-variables.html"><a href="random-variables.html#continuous-case"><i class="fa fa-check"></i><b>5.0.5</b> Continuous Case</a></li>
<li class="chapter" data-level="5.0.6" data-path="random-variables.html"><a href="random-variables.html#definitions"><i class="fa fa-check"></i><b>5.0.6</b> Definitions</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="probability-distributions.html"><a href="probability-distributions.html"><i class="fa fa-check"></i><b>6</b> Probability Distributions</a><ul>
<li class="chapter" data-level="6.1" data-path="probability-distributions.html"><a href="probability-distributions.html#introduction"><i class="fa fa-check"></i><b>6.1</b> Introduction</a></li>
<li class="chapter" data-level="6.2" data-path="probability-distributions.html"><a href="probability-distributions.html#discrete-distributions"><i class="fa fa-check"></i><b>6.2</b> Discrete Distributions</a><ul>
<li class="chapter" data-level="6.2.1" data-path="probability-distributions.html"><a href="probability-distributions.html#binomial-distribution---coin-toss-yet-again"><i class="fa fa-check"></i><b>6.2.1</b> <a id="binom"></a> Binomial Distribution - Coin toss yet again!</a></li>
<li class="chapter" data-level="6.2.2" data-path="probability-distributions.html"><a href="probability-distributions.html#discrete-uniform-distribution"><i class="fa fa-check"></i><b>6.2.2</b> Discrete Uniform Distribution</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="probability-distributions.html"><a href="probability-distributions.html#continuous-distributions"><i class="fa fa-check"></i><b>6.3</b> Continuous Distributions</a><ul>
<li class="chapter" data-level="6.3.1" data-path="probability-distributions.html"><a href="probability-distributions.html#uniform-distribution"><i class="fa fa-check"></i><b>6.3.1</b> Uniform Distribution</a></li>
<li class="chapter" data-level="6.3.2" data-path="probability-distributions.html"><a href="probability-distributions.html#normal-distribution"><i class="fa fa-check"></i><b>6.3.2</b> Normal distribution</a></li>
<li class="chapter" data-level="6.3.3" data-path="probability-distributions.html"><a href="probability-distributions.html#chi2-distribution"><i class="fa fa-check"></i><b>6.3.3</b> <span class="math inline">\(\chi^2\)</span> Distribution</a></li>
<li class="chapter" data-level="6.3.4" data-path="probability-distributions.html"><a href="probability-distributions.html#t-distribution"><i class="fa fa-check"></i><b>6.3.4</b> t-Distribution</a></li>
<li class="chapter" data-level="6.3.5" data-path="probability-distributions.html"><a href="probability-distributions.html#f-distribution"><i class="fa fa-check"></i><b>6.3.5</b> F-Distribution</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="probability-distributions.html"><a href="probability-distributions.html#appendix"><i class="fa fa-check"></i><b>6.4</b> Appendix</a><ul>
<li class="chapter" data-level="6.4.1" data-path="probability-distributions.html"><a href="probability-distributions.html#derivation-of-the-varaince-of-the-binomial-distribution"><i class="fa fa-check"></i><b>6.4.1</b> Derivation of the varaince of the binomial distribution</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="summary-statistics.html"><a href="summary-statistics.html"><i class="fa fa-check"></i><b>7</b> Summary statistics</a><ul>
<li class="chapter" data-level="7.1" data-path="summary-statistics.html"><a href="summary-statistics.html#categorical-variables"><i class="fa fa-check"></i><b>7.1</b> Categorical variables</a></li>
<li class="chapter" data-level="7.2" data-path="summary-statistics.html"><a href="summary-statistics.html#continuous-variables"><i class="fa fa-check"></i><b>7.2</b> Continuous variables</a><ul>
<li class="chapter" data-level="7.2.1" data-path="summary-statistics.html"><a href="summary-statistics.html#descriptive-statistics"><i class="fa fa-check"></i><b>7.2.1</b> Descriptive statistics</a></li>
<li class="chapter" data-level="7.2.2" data-path="summary-statistics.html"><a href="summary-statistics.html#creating-subsets"><i class="fa fa-check"></i><b>7.2.2</b> Creating subsets</a></li>
<li class="chapter" data-level="7.2.3" data-path="summary-statistics.html"><a href="summary-statistics.html#confidence-intervals"><i class="fa fa-check"></i><b>7.2.3</b> Confidence intervals</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="data-visualization.html"><a href="data-visualization.html"><i class="fa fa-check"></i><b>8</b> Data visualization</a><ul>
<li class="chapter" data-level="8.1" data-path="data-visualization.html"><a href="data-visualization.html#categorical-variables-1"><i class="fa fa-check"></i><b>8.1</b> Categorical variables</a><ul>
<li class="chapter" data-level="8.1.1" data-path="data-visualization.html"><a href="data-visualization.html#bar-plot"><i class="fa fa-check"></i><b>8.1.1</b> Bar plot</a></li>
<li class="chapter" data-level="8.1.2" data-path="data-visualization.html"><a href="data-visualization.html#covariation-plots"><i class="fa fa-check"></i><b>8.1.2</b> Covariation plots</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="data-visualization.html"><a href="data-visualization.html#continuous-variables-1"><i class="fa fa-check"></i><b>8.2</b> Continuous variables</a><ul>
<li class="chapter" data-level="8.2.1" data-path="data-visualization.html"><a href="data-visualization.html#histogram"><i class="fa fa-check"></i><b>8.2.1</b> Histogram</a></li>
<li class="chapter" data-level="8.2.2" data-path="data-visualization.html"><a href="data-visualization.html#boxplot"><i class="fa fa-check"></i><b>8.2.2</b> Boxplot</a></li>
<li class="chapter" data-level="8.2.3" data-path="data-visualization.html"><a href="data-visualization.html#plot-of-means"><i class="fa fa-check"></i><b>8.2.3</b> Plot of means</a></li>
<li class="chapter" data-level="8.2.4" data-path="data-visualization.html"><a href="data-visualization.html#scatter-plot"><i class="fa fa-check"></i><b>8.2.4</b> Scatter plot</a></li>
<li class="chapter" data-level="8.2.5" data-path="data-visualization.html"><a href="data-visualization.html#line-plot"><i class="fa fa-check"></i><b>8.2.5</b> Line plot</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="data-visualization.html"><a href="data-visualization.html#saving-plots"><i class="fa fa-check"></i><b>8.3</b> Saving plots</a></li>
<li class="chapter" data-level="8.4" data-path="data-visualization.html"><a href="data-visualization.html#additional-options"><i class="fa fa-check"></i><b>8.4</b> Additional options</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html"><i class="fa fa-check"></i><b>9</b> Hypothesis testing</a><ul>
<li class="chapter" data-level="9.1" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#introduction-1"><i class="fa fa-check"></i><b>9.1</b> Introduction</a><ul>
<li class="chapter" data-level="9.1.1" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#why-do-we-test-hypotheses"><i class="fa fa-check"></i><b>9.1.1</b> Why do we test hypotheses?</a></li>
<li class="chapter" data-level="9.1.2" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#the-process-of-hypothesis-testing"><i class="fa fa-check"></i><b>9.1.2</b> The process of hypothesis testing</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#parametric-tests"><i class="fa fa-check"></i><b>9.2</b> Parametric tests</a><ul>
<li class="chapter" data-level="9.2.1" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#independent-means-t-test"><i class="fa fa-check"></i><b>9.2.1</b> Independent-means t-test</a></li>
<li class="chapter" data-level="9.2.2" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#dependent-means-t-test"><i class="fa fa-check"></i><b>9.2.2</b> Dependent-means t-test</a></li>
<li class="chapter" data-level="9.2.3" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#one-sample-t-test"><i class="fa fa-check"></i><b>9.2.3</b> One-sample t-test</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#non-parametric-tests"><i class="fa fa-check"></i><b>9.3</b> Non-parametric tests</a><ul>
<li class="chapter" data-level="9.3.1" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#mann-whitney-u-test-a.k.a.-wilcoxon-rank-sum-test"><i class="fa fa-check"></i><b>9.3.1</b> Mann-Whitney U Test (a.k.a. Wilcoxon rank-sum test)</a></li>
<li class="chapter" data-level="9.3.2" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#wilcoxon-signed-rank-test"><i class="fa fa-check"></i><b>9.3.2</b> Wilcoxon signed-rank test</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#categorical-data"><i class="fa fa-check"></i><b>9.4</b> Categorical data</a><ul>
<li class="chapter" data-level="9.4.1" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#comparing-proportions"><i class="fa fa-check"></i><b>9.4.1</b> Comparing proportions</a></li>
<li class="chapter" data-level="9.4.2" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#chi-square-test"><i class="fa fa-check"></i><b>9.4.2</b> Chi-square test</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html"><i class="fa fa-check"></i><b>10</b> Analysis of variance</a><ul>
<li class="chapter" data-level="10.1" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html#introduction-2"><i class="fa fa-check"></i><b>10.1</b> Introduction</a></li>
<li class="chapter" data-level="10.2" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html#decomposing-variance"><i class="fa fa-check"></i><b>10.2</b> Decomposing variance</a><ul>
<li class="chapter" data-level="10.2.1" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html#total-sum-of-squares"><i class="fa fa-check"></i><b>10.2.1</b> Total sum of squares</a></li>
<li class="chapter" data-level="10.2.2" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html#model-sum-of-squares"><i class="fa fa-check"></i><b>10.2.2</b> Model sum of squares</a></li>
<li class="chapter" data-level="10.2.3" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html#residual-sum-of-squares"><i class="fa fa-check"></i><b>10.2.3</b> Residual sum of squares</a></li>
<li class="chapter" data-level="10.2.4" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html#effect-strength"><i class="fa fa-check"></i><b>10.2.4</b> Effect strength</a></li>
<li class="chapter" data-level="10.2.5" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html#test-of-significance"><i class="fa fa-check"></i><b>10.2.5</b> Test of significance</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html#one-way-anova"><i class="fa fa-check"></i><b>10.3</b> One-way ANOVA</a><ul>
<li class="chapter" data-level="10.3.1" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html#basic-anova"><i class="fa fa-check"></i><b>10.3.1</b> Basic ANOVA</a></li>
<li class="chapter" data-level="10.3.2" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html#post-hoc-tests"><i class="fa fa-check"></i><b>10.3.2</b> Post-hoc tests</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html#n-way-anova"><i class="fa fa-check"></i><b>10.4</b> N-way ANOVA</a></li>
<li class="chapter" data-level="10.5" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html#non-parametric-tests-1"><i class="fa fa-check"></i><b>10.5</b> Non-parametric tests</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="regression.html"><a href="regression.html"><i class="fa fa-check"></i><b>11</b> Regression</a><ul>
<li class="chapter" data-level="11.1" data-path="regression.html"><a href="regression.html#correlation"><i class="fa fa-check"></i><b>11.1</b> Correlation</a><ul>
<li class="chapter" data-level="11.1.1" data-path="regression.html"><a href="regression.html#correlation-coefficient"><i class="fa fa-check"></i><b>11.1.1</b> Correlation coefficient</a></li>
<li class="chapter" data-level="11.1.2" data-path="regression.html"><a href="regression.html#significance-testing"><i class="fa fa-check"></i><b>11.1.2</b> Significance testing</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="regression.html"><a href="regression.html#regression-1"><i class="fa fa-check"></i><b>11.2</b> Regression</a><ul>
<li class="chapter" data-level="11.2.1" data-path="regression.html"><a href="regression.html#simple-linear-regression"><i class="fa fa-check"></i><b>11.2.1</b> Simple linear regression</a></li>
<li class="chapter" data-level="11.2.2" data-path="regression.html"><a href="regression.html#multiple-linear-regression"><i class="fa fa-check"></i><b>11.2.2</b> Multiple linear regression</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="regression.html"><a href="regression.html#potential-problems"><i class="fa fa-check"></i><b>11.3</b> Potential problems</a><ul>
<li class="chapter" data-level="11.3.1" data-path="regression.html"><a href="regression.html#outliers"><i class="fa fa-check"></i><b>11.3.1</b> Outliers</a></li>
<li class="chapter" data-level="11.3.2" data-path="regression.html"><a href="regression.html#influential-observations"><i class="fa fa-check"></i><b>11.3.2</b> Influential observations</a></li>
<li class="chapter" data-level="11.3.3" data-path="regression.html"><a href="regression.html#non-linearity"><i class="fa fa-check"></i><b>11.3.3</b> Non-linearity</a></li>
<li class="chapter" data-level="11.3.4" data-path="regression.html"><a href="regression.html#non-constant-error-variance"><i class="fa fa-check"></i><b>11.3.4</b> Non-constant error variance</a></li>
<li class="chapter" data-level="11.3.5" data-path="regression.html"><a href="regression.html#non-normally-distributed-errors"><i class="fa fa-check"></i><b>11.3.5</b> Non-normally distributed errors</a></li>
<li class="chapter" data-level="11.3.6" data-path="regression.html"><a href="regression.html#correlation-of-errors"><i class="fa fa-check"></i><b>11.3.6</b> Correlation of errors</a></li>
<li class="chapter" data-level="11.3.7" data-path="regression.html"><a href="regression.html#collinearity"><i class="fa fa-check"></i><b>11.3.7</b> Collinearity</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="regression.html"><a href="regression.html#categorical-predictors"><i class="fa fa-check"></i><b>11.4</b> Categorical predictors</a><ul>
<li class="chapter" data-level="11.4.1" data-path="regression.html"><a href="regression.html#two-categories"><i class="fa fa-check"></i><b>11.4.1</b> Two categories</a></li>
<li class="chapter" data-level="11.4.2" data-path="regression.html"><a href="regression.html#more-than-two-categories"><i class="fa fa-check"></i><b>11.4.2</b> More than two categories</a></li>
</ul></li>
<li class="chapter" data-level="11.5" data-path="regression.html"><a href="regression.html#extensions-of-the-linear-model"><i class="fa fa-check"></i><b>11.5</b> Extensions of the linear model</a><ul>
<li class="chapter" data-level="11.5.1" data-path="regression.html"><a href="regression.html#interaction-effects"><i class="fa fa-check"></i><b>11.5.1</b> Interaction effects</a></li>
<li class="chapter" data-level="11.5.2" data-path="regression.html"><a href="regression.html#non-linear-relationships"><i class="fa fa-check"></i><b>11.5.2</b> Non-linear relationships</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="12" data-path="logistic-regression.html"><a href="logistic-regression.html"><i class="fa fa-check"></i><b>12</b> Logistic regression</a><ul>
<li class="chapter" data-level="12.1" data-path="logistic-regression.html"><a href="logistic-regression.html#motivation-and-intuition"><i class="fa fa-check"></i><b>12.1</b> Motivation and intuition</a></li>
<li class="chapter" data-level="12.2" data-path="logistic-regression.html"><a href="logistic-regression.html#technical-details-of-the-model"><i class="fa fa-check"></i><b>12.2</b> Technical details of the model</a></li>
<li class="chapter" data-level="12.3" data-path="logistic-regression.html"><a href="logistic-regression.html#estimation-in-r"><i class="fa fa-check"></i><b>12.3</b> Estimation in R</a><ul>
<li class="chapter" data-level="12.3.1" data-path="logistic-regression.html"><a href="logistic-regression.html#model-selection"><i class="fa fa-check"></i><b>12.3.1</b> Model selection</a></li>
<li class="chapter" data-level="12.3.2" data-path="logistic-regression.html"><a href="logistic-regression.html#predictions"><i class="fa fa-check"></i><b>12.3.2</b> Predictions</a></li>
<li class="chapter" data-level="12.3.3" data-path="logistic-regression.html"><a href="logistic-regression.html#perfect-prediction-logit"><i class="fa fa-check"></i><b>12.3.3</b> Perfect Prediction Logit</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="logistic-regression.html"><a href="logistic-regression.html#appendix-1"><i class="fa fa-check"></i><b>12.4</b> Appendix</a><ul>
<li class="chapter" data-level="12.4.1" data-path="logistic-regression.html"><a href="logistic-regression.html#estimation-of-the-parameters-beta_i"><i class="fa fa-check"></i><b>12.4.1</b> Estimation of the parameters <span class="math inline">\(\beta_i\)</span></a></li>
<li class="chapter" data-level="12.4.2" data-path="logistic-regression.html"><a href="logistic-regression.html#example-1"><i class="fa fa-check"></i><b>12.4.2</b> Example</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="exploratory-factor-analysis.html"><a href="exploratory-factor-analysis.html"><i class="fa fa-check"></i><b>13</b> Exploratory factor analysis</a><ul>
<li class="chapter" data-level="13.1" data-path="exploratory-factor-analysis.html"><a href="exploratory-factor-analysis.html#introduction-3"><i class="fa fa-check"></i><b>13.1</b> Introduction</a></li>
<li class="chapter" data-level="13.2" data-path="exploratory-factor-analysis.html"><a href="exploratory-factor-analysis.html#steps-in-factor-analysis"><i class="fa fa-check"></i><b>13.2</b> Steps in factor analysis</a><ul>
<li class="chapter" data-level="13.2.1" data-path="exploratory-factor-analysis.html"><a href="exploratory-factor-analysis.html#are-the-assumptions-satisfied"><i class="fa fa-check"></i><b>13.2.1</b> Are the assumptions satisfied?</a></li>
<li class="chapter" data-level="13.2.2" data-path="exploratory-factor-analysis.html"><a href="exploratory-factor-analysis.html#deriving-factors"><i class="fa fa-check"></i><b>13.2.2</b> Deriving factors</a></li>
<li class="chapter" data-level="13.2.3" data-path="exploratory-factor-analysis.html"><a href="exploratory-factor-analysis.html#factor-interpretation"><i class="fa fa-check"></i><b>13.2.3</b> Factor interpretation</a></li>
<li class="chapter" data-level="13.2.4" data-path="exploratory-factor-analysis.html"><a href="exploratory-factor-analysis.html#creating-new-variables"><i class="fa fa-check"></i><b>13.2.4</b> Creating new variables</a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="exploratory-factor-analysis.html"><a href="exploratory-factor-analysis.html#reliability-analysis"><i class="fa fa-check"></i><b>13.3</b> Reliability analysis</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="assignments.html"><a href="assignments.html"><i class="fa fa-check"></i><b>14</b> Assignments</a><ul>
<li class="chapter" data-level="14.1" data-path="assignments.html"><a href="assignments.html#assignment-2-hypothesis-testing"><i class="fa fa-check"></i><b>14.1</b> Assignment 2 (Hypothesis Testing)</a><ul>
<li class="chapter" data-level="14.1.1" data-path="assignments.html"><a href="assignments.html#load-data"><i class="fa fa-check"></i><b>14.1.1</b> Load data</a></li>
<li class="chapter" data-level="14.1.2" data-path="assignments.html"><a href="assignments.html#question-1"><i class="fa fa-check"></i><b>14.1.2</b> Question 1</a></li>
<li class="chapter" data-level="14.1.3" data-path="assignments.html"><a href="assignments.html#question-2"><i class="fa fa-check"></i><b>14.1.3</b> Question 2</a></li>
<li class="chapter" data-level="14.1.4" data-path="assignments.html"><a href="assignments.html#question-3"><i class="fa fa-check"></i><b>14.1.4</b> Question 3</a></li>
<li class="chapter" data-level="14.1.5" data-path="assignments.html"><a href="assignments.html#question-4"><i class="fa fa-check"></i><b>14.1.5</b> Question 4</a></li>
</ul></li>
<li class="chapter" data-level="14.2" data-path="assignments.html"><a href="assignments.html#assignment-3-anova"><i class="fa fa-check"></i><b>14.2</b> Assignment 3 (Anova)</a><ul>
<li class="chapter" data-level="14.2.1" data-path="assignments.html"><a href="assignments.html#load-data-1"><i class="fa fa-check"></i><b>14.2.1</b> Load data</a></li>
<li class="chapter" data-level="14.2.2" data-path="assignments.html"><a href="assignments.html#question-1-1"><i class="fa fa-check"></i><b>14.2.2</b> Question 1</a></li>
<li class="chapter" data-level="14.2.3" data-path="assignments.html"><a href="assignments.html#question-2-1"><i class="fa fa-check"></i><b>14.2.3</b> Question 2</a></li>
<li class="chapter" data-level="14.2.4" data-path="assignments.html"><a href="assignments.html#question-3-1"><i class="fa fa-check"></i><b>14.2.4</b> Question 3</a></li>
</ul></li>
<li class="chapter" data-level="14.3" data-path="assignments.html"><a href="assignments.html#assignment-4-regression"><i class="fa fa-check"></i><b>14.3</b> Assignment 4 (Regression)</a><ul>
<li class="chapter" data-level="14.3.1" data-path="assignments.html"><a href="assignments.html#load-data-2"><i class="fa fa-check"></i><b>14.3.1</b> Load data</a></li>
<li class="chapter" data-level="14.3.2" data-path="assignments.html"><a href="assignments.html#question-1-2"><i class="fa fa-check"></i><b>14.3.2</b> Question 1</a></li>
<li class="chapter" data-level="14.3.3" data-path="assignments.html"><a href="assignments.html#question-2-2"><i class="fa fa-check"></i><b>14.3.3</b> Question 2</a></li>
<li class="chapter" data-level="14.3.4" data-path="assignments.html"><a href="assignments.html#question-3-2"><i class="fa fa-check"></i><b>14.3.4</b> Question 3</a></li>
<li class="chapter" data-level="14.3.5" data-path="assignments.html"><a href="assignments.html#question-4-1"><i class="fa fa-check"></i><b>14.3.5</b> Question 4</a></li>
<li class="chapter" data-level="14.3.6" data-path="assignments.html"><a href="assignments.html#question-5"><i class="fa fa-check"></i><b>14.3.6</b> Question 5</a></li>
</ul></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Marketing Research Design &amp; Analysis 2017</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="analysis-of-variance" class="section level1">
<h1><span class="header-section-number">10</span> Analysis of variance</h1>
<p>This chapter is primarily based on Field, A., Miles J., &amp; Field, Z. (2012): Discovering Statistics Using R. Sage Publications, <strong>chapters 10 &amp; 12</strong>.</p>
<div id="introduction-2" class="section level2">
<h2><span class="header-section-number">10.1</span> Introduction</h2>
<p>In the previous section we learned how to compare means using a t-test. The t-test has some limitations since it only lets you compare 2 means and you can only use it with one independent variable. However, often we would like to compare means from 3 or more groups. In addition, there may be instances in which you manipulate more than one independent variable. For these applications, ANOVA (ANalysis Of VAriance) can be used. Hence, to conduct ANOVA you need:</p>
<ul>
<li>A metric dependent variable (i.e., measured using an interval or ratio scale)</li>
<li>One or more non-metric (categorical) independent variables (also called factors)</li>
</ul>
<p>A treatment is a particular combination of factor levels, or categories. One-way ANOVA is used when there is only one categorical variable (factor). In this case, a treatment is the same as a factor level. N-way ANOVA is used with two or more factors.</p>
<p>Let’s use an example to see how ANOVA works. Assume that you are a marketing manager at an online fashion store, and you wish to analyze the effect of online promotion on sales. You conduct an experiment and select a sample of 30 comparable products to be included in the experiment. Then you randomly assign the products to one of three groups: (1) = high promotion level, (2) = medium promotion level, (3) = low promotion level, and record the sales over one day. This means that you have 10 products assigned to each treatment.</p>
<p>As always, we load and inspect the data first:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">online_store_promo &lt;-<span class="st"> </span><span class="kw">read.table</span>(<span class="st">&quot;https://raw.githubusercontent.com/IMSMWU/Teaching/master/MRDA2017/online_store_promo.dat&quot;</span>, 
    <span class="dt">sep =</span> <span class="st">&quot;</span><span class="ch">\t</span><span class="st">&quot;</span>, <span class="dt">header =</span> <span class="ot">TRUE</span>)  <span class="co">#read in data</span>
online_store_promo<span class="op">$</span>Promotion &lt;-<span class="st"> </span><span class="kw">factor</span>(online_store_promo<span class="op">$</span>Promotion, 
    <span class="dt">levels =</span> <span class="kw">c</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">3</span>), <span class="dt">labels =</span> <span class="kw">c</span>(<span class="st">&quot;high&quot;</span>, <span class="st">&quot;medium&quot;</span>, <span class="st">&quot;low&quot;</span>))  <span class="co">#convert grouping variable to factor</span>
<span class="kw">str</span>(online_store_promo)  <span class="co">#inspect data</span></code></pre></div>
<pre><code>## &#39;data.frame&#39;:    30 obs. of  4 variables:
##  $ Obs       : int  1 2 3 4 5 6 7 8 9 10 ...
##  $ Promotion : Factor w/ 3 levels &quot;high&quot;,&quot;medium&quot;,..: 1 1 1 1 1 1 1 1 1 1 ...
##  $ Newsletter: int  1 1 1 1 1 0 0 0 0 0 ...
##  $ Sales     : int  10 9 10 8 9 8 9 7 7 6 ...</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">print</span>(online_store_promo)  <span class="co">#inspect data</span></code></pre></div>
<pre><code>##    Obs Promotion Newsletter Sales
## 1    1      high          1    10
## 2    2      high          1     9
## 3    3      high          1    10
## 4    4      high          1     8
## 5    5      high          1     9
## 6    6      high          0     8
## 7    7      high          0     9
## 8    8      high          0     7
## 9    9      high          0     7
## 10  10      high          0     6
## 11   1    medium          1     8
## 12   2    medium          1     8
## 13   3    medium          1     7
## 14   4    medium          1     9
## 15   5    medium          1     6
## 16   6    medium          0     4
## 17   7    medium          0     5
## 18   8    medium          0     5
## 19   9    medium          0     6
## 20  10    medium          0     4
## 21   1       low          1     5
## 22   2       low          1     7
## 23   3       low          1     6
## 24   4       low          1     4
## 25   5       low          1     5
## 26   6       low          0     2
## 27   7       low          0     3
## 28   8       low          0     2
## 29   9       low          0     1
## 30  10       low          0     2</code></pre>
<p>The null hypothesis, typically, is that all means are equal (non-directional hypothesis). Hence, in our case:</p>
<p style="text-align:center;">
<span class="math inline">\(H_0: \mu_1 = \mu_2 = \mu_3\)</span> <br>
</p>
<p>To get a first impression if there are any differences in sales across the experimental groups, we use the <code>describeBy(...)</code> function from the <code>psych</code> package:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(psych)
<span class="kw">describeBy</span>(online_store_promo<span class="op">$</span>Sales, online_store_promo<span class="op">$</span>Promotion)  <span class="co">#inspect data</span></code></pre></div>
<pre><code>## 
##  Descriptive statistics by group 
## group: high
##    vars  n mean   sd median trimmed  mad min max range  skew kurtosis   se
## X1    1 10  8.3 1.34    8.5    8.38 1.48   6  10     4 -0.24     -1.4 0.42
## -------------------------------------------------------- 
## group: medium
##    vars  n mean   sd median trimmed  mad min max range skew kurtosis   se
## X1    1 10  6.2 1.75      6    6.12 2.22   4   9     5 0.17    -1.58 0.55
## -------------------------------------------------------- 
## group: low
##    vars  n mean sd median trimmed  mad min max range skew kurtosis   se
## X1    1 10  3.7  2    3.5    3.62 2.22   1   7     6 0.22    -1.57 0.63</code></pre>
<p>In addition, you should visualize the data using appropriate plots:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co">#Plot of means</span>
<span class="kw">library</span>(plyr)
<span class="kw">library</span>(ggplot2)
<span class="kw">ggplot</span>(online_store_promo, <span class="kw">aes</span>(Promotion, Sales)) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">stat_summary</span>(<span class="dt">fun.y =</span> mean, <span class="dt">geom =</span> <span class="st">&quot;bar&quot;</span>, <span class="dt">fill =</span> <span class="st">&quot;White&quot;</span>, <span class="dt">colour =</span> <span class="st">&quot;Black&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">stat_summary</span>(<span class="dt">fun.data =</span> mean_cl_normal, <span class="dt">geom =</span> <span class="st">&quot;pointrange&quot;</span>) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&quot;Experimental group (promotion level)&quot;</span>, <span class="dt">y =</span> <span class="st">&quot;Sales (thsd. units)&quot;</span>) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">theme_bw</span>()</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-213"></span>
<img src="_main_files/figure-html/unnamed-chunk-213-1.png" alt="Plot of means" width="672" />
<p class="caption">
Figure 10.1: Plot of means
</p>
</div>
<p>Note that ANOVA is an omnibus test, which means that we test for an overall difference between groups. Hence, the test will only tell you if the group means are different, but it won’t tell you exactly which groups are differnt from another.</p>
<p>So why don’t we then just conduct a series of t-tests for all combinations of groups (i.e., high vs. low, low vs. medium, medium vs. high)? The reason is that if we assume each test to be independent, then there is a 5% probability of falsely rejecting the null hypothesis (Type I error) for each test. In our case:</p>
<ul>
<li>High vs. low (α = 0.05)</li>
<li>High vs. medium (α = 0.05)</li>
<li>Medium vs. low (α = 0.05)</li>
</ul>
<p>This means that the overall probability of making a Type I error is 1-(0.95<sup>3</sup>) = 0.143, since the probability of no Type I error is 0.95 for each of the three tests. Consequently, the Type I error probability would be 14.3%, which is above the conventional standard of 5%. This is also known as the familywise or experimentwise error.</p>
</div>
<div id="decomposing-variance" class="section level2">
<h2><span class="header-section-number">10.2</span> Decomposing variance</h2>
<p>The basic concept underlying ANOVA is the decomposition of the variance in the data. There are three variance components which we need to consider:</p>
<ul>
<li>We calculate how much variability there is between scores: <b>Total sum of squares (SS<sub>T</sub>)</b></li>
<li>We then calculate how much of this variability can be explained by the model we fit to the data (i.e., how much variability is due to the experimental manipulation): <b>Model sum of squares (SS<sub>M</sub>)</b></li>
<li>… and how much cannot be explained (i.e., how much variability is due to individual differences in performance): <b>Residual sum of squares (SS<sub>R</sub>)</b></li>
</ul>
<p>The following figure shows the different variance components using a generalized data matrix:</p>
<div class="figure">
<img src="https://github.com/IMSMWU/Teaching/raw/master/MRDA2017/sum_of_squares.JPG" alt="Decomposing variance" />
<p class="caption">Decomposing variance</p>
</div>
<p>The total variation is determined by the variation between the categories (due to our experimental manipulation) and the within-category variation that is due to extraneous factors (e.g., differences between the products that are included in each group):</p>
<span class="math display" id="eq:vardecomp">\[\begin{equation} 
\begin{split}
SS_T= SS_M+SS_R
\end{split}
\tag{10.1}
\end{equation}\]</span>
<p>To get a better feeling how this relates to our data set, we can look at the data in a slightly different way. Specifically, we can use the <code>dcast(...)</code> function from the <code>reshape2</code> package to convert the data to wide format:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(reshape2)
<span class="kw">dcast</span>(online_store_promo, Obs <span class="op">~</span><span class="st"> </span>Promotion, <span class="dt">value.var =</span> <span class="st">&quot;Sales&quot;</span>)</code></pre></div>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["Obs"],"name":[1],"type":["int"],"align":["right"]},{"label":["high"],"name":[2],"type":["int"],"align":["right"]},{"label":["medium"],"name":[3],"type":["int"],"align":["right"]},{"label":["low"],"name":[4],"type":["int"],"align":["right"]}],"data":[{"1":"1","2":"10","3":"8","4":"5"},{"1":"2","2":"9","3":"8","4":"7"},{"1":"3","2":"10","3":"7","4":"6"},{"1":"4","2":"8","3":"9","4":"4"},{"1":"5","2":"9","3":"6","4":"5"},{"1":"6","2":"8","3":"4","4":"2"},{"1":"7","2":"9","3":"5","4":"3"},{"1":"8","2":"7","3":"5","4":"2"},{"1":"9","2":"7","3":"6","4":"1"},{"1":"10","2":"6","3":"4","4":"2"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<p>In this example, X<sub>1</sub> from the generalized data matrix above would refer to the factor level “high”, X<sub>2</sub> to the level “medium”, and X<sub>3</sub> to the level “low”. Y<sub>11</sub> refers to the first data point in the first row (i.e., “10”), Y<sub>12</sub> to the second data point in the first row (i.e., “8”), etc.. The grand mean (<span class="math inline">\(\overline{Y}\)</span>) and the category means (<span class="math inline">\(\overline{Y}_c\)</span>) can be easily computed:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">mean</span>(online_store_promo<span class="op">$</span>Sales)  <span class="co">#grand mean</span></code></pre></div>
<pre><code>## [1] 6.066667</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">by</span>(online_store_promo<span class="op">$</span>Sales, online_store_promo<span class="op">$</span>Promotion, 
    mean)  <span class="co">#category mean</span></code></pre></div>
<pre><code>## online_store_promo$Promotion: high
## [1] 8.3
## -------------------------------------------------------- 
## online_store_promo$Promotion: medium
## [1] 6.2
## -------------------------------------------------------- 
## online_store_promo$Promotion: low
## [1] 3.7</code></pre>
<p>To see how each variance component can be derived, let’s look at the data again. The following graph shows the individual observations by experimental group:</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-216"></span>
<img src="_main_files/figure-html/unnamed-chunk-216-1.png" alt="Sum of Squares" width="672" />
<p class="caption">
Figure 10.2: Sum of Squares
</p>
</div>
<div id="total-sum-of-squares" class="section level3">
<h3><span class="header-section-number">10.2.1</span> Total sum of squares</h3>
<p>To compute the total variation in the data, we consider the difference between each observation and the grand mean. The grand mean is the mean over all observations in the dataset. The vertical lines in the following plot measure how far each observation is away from the grand mean:</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-217"></span>
<img src="_main_files/figure-html/unnamed-chunk-217-1.png" alt="Total Sum of Squares" width="672" />
<p class="caption">
Figure 10.3: Total Sum of Squares
</p>
</div>
<p>The formal representation of the total sum of squares (SS<sub>T</sub>) is:</p>
<span class="math display" id="eq:sumsquares">\[\begin{equation} 
\begin{split}
SS_T= \sum_{i=1}^{N} (Y_i-\overline{Y})^2
\end{split}
\tag{10.2}
\end{equation}\]</span>
<p>This means that we need to subtract the grand mean from each individual data point, square the difference, and sum up over all the squared differences. Thus, in our example, the total sum of squares can be calculated as:</p>
<p><span class="math display">\[ 
\begin{align}
SS_T =&amp;(10−6.067)^2 + (9−6.067)^2 + … + (7−6.067)^2\\
      &amp;+(8−6.067)^2 + (8−6.067)^2 + … + (4−6.067)^2\\
      &amp;+(5−6.067)^2 + (7−6.067)^2 + … + (2−6.067)^2\\ 
     =&amp; 185.867
\end{align}
\]</span></p>
<p>You could also compute this in R using:</p>
<pre><code>## [1] 185.8667</code></pre>
<p>For the subsequent analyses, it is important to understand the concept behind the <b>degrees of freedom</b>. Remember that in order to estimate a population value from a sample, we need to hold something in the population constant. In ANOVA, the df are generally one less than the number of values used to calculate the SS. For example, when we estimate the population mean from a sample, we assume that the sample mean is equal to the population mean. Then, in order to estimate the population mean from the sample, all but one scores are free to vary and the remaining score needs to be the value that keeps the population mean constant. In our example, we used all 30 observations to calculate the sum of square, so the total degrees of freedom (df<sub>T</sub>) are:</p>
<span class="math display" id="eq:dfT">\[\begin{equation} 
\begin{split}
df_T = N-1=30-1=29
\end{split}
\tag{10.3}
\end{equation}\]</span>
</div>
<div id="model-sum-of-squares" class="section level3">
<h3><span class="header-section-number">10.2.2</span> Model sum of squares</h3>
<p>Now we know that there are 185.867 units of total variation in our data. Next, we compute how much of the total variation can be explained by the differences between groups (i.e., our experimental manipulation). To compute the explained variation in the data, we consider the difference between the values predicted by our model for each observation (i.e., the group mean) and the grand mean. The group mean refers to the mean value within the experimental group. The vertical lines in the following plot measure how far the predicted value for each observation (i.e., the group mean) is away from the grand mean:</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-219"></span>
<img src="_main_files/figure-html/unnamed-chunk-219-1.png" alt="Model Sum of Squares" width="672" />
<p class="caption">
Figure 10.4: Model Sum of Squares
</p>
</div>
<p>The formal representation of the model sum of squares (SS<sub>M</sub>) is:</p>
<span class="math display" id="eq:modelsumsquares">\[\begin{equation} 
\begin{split}
SS_M= \sum_{j=1}^{c} n_j(\overline{Y}_j-\overline{Y})^2
\end{split}
\tag{10.4}
\end{equation}\]</span>
<p>where c denotes the number of categories (experimental groups). This means that we need to subtract the grand mean from each group mean, square the difference, and sum up over all the squared differences. Thus, in our example, the model sum of squares can be calculated as:</p>
<p><span class="math display">\[ 
\begin{align}
SS_M &amp;= 10*(8.3−6.067)^2 + 10*(6.2−6.067)^2 + 10*(3.7−6.067)^2 \\
     &amp;= 106.067
\end{align}
\]</span></p>
<p>You could also compute this manually in R using:</p>
<pre><code>## [1] 106.0667</code></pre>
<p>In this case, we used the three group means to calculate the sum of squares, so the model degrees of freedom (df<sub>M</sub>) are:</p>
<span class="math display" id="eq:dfM">\[\begin{equation} 
\begin{split}
df_M= c-1=3-1=2
\end{split}
\tag{10.5}
\end{equation}\]</span>
</div>
<div id="residual-sum-of-squares" class="section level3">
<h3><span class="header-section-number">10.2.3</span> Residual sum of squares</h3>
<p>Lastly, we calculate the amount of variation that cannot be explained by our model. In ANOVA, this is the sum of squared distances between what the model predicts for each data point (i.e., the group means) and the observed values. In other words, this refers to the amount of variation that is caused by extraneous factors, such as differences between product characteristics of the products in the different experimental groups. The vertical lines in the following plot measure how far each observation is away from the group mean:</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-221"></span>
<img src="_main_files/figure-html/unnamed-chunk-221-1.png" alt="Residual Sum of Squares" width="672" />
<p class="caption">
Figure 10.5: Residual Sum of Squares
</p>
</div>
<p>The formal representation of the residual sum of squares (SS<sub>R</sub>) is:</p>
<span class="math display" id="eq:sumsquaresresid">\[\begin{equation} 
\begin{split}
SS_R= \sum_{j=1}^{c} \sum_{i=1}^{n} ({Y}_{ij}-\overline{Y}_{j})^2
\end{split}
\tag{10.6}
\end{equation}\]</span>
<p>This means that we need to subtract the group mean from each individual obseration, square the difference, and sum up over all the squared differences. Thus, in our example, the model sum of squares can be calculated as:</p>
<p><span class="math display">\[ 
\begin{align}
SS_R =&amp; (10−8.3)^2 + (9−8.3)^2 + … + (6−8.3)^2 \\
     &amp;+(8−6.2)^2 + (8−6.2)^2 + … + (4−6.2)^2 \\
     &amp;+ (5−3.7)^2 + (7−3.7)^2 + … + (2−3.7)^2 \\
     =&amp; 79.8
\end{align}
\]</span></p>
<p>You could also compute this in R using:</p>
<pre><code>## [1] 79.8</code></pre>
<p>In this case, we used the 10 values for each of the SS for each group, so the residual degrees of freedom (df<sub>R</sub>) are:</p>
<span class="math display" id="eq:dfR">\[\begin{equation} 
\begin{split}
df_R= (n_1-1)+(n_2-1)+(n_3-1) \\
=(10-1)+(10-1)+(10-1)=27
\end{split}
\tag{10.7}
\end{equation}\]</span>
</div>
<div id="effect-strength" class="section level3">
<h3><span class="header-section-number">10.2.4</span> Effect strength</h3>
<p>Once you have computed the different sum of squares, you can investigate the effect strength. Eta<sup>2</sup> is a measure of the variation in Y that is explained by X:</p>
<span class="math display" id="eq:eta">\[\begin{equation} 
\begin{split}
\eta^2= \frac{SS_M}{SS_T}=\frac{106.067}{185.876}=0.571
\end{split}
\tag{10.8}
\end{equation}\]</span>
<p>To compute this in R:</p>
<pre><code>## [1] 0.57066</code></pre>
<p>The statistic can only take values between 0 and 1. It is equal to 0 when all the category means are equal, indicating that X has no effect on Y. In contrast, it has a value of 1 when there is no variability within each category of X but there is some variability between categories.</p>
</div>
<div id="test-of-significance" class="section level3">
<h3><span class="header-section-number">10.2.5</span> Test of significance</h3>
<p>How can we determine whether the effect of X on Y is significant?</p>
<ul>
<li>First, we calculate the fit of the most basic model (i.e., the grand mean)</li>
<li>Then, we calculate the fit of the “best” model (i.e., the group means)</li>
<li>A good model should fit the data significantly better than the basic model</li>
<li>The F-statistic or F-ratio compares the amount of systematic variance in the data to the amount of unsystematic variance</li>
</ul>
<p>The F-statistic uses the ratio of mean square related to X (explained variation) and the mean square related to the error (unexplained variation):</p>
<p style="text-align:center;">
<span class="math inline">\(\frac{SS_M}{SS_R}\)</span> <br>
</p>
<p>However, since these are summed values, their magnitude is influenced by the number of scores that were summed. For example, to calculate SS<sub>M</sub> we only used the sum of 3 values (the group means), while we used 30 and 27 values to calculate SS<sub>T</sub> and SS<sub>R</sub>, respectively. Thus, we calculate the average sum of squares (“mean square”) to compare the average amount of systematic vs. unsystematic variation by deviding the SS values by the degrees of freedom associated with the respective statistic.</p>
<p>Mean square due to X:</p>
<span class="math display" id="eq:MSM">\[\begin{equation} 
\begin{split}
MS_M= \frac{SS_M}{df_M}=\frac{SS_M}{c-1}=\frac{106.067}{(3-1)}
\end{split}
\tag{10.9}
\end{equation}\]</span>
<p>Mean square due to error:</p>
<span class="math display" id="eq:MSR">\[\begin{equation} 
\begin{split}
MS_R= \frac{SS_R}{df_R}=\frac{SS_R}{N-c}=\frac{79.8}{(30-3)}
\end{split}
\tag{10.10}
\end{equation}\]</span>
<p>Now, we compare the amount of variability explained by the model (experiment), to the error in the model (variation due to extraneous variables). If the model explains more variability than it can’t explain, then the experimental manipulation has had a significant effect on the outcome (DV). The F-radio can be derived as follows:</p>
<span class="math display" id="eq:Fstat">\[\begin{equation} 
\begin{split}
F= \frac{MS_M}{MS_R}=\frac{SS_R}{N-c}=\frac{\frac{106.067}{(3-1)}}{\frac{79.8}{(30-3)}}=17.944
\end{split}
\tag{10.11}
\end{equation}\]</span>
<p>You can easily compute this in R:</p>
<pre><code>## [1] 17.94361</code></pre>
<p>This statistic follows the F distribution with (m = c – 1) and (n = N – c) degrees of freedom. This means that, like the <span class="math inline">\(\chi^2\)</span> distribution, the shape of the F-distribution depends on the degrees of freedom. In this case, the shape depends on the degrees of freedom associated with the numerator and denominator used to compute the F-ratio. The following figure shows the shape of the F-distribution for different degrees of freedom:</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-225"></span>
<img src="_main_files/figure-html/unnamed-chunk-225-1.png" alt="The F distribution" width="672" />
<p class="caption">
Figure 10.6: The F distribution
</p>
</div>
<p>The outcome of the test is one of the following:</p>
<ul>
<li>If the null hypothesis of equal category means is not rejected, then the independent variable does not have a significant effect on the dependent variable</li>
<li>If the null hypothesis is rejected, then the effect of the independent variable is significant</li>
</ul>
<p>For 2 and 27 degrees of freedom, the critical value of F is 3.35 for α=0.05. As usual, you can either look up these values in a table or use the appropriate function in R:</p>
<pre><code>## [1] 3.354131</code></pre>
<pre><code>## [1] TRUE</code></pre>
<p>The output tells us that the calculated test statistic exeeds the critical value. We can also show the test result visually:</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-227"></span>
<img src="_main_files/figure-html/unnamed-chunk-227-1.png" alt="Visual depiction of the test result" width="672" />
<p class="caption">
Figure 10.7: Visual depiction of the test result
</p>
</div>
<p>Thus, we conclude that because F<sub>CAL</sub> = 17.944 &gt; F<sub>CR</sub> = 3.35, H<sub>0</sub> is rejected!</p>
<p>Interpretation: one or more of the differences between means are statistically significant.</p>
<p>Reporting: There was a significant effect of promotion on sales levels, F(2,27) = 17.94, p &lt; 0.05, η = 0.571.</p>
<p>Remember: This doesn’t tell us where the differences between groups lie. To find out which group means exactly differ, we need to use post-hoc procedures (see below).</p>
<p>You don’t have to compute these statistics manually! Luckily, there is a function for ANOVA in R, which does the above calculations for you as we will see in the next section.</p>
</div>
</div>
<div id="one-way-anova" class="section level2">
<h2><span class="header-section-number">10.3</span> One-way ANOVA</h2>
<div id="basic-anova" class="section level3">
<h3><span class="header-section-number">10.3.1</span> Basic ANOVA</h3>
<p>As already indicated, one-way ANOVA is used when there is only one categorical variable (factor). Before conducting ANOVA, you need to check if the assumptions of the test are fulfilled. The assumptions of ANOVA are discussed in the following sections.</p>
<div id="independence-of-observations" class="section level4 unnumbered">
<h4>Independence of observations</h4>
<p>The observations in the groups should be independent. Because we randomly assigned the products to the experimental conditions, this assumption can be assumed to be met.</p>
</div>
<div id="distributional-assumptions" class="section level4 unnumbered">
<h4>Distributional assumptions</h4>
<p>ANOVA is relatively immune to violations to the normality assumption when sample sizes are large due to the Central Limit Theorem. However, if your sample is small (i.e., n &lt; 30 per group) you may nevertheless want to check the normality of your data, e.g., by using the Shapiro-Wilk test or QQ-Plot. In our example, we only have 10 observations per group, which means that we cannot rely on the Central Limit Theorem and we should test the normality of our data. This can be done using the Shapiro-Wilk Test, which has the Null Hyposesis that the data is normally distributed. Hence, an insignificant test results means that the data can be assumed to be approximately normally distributed:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">shapiro.test</span>(online_store_promo[online_store_promo<span class="op">$</span>Promotion <span class="op">==</span><span class="st"> </span>
<span class="st">    &quot;low&quot;</span>, ]<span class="op">$</span>Sales)</code></pre></div>
<pre><code>## 
##  Shapiro-Wilk normality test
## 
## data:  online_store_promo[online_store_promo$Promotion == &quot;low&quot;, ]$Sales
## W = 0.93497, p-value = 0.4985</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">shapiro.test</span>(online_store_promo[online_store_promo<span class="op">$</span>Promotion <span class="op">==</span><span class="st"> </span>
<span class="st">    &quot;medium&quot;</span>, ]<span class="op">$</span>Sales)</code></pre></div>
<pre><code>## 
##  Shapiro-Wilk normality test
## 
## data:  online_store_promo[online_store_promo$Promotion == &quot;medium&quot;,     ]$Sales
## W = 0.93247, p-value = 0.4726</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">shapiro.test</span>(online_store_promo[online_store_promo<span class="op">$</span>Promotion <span class="op">==</span><span class="st"> </span>
<span class="st">    &quot;high&quot;</span>, ]<span class="op">$</span>Sales)</code></pre></div>
<pre><code>## 
##  Shapiro-Wilk normality test
## 
## data:  online_store_promo[online_store_promo$Promotion == &quot;high&quot;, ]$Sales
## W = 0.93185, p-value = 0.4664</code></pre>
<p>Since the test result is insignificant for all groups, we can conclude that the data approximately follow a normal distribution.</p>
<p>We could also test the distributional assumptions visually using a Q-Q plot (i.e., quantile-quantile plot). This plot can be used to assess if a set of data plausibly came from some theoretical distribution such as the Normal distribution. Since this is just a visual check, it is somewhat subjective. But it may help us to judge if our assumption is plausible, and if not, which data points contribute to the violation. A Q-Q plot is a scatterplot created by plotting two sets of quantiles against one another. If both sets of quantiles came from the same distribution, we should see the points forming a line that’s roughly straight. In other words, Q-Q plots take your sample data, sort it in ascending order, and then plot them versus quantiles calculated from a theoretical distribution. Quantiles are often referred to as “percentiles” and refer to the points in your data below which a certain proportion of your data fall. Recall, for example, the standard Normal distribution with a mean of 0 and a standard deviation of 1. Since the 50th percentile (or 0.5 quantile) is 0, half the data lie below 0. The 95th percentile (or 0.95 quantile), is about 1.64, which means that 95 percent of the data lie below 1.64. The 97.5th quantile is about 1.96, which means that 97.5% of the data lie below 1.96. In the Q-Q plot, the number of quantiles is selected to match the size of your sample data.</p>
<p>To create the Q-Q plot for the normal distribution, you may use the <code>qqnorm()</code> function, which takes the data to be tested as an argument. Using the <code>qqline()</code> function subsequently on the data creates the line on which the data points should fall based on the theoretical quantiles. If the individual data points deviate a lot from this line, it means that the data is not likely to follow a normal distribution.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">qqnorm</span>(online_store_promo[online_store_promo<span class="op">$</span>Promotion <span class="op">==</span><span class="st"> </span>
<span class="st">    &quot;low&quot;</span>, ]<span class="op">$</span>Sales)
<span class="kw">qqline</span>(online_store_promo[online_store_promo<span class="op">$</span>Promotion <span class="op">==</span><span class="st"> </span>
<span class="st">    &quot;low&quot;</span>, ]<span class="op">$</span>Sales)</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-229"></span>
<img src="_main_files/figure-html/unnamed-chunk-229-1.png" alt="Q-Q plot 1" width="672" />
<p class="caption">
Figure 10.8: Q-Q plot 1
</p>
</div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">qqnorm</span>(online_store_promo[online_store_promo<span class="op">$</span>Promotion <span class="op">==</span><span class="st"> </span>
<span class="st">    &quot;medium&quot;</span>, ]<span class="op">$</span>Sales)
<span class="kw">qqline</span>(online_store_promo[online_store_promo<span class="op">$</span>Promotion <span class="op">==</span><span class="st"> </span>
<span class="st">    &quot;medium&quot;</span>, ]<span class="op">$</span>Sales)</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-229"></span>
<img src="_main_files/figure-html/unnamed-chunk-229-2.png" alt="Q-Q plot 2" width="672" />
<p class="caption">
Figure 10.8: Q-Q plot 2
</p>
</div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">qqnorm</span>(online_store_promo[online_store_promo<span class="op">$</span>Promotion <span class="op">==</span><span class="st"> </span>
<span class="st">    &quot;high&quot;</span>, ]<span class="op">$</span>Sales)
<span class="kw">qqline</span>(online_store_promo[online_store_promo<span class="op">$</span>Promotion <span class="op">==</span><span class="st"> </span>
<span class="st">    &quot;high&quot;</span>, ]<span class="op">$</span>Sales)</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-229"></span>
<img src="_main_files/figure-html/unnamed-chunk-229-3.png" alt="Q-Q plot 3" width="672" />
<p class="caption">
Figure 10.8: Q-Q plot 3
</p>
</div>
<p>The Q-Q plots suggest an approximately Normal distribution. If the assumption had been violated, you might consider transforming your data or resort to a non-parametric test.</p>
</div>
<div id="homogeneity-of-variance" class="section level4 unnumbered">
<h4>Homogeneity of variance</h4>
<p>You can test the homogeneity of variances in R using Levene’s test:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(car)
<span class="kw">leveneTest</span>(Sales <span class="op">~</span><span class="st"> </span>Promotion, <span class="dt">data =</span> online_store_promo, 
    <span class="dt">center =</span> mean)</code></pre></div>
<pre><code>## Levene&#39;s Test for Homogeneity of Variance (center = mean)
##       Df F value Pr(&gt;F)
## group  2  1.3532 0.2754
##       27</code></pre>
<p>The null hypothesis of the test is that the group variances are equal. Thus, if the test result is significant it means that the variances are not equal. If we cannot reject the null hypothesis (i.e., the group variances are not significantly different), we can proceed with the ANOVA as follows:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">aov &lt;-<span class="st"> </span><span class="kw">aov</span>(Sales <span class="op">~</span><span class="st"> </span>Promotion, <span class="dt">data =</span> online_store_promo)
<span class="kw">summary</span>(aov)</code></pre></div>
<pre><code>##             Df Sum Sq Mean Sq F value   Pr(&gt;F)    
## Promotion    2  106.1   53.03   17.94 0.000011 ***
## Residuals   27   79.8    2.96                     
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>You can see that the p-value is smaller than 0.05. This means that, if there really was no difference between the population means (i.e., the Null hyposesis was true), the probability of the observed differences (or larger differences) is less than 5%.</p>
<p>To compute η<sup>2</sup> from the output, we can extract the relevant sum of squares as follows</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(aov)[[<span class="dv">1</span>]]<span class="op">$</span><span class="st">&quot;Sum Sq&quot;</span>[<span class="dv">1</span>]<span class="op">/</span>(<span class="kw">summary</span>(aov)[[<span class="dv">1</span>]]<span class="op">$</span><span class="st">&quot;Sum Sq&quot;</span>[<span class="dv">1</span>] <span class="op">+</span><span class="st"> </span>
<span class="st">    </span><span class="kw">summary</span>(aov)[[<span class="dv">1</span>]]<span class="op">$</span><span class="st">&quot;Sum Sq&quot;</span>[<span class="dv">2</span>])</code></pre></div>
<pre><code>## [1] 0.57066</code></pre>
<p>You can see that the results match the results from our manual computation above.</p>
<p>The <code>aov()</code> function also automatically generates some plots that you can use to judge if the model assumptions are met. We will inspect two of the plots here.</p>
<p>We will use the first plot to inspect if the residual variances are equal across the experimental groups:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(aov, <span class="dv">1</span>)</code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-233-1.png" width="672" /></p>
<p>Generally, the residual variance (i.e., the range of values on the y-axis) should be the same for different levels auf our independent variable. The plot shows, that there are some slight differences. Notably, the range of residuals is highest for the “low” group and lowest for the “high” group. However, the differences are not that large and since the Levene’s test could not reject the Null of equal variances, we conclude that the variances are similar enough in this case.</p>
<p>The second plot can be used to test the assumption that the residuals are approximately normally distributed. We use a Q-Q plot to test this assumption:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(aov, <span class="dv">2</span>)</code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-234-1.png" width="672" /></p>
<p>The plot suggests that the residuals are approximately normally distributed. We could also test this by extracting the residuals from the anova output using the <code>resid()</code> function and using the Shapiro-Wilk test:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">shapiro.test</span>(<span class="kw">resid</span>(aov))</code></pre></div>
<pre><code>## 
##  Shapiro-Wilk normality test
## 
## data:  resid(aov)
## W = 0.96094, p-value = 0.3272</code></pre>
<p>Confirming the impression from the Q-Q plot, we cannot reject the Null that the residuals are approximately normally distributed.</p>
<p>Note that if Levene’s test would have been significant (i.e., variances are not equal), we would have needed to either resort to non-parametric tests (see below), or compute the Welch’s F-ratio instead:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">oneway.test</span>(Sales <span class="op">~</span><span class="st"> </span>Promotion, <span class="dt">data =</span> online_store_promo)</code></pre></div>
<pre><code>## 
##  One-way analysis of means (not assuming equal variances)
## 
## data:  Sales and Promotion
## F = 18.09, num df = 2.00, denom df = 17.47, p-value = 0.00005541</code></pre>
<p>You can see that the results are fairly similar, since the variances turned out to be fairly equal across groups.</p>
</div>
</div>
<div id="post-hoc-tests" class="section level3">
<h3><span class="header-section-number">10.3.2</span> Post-hoc tests</h3>
<p>Provided that significant differences were detected by the overall ANOVA you can find out which group means are different using post hoc procedures. Post hoc procedures are designed to conduct pairwise comparisons of all different combinations of the treatment groups by correcting the level of significance for each test such that the overall Type I error rate (α) across all comparisons remains at 0.05.</p>
<p>In other words, we rejected H<sub>0</sub>: μ<sub>1</sub>= μ<sub>2</sub>= μ<sub>3</sub>, and now we would like to test:</p>
<p>Test1:</p>
<p style="text-align:center;">
<span class="math inline">\(H_0: \mu_1 = \mu_2\)</span> <br>
</p>
<p>Test2:</p>
<p style="text-align:center;">
<span class="math inline">\(H_0: \mu_1 = \mu_3\)</span> <br>
</p>
<p>Test3:</p>
<p style="text-align:center;">
<span class="math inline">\(H_0: \mu_2 = \mu_3\)</span> <br>
</p>
<p>There are several post hoc procedures available to choose from. In this tutorial, we will cover Bonferroni and Tukey’s HSD (“honest significant differences”). Both tests control for familywise error. Bonferroni tends to have more power when the number of comparisons is small, whereas Tukey’ HSDs is better when testing large numbers of means.</p>
<div id="bonferroni" class="section level4">
<h4><span class="header-section-number">10.3.2.1</span> Bonferroni</h4>
<p>One of the most popular (and easiest) methods to correct for the familywise error rate is to conduct the individual t-tests and divide α by the number of comparisons („k“):</p>
<span class="math display" id="eq:pCR">\[\begin{equation} 
\begin{split}
p_{CR}= \frac{\alpha}{k}
\end{split}
\tag{10.12}
\end{equation}\]</span>
<p>In our example with three groups:</p>
<p style="text-align:center;">
<span class="math inline">\(p_{CR}= \frac{0.05}{3}=0.017\)</span> <br>
</p>
<p>Thus, the “corrected” critical p-value is now 0.017 instead of 0.05 (i.e., the critical t value is higher). You can implement the Bonferroni procedure in R using:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">pairwise.t.test</span>(online_store_promo<span class="op">$</span>Sales, online_store_promo<span class="op">$</span>Promotion, 
    <span class="dt">data =</span> online_store_promo, <span class="dt">p.adjust.method =</span> <span class="st">&quot;bonferroni&quot;</span>)</code></pre></div>
<pre><code>## 
##  Pairwise comparisons using t tests with pooled SD 
## 
## data:  online_store_promo$Sales and online_store_promo$Promotion 
## 
##        high      medium
## medium 0.0329    -     
## low    0.0000066 0.0092
## 
## P value adjustment method: bonferroni</code></pre>
<p>In the output, you will get the corrected p-values for the individual tests. In our example, we can reject H<sub>0</sub> of equal means for all three tests, since p &lt; 0.05 for all combinations of groups.</p>
<p>Note the difference between the results from the post-hoc test compared to individual t-tests. For example, when we test the “medium” vs. “high” groups, the result from a t-test would be:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">data_subset &lt;-<span class="st"> </span><span class="kw">subset</span>(online_store_promo, Promotion <span class="op">!=</span><span class="st"> </span>
<span class="st">    &quot;low&quot;</span>)
<span class="kw">t.test</span>(Sales <span class="op">~</span><span class="st"> </span>Promotion, <span class="dt">data =</span> data_subset)</code></pre></div>
<pre><code>## 
##  Welch Two Sample t-test
## 
## data:  Sales by Promotion
## t = 3.0137, df = 16.834, p-value = 0.007888
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  0.6287384 3.5712616
## sample estimates:
##   mean in group high mean in group medium 
##                  8.3                  6.2</code></pre>
<p>The p-value is lower in the t-test, reflecting the fact that the familywise error is not corrected (i.e., the test is less conservative).</p>
</div>
<div id="tukeys-hsd" class="section level4">
<h4><span class="header-section-number">10.3.2.2</span> Tukey’s HSD</h4>
<p>Tukey’s HSD also compares all possible pairs of means (two-by-two combinations; i.e., like a t-test, except that it corrects for family-wise error rate).</p>
<p>Test statistic:</p>
<span class="math display" id="eq:tukey">\[\begin{equation} 
\begin{split}
HSD= q\sqrt{\frac{MS_R}{n_c}}
\end{split}
\tag{10.13}
\end{equation}\]</span>
<p>where:</p>
<ul>
<li>q = value from studentized range table (see e.g., <a href="http://www.real-statistics.com/statistics-tables/studentized-range-q-table/" target="_blank">here</a>)</li>
<li>MS<sub>R</sub> = Mean Square Error from ANOVA</li>
<li>n<sub>c</sub> = number of observations per group</li>
<li>Decision: Reject H<sub>0</sub> if</li>
</ul>
<p style="text-align:center;">
<span class="math inline">\(|\overline{Y}_i-\overline{Y}_j | &gt; HSD\)</span> , <br>
</p>
<p>The value from the studentized range table can be obtained using the <code>qtukey()</code> function.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">q &lt;-<span class="st"> </span><span class="kw">qtukey</span>(<span class="fl">0.95</span>, <span class="dt">nm =</span> <span class="dv">3</span>, <span class="dt">df =</span> <span class="dv">27</span>)
q</code></pre></div>
<pre><code>## [1] 3.506426</code></pre>
<p>Hence:</p>
<p style="text-align:center;">
<span class="math inline">\(HSD= 3.506\sqrt{\frac{2.96}{10}}=1.906\)</span> <br>
</p>
<p>Or, in R:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">hsd &lt;-<span class="st"> </span>q <span class="op">*</span><span class="st"> </span><span class="kw">sqrt</span>(<span class="kw">summary</span>(aov)[[<span class="dv">1</span>]]<span class="op">$</span><span class="st">&quot;Mean Sq&quot;</span>[<span class="dv">2</span>]<span class="op">/</span><span class="dv">10</span>)
hsd</code></pre></div>
<pre><code>## [1] 1.906269</code></pre>
<p>Since all mean differences between groups are larger than 1.906, we can reject the null hypothsis for all individual tests, confirming the results from the Bonferroni test. To compute Tukey’s HSD, we can use the appropriate function from the <code>multcomp</code> package.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(multcomp)
tukeys &lt;-<span class="st"> </span><span class="kw">glht</span>(aov, <span class="dt">linfct =</span> <span class="kw">mcp</span>(<span class="dt">Promotion =</span> <span class="st">&quot;Tukey&quot;</span>))
<span class="kw">summary</span>(tukeys)</code></pre></div>
<pre><code>## 
##   Simultaneous Tests for General Linear Hypotheses
## 
## Multiple Comparisons of Means: Tukey Contrasts
## 
## 
## Fit: aov(formula = Sales ~ Promotion, data = online_store_promo)
## 
## Linear Hypotheses:
##                    Estimate Std. Error t value Pr(&gt;|t|)    
## medium - high == 0  -2.1000     0.7688  -2.731  0.02850 *  
## low - high == 0     -4.6000     0.7688  -5.983  &lt; 0.001 ***
## low - medium == 0   -2.5000     0.7688  -3.252  0.00826 ** 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## (Adjusted p values reported -- single-step method)</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">confint</span>(tukeys)</code></pre></div>
<pre><code>## 
##   Simultaneous Confidence Intervals
## 
## Multiple Comparisons of Means: Tukey Contrasts
## 
## 
## Fit: aov(formula = Sales ~ Promotion, data = online_store_promo)
## 
## Quantile = 2.4797
## 95% family-wise confidence level
##  
## 
## Linear Hypotheses:
##                    Estimate lwr     upr    
## medium - high == 0 -2.1000  -4.0065 -0.1935
## low - high == 0    -4.6000  -6.5065 -2.6935
## low - medium == 0  -2.5000  -4.4065 -0.5935</code></pre>
<p>We may also plot the result for the mean differences incl. their confidence intervals:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(tukeys)</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-242"></span>
<img src="_main_files/figure-html/unnamed-chunk-242-1.png" alt="Tukey's HSD" width="672" />
<p class="caption">
Figure 10.9: Tukey’s HSD
</p>
</div>
<p>You can see that the CIs do not cross zero, which means that the true difference between group means is unlikely zero.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mean1 &lt;-<span class="st"> </span><span class="kw">mean</span>(online_store_promo[online_store_promo<span class="op">$</span>Promotion <span class="op">==</span><span class="st"> </span>
<span class="st">    &quot;high&quot;</span>, <span class="st">&quot;Sales&quot;</span>])  <span class="co">#mean group &#39;high&#39;</span>
mean1</code></pre></div>
<pre><code>## [1] 8.3</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mean2 &lt;-<span class="st"> </span><span class="kw">mean</span>(online_store_promo[online_store_promo<span class="op">$</span>Promotion <span class="op">==</span><span class="st"> </span>
<span class="st">    &quot;medium&quot;</span>, <span class="st">&quot;Sales&quot;</span>])  <span class="co">#mean group &#39;medium&#39;</span>
mean2</code></pre></div>
<pre><code>## [1] 6.2</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mean3 &lt;-<span class="st"> </span><span class="kw">mean</span>(online_store_promo[online_store_promo<span class="op">$</span>Promotion <span class="op">==</span><span class="st"> </span>
<span class="st">    &quot;low&quot;</span>, <span class="st">&quot;Sales&quot;</span>])  <span class="co">#mean group &#39;low&#39;</span>
mean3</code></pre></div>
<pre><code>## [1] 3.7</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># CI high vs. medium</span>
mean_diff_high_med &lt;-<span class="st"> </span>mean2 <span class="op">-</span><span class="st"> </span>mean1
mean_diff_high_med</code></pre></div>
<pre><code>## [1] -2.1</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">ci_med_high_lower &lt;-<span class="st"> </span>mean_diff_high_med <span class="op">-</span><span class="st"> </span>hsd
ci_med_high_upper &lt;-<span class="st"> </span>mean_diff_high_med <span class="op">+</span><span class="st"> </span>hsd
ci_med_high_lower</code></pre></div>
<pre><code>## [1] -4.006269</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">ci_med_high_upper</code></pre></div>
<pre><code>## [1] -0.1937307</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># CI high vs.low</span>
mean_diff_high_low &lt;-<span class="st"> </span>mean3 <span class="op">-</span><span class="st"> </span>mean1
mean_diff_high_low</code></pre></div>
<pre><code>## [1] -4.6</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">ci_low_high_lower &lt;-<span class="st"> </span>mean_diff_high_low <span class="op">-</span><span class="st"> </span>hsd
ci_low_high_upper &lt;-<span class="st"> </span>mean_diff_high_low <span class="op">+</span><span class="st"> </span>hsd
ci_low_high_lower</code></pre></div>
<pre><code>## [1] -6.506269</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">ci_low_high_upper</code></pre></div>
<pre><code>## [1] -2.693731</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># CI medium vs.low</span>
mean_diff_med_low &lt;-<span class="st"> </span>mean3 <span class="op">-</span><span class="st"> </span>mean2
mean_diff_med_low</code></pre></div>
<pre><code>## [1] -2.5</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">ci_low_med_lower &lt;-<span class="st"> </span>mean_diff_med_low <span class="op">-</span><span class="st"> </span>hsd
ci_low_med_upper &lt;-<span class="st"> </span>mean_diff_med_low <span class="op">+</span><span class="st"> </span>hsd
ci_low_med_lower</code></pre></div>
<pre><code>## [1] -4.406269</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">ci_low_med_upper</code></pre></div>
<pre><code>## [1] -0.5937307</code></pre>
<p>Reporting of post hoc results:</p>
<p>The post hoc tests based on Bonferroni and Tukey’s HSD revealed that sales were significantly higher when using medium vs. low levels, high vs. medium levels, as well high vs. low levels of promotion.</p>
<p><strong>The following video summarizes how to conduct a one-way ANOVA in R</strong></p>
<div align="center">
<iframe width="560" height="315" src="https://www.youtube.com/embed/vl32om-3KpY" frameborder="0" allowfullscreen>
</iframe>
</div>
</div>
</div>
</div>
<div id="n-way-anova" class="section level2">
<h2><span class="header-section-number">10.4</span> N-way ANOVA</h2>
<p>As stated above, N-way ANOVA is used when you have a metric dependent variable and two or more factors with two or more factor levels. In other words, with N-way ANOVA, you can investigate the effects of more than one factor simultaneously. In addition, you can assess interactions between the factors that occur when the effects of one factor on the dependent variable depend on the level (category) of the other factors. An experiment with two or more independent variables is also called a <strong>factorial design</strong> and N-way ANOVA is therefore also referred to as factorial ANOVA.</p>
<p>Let’s extend our example from above and assume that there was a second factor considered in the experiment. Besides the different levels of promotion intensity, the 30 products were also randomly assigned to two experimental groups that determined whether the product was featured in a newsletter or not. Hence, there is a second factor “newsletter” with two factor levels (i.e., “yes” and “no”):</p>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["Obs"],"name":[1],"type":["int"],"align":["right"]},{"label":["Promotion"],"name":[2],"type":["fctr"],"align":["left"]},{"label":["Newsletter"],"name":[3],"type":["fctr"],"align":["left"]},{"label":["Sales"],"name":[4],"type":["int"],"align":["right"]}],"data":[{"1":"1","2":"high","3":"yes","4":"10"},{"1":"2","2":"high","3":"yes","4":"9"},{"1":"3","2":"high","3":"yes","4":"10"},{"1":"4","2":"high","3":"yes","4":"8"},{"1":"5","2":"high","3":"yes","4":"9"},{"1":"6","2":"high","3":"no","4":"8"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<p>This means that we have a 2x3 factorial design since we have one factor with 3 levels (i.e., online promotion (1) “high”, (2) “medium”, (3) “low”), and one factor with 2 levels (i.e., newsletter (1) “yes”, (2) “no”). In a next step, we create a new grouping variable that specifiess the treatment using the <code>paste(...)</code> function. The <code>paste(...)</code> function basically concatenates its arguments and separates them by the string given by <code>sep = &quot;&quot;</code>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">online_store_promo<span class="op">$</span>Group &lt;-<span class="st"> </span><span class="kw">paste</span>(online_store_promo<span class="op">$</span>Promotion, 
    online_store_promo<span class="op">$</span>Newsletter, <span class="dt">sep =</span> <span class="st">&quot;_&quot;</span>)  <span class="co">#create new grouping variable</span>
online_store_promo</code></pre></div>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["Obs"],"name":[1],"type":["int"],"align":["right"]},{"label":["Promotion"],"name":[2],"type":["fctr"],"align":["left"]},{"label":["Newsletter"],"name":[3],"type":["fctr"],"align":["left"]},{"label":["Sales"],"name":[4],"type":["int"],"align":["right"]},{"label":["Group"],"name":[5],"type":["chr"],"align":["left"]}],"data":[{"1":"1","2":"high","3":"yes","4":"10","5":"high_yes"},{"1":"2","2":"high","3":"yes","4":"9","5":"high_yes"},{"1":"3","2":"high","3":"yes","4":"10","5":"high_yes"},{"1":"4","2":"high","3":"yes","4":"8","5":"high_yes"},{"1":"5","2":"high","3":"yes","4":"9","5":"high_yes"},{"1":"6","2":"high","3":"no","4":"8","5":"high_no"},{"1":"7","2":"high","3":"no","4":"9","5":"high_no"},{"1":"8","2":"high","3":"no","4":"7","5":"high_no"},{"1":"9","2":"high","3":"no","4":"7","5":"high_no"},{"1":"10","2":"high","3":"no","4":"6","5":"high_no"},{"1":"1","2":"medium","3":"yes","4":"8","5":"medium_yes"},{"1":"2","2":"medium","3":"yes","4":"8","5":"medium_yes"},{"1":"3","2":"medium","3":"yes","4":"7","5":"medium_yes"},{"1":"4","2":"medium","3":"yes","4":"9","5":"medium_yes"},{"1":"5","2":"medium","3":"yes","4":"6","5":"medium_yes"},{"1":"6","2":"medium","3":"no","4":"4","5":"medium_no"},{"1":"7","2":"medium","3":"no","4":"5","5":"medium_no"},{"1":"8","2":"medium","3":"no","4":"5","5":"medium_no"},{"1":"9","2":"medium","3":"no","4":"6","5":"medium_no"},{"1":"10","2":"medium","3":"no","4":"4","5":"medium_no"},{"1":"1","2":"low","3":"yes","4":"5","5":"low_yes"},{"1":"2","2":"low","3":"yes","4":"7","5":"low_yes"},{"1":"3","2":"low","3":"yes","4":"6","5":"low_yes"},{"1":"4","2":"low","3":"yes","4":"4","5":"low_yes"},{"1":"5","2":"low","3":"yes","4":"5","5":"low_yes"},{"1":"6","2":"low","3":"no","4":"2","5":"low_no"},{"1":"7","2":"low","3":"no","4":"3","5":"low_no"},{"1":"8","2":"low","3":"no","4":"2","5":"low_no"},{"1":"9","2":"low","3":"no","4":"1","5":"low_no"},{"1":"10","2":"low","3":"no","4":"2","5":"low_no"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<p>As you can see, we now have six experimental groups:</p>
<ol style="list-style-type: decimal">
<li>= high promotion, newsletter</li>
<li>= high promotion, no newsletter</li>
<li>= medium promotion, newsletter</li>
<li>= medium promotion, no newsletter</li>
<li>= low promotion, newsletter</li>
<li>= low promotion, no newletter</li>
</ol>
<p>In our analysis, we now focus on the comparsion of the means between the six groups. Let’s inspect the group means:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">by</span>(online_store_promo<span class="op">$</span>Sales, online_store_promo<span class="op">$</span>Group, 
    mean)  <span class="co">#category means</span></code></pre></div>
<pre><code>## online_store_promo$Group: high_no
## [1] 7.4
## -------------------------------------------------------- 
## online_store_promo$Group: high_yes
## [1] 9.2
## -------------------------------------------------------- 
## online_store_promo$Group: low_no
## [1] 2
## -------------------------------------------------------- 
## online_store_promo$Group: low_yes
## [1] 5.4
## -------------------------------------------------------- 
## online_store_promo$Group: medium_no
## [1] 4.8
## -------------------------------------------------------- 
## online_store_promo$Group: medium_yes
## [1] 7.6</code></pre>
<p>We can also plot the means for each factor individually and for both factors combined.</p>
<p>Plot means for first factor “promotion” (same as before):</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co">#Plot of means</span>
<span class="kw">ggplot</span>(online_store_promo, <span class="kw">aes</span>(Promotion, Sales)) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">stat_summary</span>(<span class="dt">fun.y =</span> mean, <span class="dt">geom =</span> <span class="st">&quot;bar&quot;</span>, <span class="dt">fill=</span><span class="st">&quot;White&quot;</span>, <span class="dt">colour=</span><span class="st">&quot;Black&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">stat_summary</span>(<span class="dt">fun.data =</span> mean_cl_normal, <span class="dt">geom =</span> <span class="st">&quot;pointrange&quot;</span>) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&quot;Experimental group (promotion level)&quot;</span>, <span class="dt">y =</span> <span class="st">&quot;Number of sales&quot;</span>) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">theme_bw</span>()</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-247"></span>
<img src="_main_files/figure-html/unnamed-chunk-247-1.png" alt="Plot of means (in-store promotion)" width="672" />
<p class="caption">
Figure 10.10: Plot of means (in-store promotion)
</p>
</div>
<p>Plot means for second factor “newsletter”:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co">#Plot of means</span>
<span class="kw">ggplot</span>(online_store_promo, <span class="kw">aes</span>(Newsletter, Sales)) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">stat_summary</span>(<span class="dt">fun.y =</span> mean, <span class="dt">geom =</span> <span class="st">&quot;bar&quot;</span>, <span class="dt">fill=</span><span class="st">&quot;White&quot;</span>, <span class="dt">colour=</span><span class="st">&quot;Black&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">stat_summary</span>(<span class="dt">fun.data =</span> mean_cl_normal, <span class="dt">geom =</span> <span class="st">&quot;pointrange&quot;</span>) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&quot;Experimental group (newsletter)&quot;</span>, <span class="dt">y =</span> <span class="st">&quot;Number of sales&quot;</span>) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">theme_bw</span>()</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-248"></span>
<img src="_main_files/figure-html/unnamed-chunk-248-1.png" alt="Plot of means (newsletter)" width="672" />
<p class="caption">
Figure 10.11: Plot of means (newsletter)
</p>
</div>
<p>Plot means for both factors together:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(online_store_promo, <span class="kw">aes</span>(<span class="dt">x =</span> <span class="kw">interaction</span>(Newsletter, Promotion), <span class="dt">y =</span> Sales, <span class="dt">fill =</span> Newsletter)) <span class="op">+</span>
<span class="st">  </span><span class="kw">stat_summary</span>(<span class="dt">fun.y =</span> mean, <span class="dt">geom =</span> <span class="st">&quot;bar&quot;</span>, <span class="dt">position =</span> <span class="kw">position_dodge</span>()) <span class="op">+</span>
<span class="st">  </span><span class="kw">stat_summary</span>(<span class="dt">fun.data =</span> mean_cl_normal, <span class="dt">geom =</span> <span class="st">&quot;pointrange&quot;</span>) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">theme_bw</span>()</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-249"></span>
<img src="_main_files/figure-html/unnamed-chunk-249-1.png" alt="Plot of means (interaction)" width="672" />
<p class="caption">
Figure 10.12: Plot of means (interaction)
</p>
</div>
<p>So what is different compared to the one-way ANOVA? In the one-way ANOVA, we computed the sum of squares as:</p>
<span class="math display" id="eq:vardecomp">\[\begin{equation} 
\begin{split}
SS_T= SS_M+SS_R
\end{split}
\tag{10.1}
\end{equation}\]</span>
<p>The main difference is that in an N-way ANOVA the model sum of squares SS<sub>M</sub> consists of different components. In our case:</p>
<span class="math display" id="eq:SSMn">\[\begin{equation} 
\begin{split}
SS_M= SS_{X_1}+SS_{X_2}+SS_{X_1X_2}
\end{split}
\tag{10.14}
\end{equation}\]</span>
<p>That is, we can further decompose the explained variance into the variance explained by the first factor (X<sub>1</sub>), the variance explained by the second factor (X<sub>2</sub>), and the variance explained by the interaction of these factors (X<sub>1</sub>X<sub>2</sub>). The interaction will tell us if the effect of one factor depends on the level of the second factor and vice versa. Because we now have more information available (the manipulation of the second factor), we would expect the amount of explained variance to increase relative to the amount of unexplained variance.</p>
<p>To visualize this, we can include this new information in the plot that we used before to inspect the model sum of squares:</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-250"></span>
<img src="_main_files/figure-html/unnamed-chunk-250-1.png" alt="Sum of Squares (N-way ANOVA)" width="672" />
<p class="caption">
Figure 10.13: Sum of Squares (N-way ANOVA)
</p>
</div>
<p>You can see that our model now better represents the data using the additional information (i.e., the distance between the individual observations and the group mean has decreased). Let’s re-run the ANOVA using the additional information. To do this, we use the same commands as before and simply include the additional factor:</p>
<pre><code>## Levene&#39;s Test for Homogeneity of Variance (center = mean)
##       Df F value Pr(&gt;F)
## group  5   0.689 0.6365
##       24</code></pre>
<pre><code>##                      Df Sum Sq Mean Sq F value        Pr(&gt;F)    
## Promotion             2 106.07   53.03   54.86 0.00000000112 ***
## Newsletter            1  53.33   53.33   55.17 0.00000011439 ***
## Promotion:Newsletter  2   3.27    1.63    1.69         0.206    
## Residuals            24  23.20    0.97                          
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>The Levene’s Test indicates that the variances of the groups are not significantly different. Theoretically, you would also need to test the distributional assumptions for each group again, but we skip this step here.</p>
<p>The ANOVA output shows us that the two main effects are significant, while the interaction is not. This means that online promotions and newsletter features result in higher sales. However, the effect of each factor is independent of the other. Note that in the presence of significant interaction effects, it would make no sense to interpret the main effects! If this would be the case, we would only conclude that the effect of one factor depends on the other factor. If the interaction effect is insignificant (as in our case), you could also conduct post hoc tests for each individual factor. However, you only need to conduct post hoc tests for factors with more than 2 levels (i.e., not for the newsletter factor) since there is no familywise error for variables with two categories.</p>
<p>In an N-way ANOVA, the multiple η<sup>2</sup> measures the strength of the joint effect of two factors (also called the overall effect). To compute the multiple η<sup>2</sup>, the revised equation is:</p>
<span class="math display" id="eq:etan">\[\begin{equation} 
\begin{split}
\eta^2= \frac{SS_{X_1}+SS_{X_2}+SS_{X_1X_2}}{SS_T}
\end{split}
\tag{10.15}
\end{equation}\]</span>
<p>From the output, we can extract the relevant sum of squares as follows:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">(<span class="kw">summary</span>(aov)[[<span class="dv">1</span>]]<span class="op">$</span><span class="st">&quot;Sum Sq&quot;</span>[<span class="dv">1</span>] <span class="op">+</span><span class="st"> </span><span class="kw">summary</span>(aov)[[<span class="dv">1</span>]]<span class="op">$</span><span class="st">&quot;Sum Sq&quot;</span>[<span class="dv">2</span>] <span class="op">+</span><span class="st"> </span>
<span class="st">    </span><span class="kw">summary</span>(aov)[[<span class="dv">1</span>]]<span class="op">$</span><span class="st">&quot;Sum Sq&quot;</span>[<span class="dv">3</span>])<span class="op">/</span>(<span class="kw">summary</span>(aov)[[<span class="dv">1</span>]]<span class="op">$</span><span class="st">&quot;Sum Sq&quot;</span>[<span class="dv">1</span>] <span class="op">+</span><span class="st"> </span>
<span class="st">    </span><span class="kw">summary</span>(aov)[[<span class="dv">1</span>]]<span class="op">$</span><span class="st">&quot;Sum Sq&quot;</span>[<span class="dv">2</span>] <span class="op">+</span><span class="st"> </span><span class="kw">summary</span>(aov)[[<span class="dv">1</span>]]<span class="op">$</span><span class="st">&quot;Sum Sq&quot;</span>[<span class="dv">3</span>] <span class="op">+</span><span class="st"> </span>
<span class="st">    </span><span class="kw">summary</span>(aov)[[<span class="dv">1</span>]]<span class="op">$</span><span class="st">&quot;Sum Sq&quot;</span>[<span class="dv">4</span>])</code></pre></div>
<pre><code>## [1] 0.8751793</code></pre>
<p>As in the one-way ANOVA, we check the residuals plots generated by R to see if the residulas are approximately normally distributed and whether the residual variance is similar across groups.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(aov, <span class="dv">1</span>)  <span class="co">#homogeneity of variances</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-253-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(aov, <span class="dv">2</span>)  <span class="co">#normal distribution of residuals</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-253-2.png" width="672" /></p>
<p>Reporting:</p>
<ul>
<li>There was a significant main effect of promotion on sales, F(2,24) = 53.03, p &lt; 0.05.</li>
<li>The post hoc tests based on Bonferroni and Tukey’s HSD revealed that the sales were significantly higher when using medium vs. low levels, high vs. medium levels, as well high vs. low levels of promotion.</li>
<li>There was a significant main effect of newsletter features on sales levels, F(1,24) = 53.33, p &lt; 0.05.<br />
</li>
<li>The effect of each factor is independent of the other since the interaction effect between the level of promotion and direct mailing was insignificant, F(2,24) = 3.27, p &gt; 0.05.</li>
</ul>
</div>
<div id="non-parametric-tests-1" class="section level2">
<h2><span class="header-section-number">10.5</span> Non-parametric tests</h2>
<p>When should you use non-parametric tests?</p>
<ul>
<li>When the dependent variable is measured at an ordinal scale and we want to compare more than 2 means</li>
<li>When the assumptions of independent ANOVA are not met (e.g., assumptions regarding the sampling distribution in small samples)</li>
</ul>
<p>The Kruskal–Wallis test is the non-parametric counterpart of the one-way independent ANOVA. It is designed to test for significant differences in population medians when you have more than two samples (otherwise you would use the Mann-Whitney U-test). The theory is very similar to that of the Mann–Whitney U-test since it is also based on ranked data. The Kruskal-Wallis test is carried out using the <code>kruskal.test()</code> function. Using the same data as before, we type:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">kruskal.test</span>(Sales <span class="op">~</span><span class="st"> </span>Promotion, <span class="dt">data =</span> online_store_promo)</code></pre></div>
<pre><code>## 
##  Kruskal-Wallis rank sum test
## 
## data:  Sales by Promotion
## Kruskal-Wallis chi-squared = 16.529, df = 2, p-value = 0.0002575</code></pre>
<p>The test-statistic follows a chi-square distribution and since the test is significant (p &lt; 0.05), we can conclude that there are significant differences in population medians. Provided that the overall effect is significant, you may perform a post hoc test to find out which groups are different. To get a first impression, we can plot the data using a boxplot:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co">#Boxplot</span>
<span class="kw">ggplot</span>(online_store_promo, <span class="kw">aes</span>(<span class="dt">x =</span> Promotion, <span class="dt">y =</span> Sales)) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_boxplot</span>() <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&quot;Experimental group (promotion level)&quot;</span>, <span class="dt">y =</span> <span class="st">&quot;Number of sales&quot;</span>) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">theme_bw</span>() </code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-255"></span>
<img src="_main_files/figure-html/unnamed-chunk-255-1.png" alt="Boxplot" width="672" />
<p class="caption">
Figure 10.14: Boxplot
</p>
</div>
<p>To test for differences between groups, we can, for example, apply post hoc tests according to Nemenyi for pairwise multiple comparisons of the ranked data using the appropriate function from the <code>PMCMR</code> package.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(PMCMR)
<span class="kw">posthoc.kruskal.nemenyi.test</span>(<span class="dt">x =</span> online_store_promo<span class="op">$</span>Sales, 
    <span class="dt">g =</span> online_store_promo<span class="op">$</span>Promotion, <span class="dt">dist =</span> <span class="st">&quot;Tukey&quot;</span>)</code></pre></div>
<pre><code>## 
##  Pairwise comparisons using Tukey and Kramer (Nemenyi) test  
##                    with Tukey-Dist approximation for independent samples 
## 
## data:  online_store_promo$Sales and online_store_promo$Promotion 
## 
##        high    medium 
## medium 0.09887 -      
## low    0.00016 0.11683
## 
## P value adjustment method: none</code></pre>
<p>The results reveal that there is a significant difference between the “low” and “high” promotion groups. Note that the results are different compared to the results from the parametric test above. This difference occurs because non-parametric tests have more power to detect differences between groups since we loose information by ranking the data. Thus, you should rely on parametric tests if the assumptions are met.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="hypothesis-testing.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="regression.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
