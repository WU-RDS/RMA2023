[
["index.html", "Marketing Research Design &amp; Analysis 2020 Welcome!", " Marketing Research Design &amp; Analysis 2020 Welcome! This is an introduction to statistics using the statistical software R presented by the Institute for Interactive Marketing and Social Media at WU Vienna. The tutorial is intended to facilitate the learning process by summarizing the content that was covered in class. It includes the code files that we used during each session, along with the commented outputs and explanations in text and video format. You can use it as a reference for this course but also for other courses that you will take throughout your studies. None of the material on the following pages is new, nor is it invented by us. We intend to provide a summary of existing material interesting to the marketing researcher. We have cited the sources used for all the tutorials. However, if we have not given you appropriate credit for your work or failed to cite something correctly please contact us through the github page of this course so we can resolve the issue amicably. "],
["course-materials.html", "Course materials", " Course materials Main reference “Discovering Statistics Using R” (Field, A., Miles, J., &amp; Field Zoe, 2012, 1st Edtn.) This textbook offers an accessible and comprehensive introduction to statistics and will be the main reference for this class. Further readings           In addition to the main readings, there are many excellent books available (many of them for free) that focus on different aspects of R. In case you would like to learn more about the capabilities of R, I can recommend the following books: “R for Data Science” An excellent book by Hadley Wickham, which introduces you to R as a tool for doing data science, focussing on a consistent set of packages known as the tidyverse. [FREE online version] “An Introduction to Statistical Learning” This book provides an introduction to statistical learning methods and covers basic methods (e.g., linear regression) as well as more advanced methods (e.g., Support Vector Machines). [FREE online version] “R for Marketing Research and Analytics” A great book that is designed to teach R to marketing practitioners and data scientists.echo= “Text Mining with R” This book explains how you can analyse unstrunctured data (texts) using R. [FREE online version] “Advanced R” another great book written by Hadley Wickham. Explains more advanced R concepts. [FREE online version] “Hands-On Machine Learning with R” to learn about the approach and to gain intuition about methods accepted in the machine learning in R. The book favors a hands-on approach, growing an intuitive understanding of machine learning through concrete examples and just a little bit of theory.[FREE online version] “Hands-On Data Science for Marketing” another great book written by Hadley Wickham. Explains more advanced R concepts. [FREE Code exercises] Finding Your Way To R   In order to successfully embark on your R tour, Rstudio has created three tracks to help learners navigate the R ecosystem. These tracks are not meant to be exhaustive, but instead are designed to help you become productive in the minimum amount of time, based on your experience level. Therefore, you can choose among 3 different tracks: beginner, intermediate and expert. “https://education.rstudio.com/learn/” free interactive tutorials by Rstudio. DataCamp   Please also make use of the abundance of web resources. For students who would like to further train the materials covered in class, we recommend DataCamp, an online platform that offers interactive courses in data science at different levels. To facilitate the learning process you will obtain full access to the entire DataCamp course curriculum for the duration of the course. Other web-resources “https://www.r-project.org/” official website “http://www.statmethods.net/” R reference by the author of “R in action” “http://www.rdocumentation.org/” R documentation aggregator “http://stackoverflow.com/” general discussion forum for programmers incl. R “http://stats.stackexchange.com/” discussion forum on statistics and data analytics “http://www.r-bloggers.com/” R blog aggregator “http://www.cookbook-r.com/” useful examples for all kind of R problems “https://ggplot2.tidyverse.org/reference/index.html” reference for data visualization "],
["getting-started.html", "1 Getting started 1.1 How to download and install R and RStudio 1.2 Getting help 1.3 Functions 1.4 Packages 1.5 A typical R session", " 1 Getting started In this course, we will work with the statistical software package R. Please make sure R is already installed on your computer before the tutorials start. The Comprehensive R Archive Network (CRAN) contains compiled versions of the program that are ready to use free of charge: “Download R” [FREE download] RStudio provides a graphical user interface (GUI) that makes working with R easier. You can also download RStudio for free: \"Download R Studio(Windows, Linux, OSX, …). Contains statistical routines not yet available in other programs. Active global community (e.g., https://www.r-bloggers.com/). Many specialized user-written packages. It has its own journal (e.g., http://journal.r-project.org). Highly integrated and interfaces to other programs. It is becoming increasingly popular among practitioners. It is a valuable skill to have on the job market. It is not as complicated as you might think. R is powerful. … 1.1 How to download and install R and RStudio 1.2 Getting help Built-in R tutorial: type in “help.start()” to get to the official R tutorial Questions regarding specific functions: type in “?function_name” to get to the help page of specific functions (e.g., “?lm” gives you help on the lm() function) Video tutorials: Make use of one of the many video tutorials on YouTube (e.g., http://www.r-bloggers.com/learn-r-from-the-ground-up/). Errors &amp; warnings: because R is interactive, consider errors your friends! Most importantly: the more time you spend using R, the more comfortable you become with it and it will be easier to see its advantages R Cheatsheets: Cheat sheets make it easy to learn about and use some popular packages (https://www.rstudio.com/resources/cheatsheets/). They can also be accessed from within RStudio under the “help” menu. 1.3 Functions When analyzing data in R, you will access most of the functionalities by calling functions. A function is a piece of code written to carry out a specified task (e.g., the lm()-function to run a linear regression). It may or may not accept arguments or parameters and it may or may not return one or more values. Functions are generally called like this: function_name(arg1 = val1, arg2 = val2, ...) To give you an example, let’s use the built-in seq()-function to generate a sequence of numbers. RStudio has some nice features that help you when writing code. For example, when you type “se” and hit TAB, a pop-up shows you possible completions. The more letters you type in, the more precise the suggestions will become and you will notice that after typing in the third letter, a pop-up with possible completions will appear automatically and you can select the desired function using the ↑/↓ arrows and hitting ENTER. The pop-up even reminds you of the arguments that a function takes. If you require more details, you may either press the F1 key or type in ?seq and you will find the details for the function in the help tab in the lower right pane. When you have selected the desired function from the pop-up, RStudio will automatically add matching opening and closing parentheses (i.e., go from seq to seq()). Within the parentheses you may now type in the arguments that the function takes. Let’s use seq() to generate a sequence of numbers from 1 to 10. To do this, you may include the argument names (i.e., from =, to =), or just the desired values in the correct order. An important thing to note is that R is case-sensitive, meaning that Seq() and seq() are viewed as two different functions by R. seq(from = 1, to = 10) #creates sequence from 1 to 10 ## [1] 1 2 3 4 5 6 7 8 9 10 seq(1,10) #same result ## [1] 1 2 3 4 5 6 7 8 9 10 Note that if you specify the argument names, you may enter them in any order. However, if do not include the argument names you must adhere to the order that is specified for the respective function. seq(to = 10,from = 1) #produces desired results ## [1] 1 2 3 4 5 6 7 8 9 10 seq(10,1) #produces reversed sequence ## [1] 10 9 8 7 6 5 4 3 2 1 1.4 Packages Most of the R functionalities are contained in distinct modules called packages. When R is installed, a small set of packages is also installed. For example, the Base R package contains the basic functions which let R function as a language: arithmetic, input/output, basic programming support, etc.. However, a large number of packages exist that contain specialized functions that will help you to achieve specific tasks. To access the functions outside the scope of the pre-installed packages, you have to install the package first using the install.packages()-function. For example, to install the ggplot2 package to create graphics, type in install.packages(\"ggplot2\"). Note that you only have to install a package once. After you have installed a package, you may load it to access its functionalities using the library()-function. E.g., to load the ggplot2-package, type in library(ggplot2). The number of R packages is rapidly increasing and there are many specialized packages to perform different types of analytics.   The video below provides a more in-depth discussion of packages, their installation and keeping them up to date. 1.5 A typical R session Open RStudio. The following video offers an introduction to the RStudio user interface: Make sure that your working directory is set correctly. The working directory is the location where R will look for files you would like to load and where any files you write to disk will be saved. If you open an existing R script from a specific folder, this folder will, by default, be the working directory. You can check your working directory by using the getwd()-function. In case you wish to change your working directory, you can use the setwd()-function and specify the desired location (i.e., setwd(path_to_project_folder)). Notice that you have to use / instead of \\ to specify the path (i.e., Windows paths copied from the explorer will not work before you change the backward slashes with forward slashes). Alternatively, you can set the working directory with R-Studio by clicking on the “Sessions” tab and selecting “Set Working Directory”. Load your data that you wish to analyze (using procedures that we will cover later) Perform statistical analysis on your data (using methods that we will cover later) Save your workspace. The R workspace is your current working environment incl. any user-defined objects (e.g., data frames, functions). You can save an image of the current workspace to a file called ”.RData”. In fact, RStudio will ask you automatically if you would like to save the workspace when you close the program at the end of the session. In addition, you may save an image of the workspace at any time during the session using the save.image()-function. This saves the workspace image to the current working directory. When you re-open R from that working directory, the workspace will be loaded, and all these things will be available to you again. You may also save the image to any other location by specifying the path to the folder explicitly (i.e., save.image(path_to_project_folder)). If you open R from a different location, you may load the workspace manually using the load(\"\")-function which points to the image file in the respective directory (e.g., load(\"path_to_project_folder/.RData\"). However, saving your workspace is not always required. Especially when you save your work in an R script file (which is highly recommended), you will be able to restore your latest results by simply executing the code contained therein again. This also prevents you from carrying over potential mistakes from one session to the next. "],
["data-handling.html", "2 Data handling 2.1 Basic data handling 2.2 Advanced data handling 2.3 Data import and export", " 2 Data handling This chapter covers the basics of data handling in R 2.1 Basic data handling You can download the corresponding R-Code here 2.1.1 Creating objects Anything created in R is an object. You can assign values to objects using the assignment operator &lt;-: x &lt;- &quot;hello world&quot; #assigns the words &quot;hello world&quot; to the object x #this is a comment Note that comments may be included in the code after a #. The text after # is not evaluated when the code is run; they can be written directly after the code or in a separate line. To see the value of an object, simply type its name into the console and hit enter: x #print the value of x to the console ## [1] &quot;hello world&quot; You can also explicitly tell R to print the value of an object: print(x) #print the value of x to the console ## [1] &quot;hello world&quot; Note that because we assign characters in this case (as opposed to e.g., numeric values), we need to wrap the words in quotation marks, which must always come in pairs. Although RStudio automatically adds a pair of quotation marks (i.e., opening and closing marks) when you enter the opening marks it could be that you end up with a mismatch by accident (e.g., x &lt;- \"hello). In this case, R will show you the continuation character “+”. The same could happen if you did not execute the full command by accident. The “+” means that R is expecting more input. If this happens, either add the missing pair, or press ESCAPE to abort the expression and try again. To change the value of an object, you can simply overwrite the previous value. For example, you could also assign a numeric value to “x” to perform some basic operations: x &lt;- 2 #assigns the value of 2 to the object x print(x) ## [1] 2 x == 2 #checks whether the value of x is equal to 2 ## [1] TRUE x != 3 #checks whether the value of x is NOT equal to 3 ## [1] TRUE x &lt; 3 #checks whether the value of x is less than 3 ## [1] TRUE x &gt; 3 #checks whether the value of x is greater than 3 ## [1] FALSE Note that the name of the object is completely arbitrary. We could also define a second object “y”, assign it a different value and use it to perform some basic mathematical operations: y &lt;- 5 #assigns the value of 2 to the object x x == y #checks whether the value of x to the value of y ## [1] FALSE x*y #multiplication of x and y ## [1] 10 x + y #adds the values of x and y together ## [1] 7 y^2 + 3*x #adds the value of y squared and 3x the value of x together ## [1] 31 Object names Please note that object names must start with a letter and can only contain letters, numbers, as well as the ., and _ separators. It is important to give your objects descriptive names and to be as consistent as possible with the naming structure. In this tutorial we will be using lower case words separated by underscores (e.g., object_name). There are other naming conventions, such as using a . as a separator (e.g., object.name), or using upper case letters (objectName). It doesn’t really matter which one you choose, as long as you are consistent. 2.1.2 Data types The most important types of data are: Data type Description Numeric Approximations of the real numbers, \\(\\normalsize\\mathbb{R}\\) (e.g., mileage a car gets: 23.6, 20.9, etc.) Integer Whole numbers, \\(\\normalsize\\mathbb{Z}\\) (e.g., number of sales: 7, 0, 120, 63, etc.) Character Text data (strings, e.g., product names) Factor Categorical data for classification (e.g., product groups) Logical TRUE, FALSE Date Date variables (e.g., sales dates: 21-06-2015, 06-21-15, 21-Jun-2015, etc.) Variables can be converted from one type to another using the appropriate functions (e.g., as.numeric(),as.integer(),as.character(), as.factor(),as.logical(), as.Date()). For example, we could convert the object y to character as follows: y &lt;- as.character(y) print(y) ## [1] &quot;5&quot; Notice how the value is in quotation marks since it is now of type character. Entering a vector of data into R can be done with the c(x1,x2,..,x_n) (“concatenate”) command. In order to be able to use our vector (or any other variable) later on we want to assign it a name using the assignment operator &lt;-. You can choose names arbitrarily (but the first character of a name cannot be a number). Just make sure they are descriptive and unique. Assigning the same name to two variables (e.g. vectors) will result in deletion of the first. Instead of converting a variable we can also create a new one and use an existing one as input. In this case we ommit the as. and simply use the name of the type (e.g. factor()). There is a subtle difference between the two: When converting a variable, with e.g. as.factor(), we can only pass the variable we want to convert without additional arguments and R determines the factor levels by the existing unique values in the variable or just returns the variable itself if it is a factor already. When we specifically create a variable (just factor(), matrix(), etc.), we can and should set the options of this type explicitly. For a factor variable these could be the labels and levels, for a matrix the number of rows and columns and so on. #Numeric: top10_track_streams &lt;- c(163608, 126687, 120480, 110022, 108630, 95639, 94690, 89011, 87869, 85599) #Character: top10_artist_names &lt;- c(&quot;Axwell /\\\\ Ingrosso&quot;, &quot;Imagine Dragons&quot;, &quot;J. Balvin&quot;, &quot;Robin Schulz&quot;, &quot;Jonas Blue&quot;, &quot;David Guetta&quot;, &quot;French Montana&quot;, &quot;Calvin Harris&quot;, &quot;Liam Payne&quot;, &quot;Lauv&quot;) # Characters have to be put in &quot;&quot; #Factor variable with two categories: top10_track_explicit &lt;- c(0,0,0,0,0,0,1,1,0,0) top10_track_explicit &lt;- factor(top10_track_explicit, levels = 0:1, labels = c(&quot;not explicit&quot;, &quot;explicit&quot;)) #Factor variable with more than two categories: top10_artist_genre &lt;- c(&quot;Dance&quot;,&quot;Alternative&quot;,&quot;Latino&quot;,&quot;Dance&quot;,&quot;Dance&quot;,&quot;Dance&quot;,&quot;Hip-Hop/Rap&quot;,&quot;Dance&quot;,&quot;Pop&quot;,&quot;Pop&quot;) top10_artist_genre &lt;- as.factor(top10_artist_genre) #Date: top_10_track_release_date &lt;- as.Date(c(&quot;2017-05-24&quot;, &quot;2017-06-23&quot;, &quot;2017-07-03&quot;, &quot;2017-06-30&quot;, &quot;2017-05-05&quot;, &quot;2017-06-09&quot;, &quot;2017-07-14&quot;, &quot;2017-06-16&quot;, &quot;2017-05-18&quot;, &quot;2017-05-19&quot;)) #Logical top10_track_explicit_1 &lt;- c(FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE) In order to “call” a vector we can now simply enter its name: top10_track_streams ## [1] 163608 126687 120480 110022 108630 95639 94690 89011 87869 85599 top_10_track_release_date ## [1] &quot;2017-05-24&quot; &quot;2017-06-23&quot; &quot;2017-07-03&quot; &quot;2017-06-30&quot; &quot;2017-05-05&quot; ## [6] &quot;2017-06-09&quot; &quot;2017-07-14&quot; &quot;2017-06-16&quot; &quot;2017-05-18&quot; &quot;2017-05-19&quot; In order to check the type of a variable the class() function is used. class(top_10_track_release_date) ## [1] &quot;Date&quot; The video below gives a general overview of vectors and provides a more in-depth discussion. 2.1.3 Data structures Now let’s create a table that contains the variables in columns and each observation in a row (like in SPSS or Excel). There are different data structures in R (e.g., Matrix, Vector, List, Array). In this course, we will mainly use data frames. Data frames are similar to matrices but are more flexible in the sense that they may contain different data types (e.g., numeric, character, etc.), where all values of vectors and matrices have to be of the same type (e.g. character). It is often more convenient to use characters instead of numbers (e.g. when indicating a persons sex: “F”, “M” instead of 1 for female , 2 for male). Thus we would like to combine both numeric and character values while retaining the respective desired features. This is where “data frames” come into play. Data frames can have different types of data in each column. For example, we can combine the vectors created above in one data frame using data.frame(). This creates a separate column for each vector, which is usually what we want (similar to SPSS or Excel). music_data &lt;- data.frame(top10_track_streams, top10_artist_names, top10_track_explicit, top10_artist_genre, top_10_track_release_date, top10_track_explicit_1) 2.1.3.1 Accessing data in data frames When entering the name of a data frame, R returns the entire data frame: music_data # Returns the entire data frame Hint: You may also use the View()-function to view the data in a table format (like in SPSS or Excel), i.e. enter the command View(data). Note that you can achieve the same by clicking on the small table icon next to the data frame in the “Environment”-window on the right in RStudio. Sometimes it is convenient to return only specific values instead of the entire data frame. There are a variety of ways to identify the elements of a data frame. One easy way is to explicitly state, which rows and columns you wish to view. The general form of the command is data.frame[rows,columns]. By leaving one of the arguments of data.frame[rows,columns] blank (e.g., data.frame[rows,]) we tell R that we want to access either all rows or columns, respectively. Here are some examples: music_data[ , 2:4] # all rows and columns 2,3,4 music_data[ ,c(&quot;top10_artist_names&quot;, &quot;top_10_track_release_date&quot;)] # all rows and columns &quot;top10_artist_names&quot; and &quot;top_10_track_release_date&quot; music_data[1:5, c(&quot;top10_artist_names&quot;, &quot;top_10_track_release_date&quot;)] # rows 1 to 5 and columns &quot;top10_artist_names&quot;&quot; and &quot;top_10_track_release_date&quot; You may also create subsets of the data frame, e.g., using mathematical expressions: music_data[top10_track_explicit == &quot;explicit&quot;,] # show only tracks with explicit lyrics music_data[top10_track_streams &gt; 100000,] # show only tracks with more than 100,000 streams music_data[top10_artist_names == &#39;Robin Schulz&#39;,] # returns all observations from artist &quot;Robin Schulz&quot; music_data[top10_track_explicit == &quot;explicit&quot;,] # show only explicit tracks The same can be achieved using the subset()-function subset(music_data,top10_track_explicit == &quot;explicit&quot;) # selects subsets of observations in a data frame #creates a new data frame that only contains tracks from genre &quot;Dance&quot; music_data_dance &lt;- subset(music_data,top10_artist_genre == &quot;Dance&quot;) music_data_dance rm(music_data_dance) # removes an object from the workspace You may also change the order of the variables in a data frame by using the order()-function #Orders by genre (ascending) and streams (descending) music_data[order(top10_artist_genre,-top10_track_streams),] 2.1.3.2 Inspecting the content of a data frame The head() function displays the first X elements/rows of a vector, matrix, table, data frame or function. head(music_data, 3) # returns the first X rows (here, the first 3 rows) The tail() function is similar, except it displays the last elements/rows. tail(music_data, 3) # returns the last X rows (here, the last 3 rows) names() returns the names of an R object. When, for example, it is called on a data frame, it returns the names of the columns. names(music_data) # returns the names of the variables in the data frame ## [1] &quot;top10_track_streams&quot; &quot;top10_artist_names&quot; ## [3] &quot;top10_track_explicit&quot; &quot;top10_artist_genre&quot; ## [5] &quot;top_10_track_release_date&quot; &quot;top10_track_explicit_1&quot; str() displays the internal structure of an R object. In the case of a data frame, it returns the class (e.g., numeric, factor, etc.) of each variable, as well as the number of observations and the number of variables. str(music_data) # returns the structure of the data frame ## &#39;data.frame&#39;: 10 obs. of 6 variables: ## $ top10_track_streams : num 163608 126687 120480 110022 108630 ... ## $ top10_artist_names : Factor w/ 10 levels &quot;Axwell /\\\\ Ingrosso&quot;,..: 1 5 6 10 7 3 4 2 9 8 ## $ top10_track_explicit : Factor w/ 2 levels &quot;not explicit&quot;,..: 1 1 1 1 1 1 2 2 1 1 ## $ top10_artist_genre : Factor w/ 5 levels &quot;Alternative&quot;,..: 2 1 4 2 2 2 3 2 5 5 ## $ top_10_track_release_date: Date, format: &quot;2017-05-24&quot; &quot;2017-06-23&quot; ... ## $ top10_track_explicit_1 : logi FALSE FALSE FALSE FALSE FALSE FALSE ... nrow() and ncol() return the rows and columns of a data frame or matrix, respectively. dim() displays the dimensions of an R object. nrow(music_data) # returns the number of rows ## [1] 10 ncol(music_data) # returns the number of columns ## [1] 6 dim(music_data) # returns the dimensions of a data frame ## [1] 10 6 ls() can be used to list all objects that are associated with an R object. ls(music_data) # list all objects associated with an object ## [1] &quot;top_10_track_release_date&quot; &quot;top10_artist_genre&quot; ## [3] &quot;top10_artist_names&quot; &quot;top10_track_explicit&quot; ## [5] &quot;top10_track_explicit_1&quot; &quot;top10_track_streams&quot; 2.1.3.3 Append and delete variables to/from data frames To call a certain column in a data frame, we may also use the $ notation. For example, this returns all values associated with the variable “top10_track_streams”: music_data$top10_track_streams ## [1] 163608 126687 120480 110022 108630 95639 94690 89011 87869 85599 Assume that you wanted to add an additional variable to the data frame. You may use the $ notation to achieve this: # Create new variable as the log of the number of streams music_data$log_streams &lt;- log(music_data$top10_track_streams) # Create an ascending count variable which might serve as an ID music_data$obs_number &lt;- 1:nrow(music_data) head(music_data) To delete a variable, you can simply create a subset of the full data frame that excludes the variables that you wish to drop: music_data &lt;- subset(music_data,select = -c(log_streams)) # deletes the variable log streams head(music_data) You can also rename variables in a data frame, e.g., using the rename()-function from the plyr package. In the following code “::” signifies that the function “rename” should be taken from the package “plyr”. This can be useful if multiple packages have a function with the same name. Calling a function this way also means that you can access a function without loading the entire package via library(). library(plyr) music_data &lt;- plyr::rename(music_data, c(top10_artist_genre=&quot;genre&quot;,top_10_track_release_date=&quot;release_date&quot;)) head(music_data) Note that the same can be achieved using: names(music_data)[names(music_data)==&quot;genre&quot;] &lt;- &quot;top10_artist_genre&quot; head(music_data) Or by referring to the index of the variable: names(music_data)[4] &lt;- &quot;genre&quot; head(music_data) 2.2 Advanced data handling This chapter covers more advanced techniques for handling data in R. It is primarily based on Wickham, H., &amp; Grolemund, G. (2016). R for Data Science - Import, Tidy, Transform, Visualize, and Model Data. O’Reilly Media. You can download the corresponding R-Code here 2.2.1 The dplyr package There are many ways to achieve the same thing in R. Data transformation and handling tasks can be solved with the functions provided by base R (i.e. the functions that come with every R installation), but the dplyrpackage offers a comprehensive suite of functions that makes many tasks easier, while keeping code very readable. As such, we will be presenting it here, but keep in mind that all of these tasks could also be achieved without any additional packages. In addition to this section, take a look at the “Data Wrangling” cheat sheet, for more information on dplyr’s functionality. Before we continue, ensure that the dplyr() package is installed and loaded. The dplyr() package includes six core functions that make many data handling tasks a lot easier: filter(): filter rows select(): select columns arrange(): re-order or arrange rows mutate(): create new columns summarise(): summarise values group_by(): allows for group operations Each of these functions will be discussed in the following in more detail. 2.2.1.1 Filter rows One of the most basic tasks one could wish to perform on a data set is select certain observations based on various characteristics. dplyr uses the filter() function to this end. To select certain rows from a data set you simply supply the data frame as the first argument and then tell filter() the logical criteria it should use to select observations. While this may sound fairly abstract, it will become very clear after a few examples. Recall the music_data data frame from the previous chapter. Suppose we want to select only observations where the lyrics are not explicit. The code to achieve this would looks as follows: filter(music_data, top10_track_explicit == &quot;not explicit&quot;) The first argument supplied to the filter() function is the data frame we want to subset. The second argument tells filter() that we only want observations where the column top10_track_explicit is equal to the value \"not explicit\". If you look into the output you will notice that only tracks with non explicit lyrics have been returned. Another way to filter observations is to choose all observations where one column is within a certain range. This can be achieved with the logical operators introduced in the basic data handling chapter. In the following example we select all tracks with less than 100000 streams. filter(music_data, top10_track_streams &lt; 100000) You can enforce multiple conditions with &amp;. The following example selects all observations with less than 150000 but more than 100000 streams. filter(music_data, top10_track_streams &gt; 100000 &amp; top10_track_streams &lt; 150000) The | symbol is the way R expresses “or”. This way you can select observations that fulfill either one or the other condition. Say we would like to select all observations with less than 100000 or more than 150000 streams. The following code would do exactly that filter(music_data, top10_track_streams &lt; 100000 | top10_track_streams &gt; 150000) A very useful feature of the filter() function is its ability to accept multiple criteria at once. Say we want to select all tracks marked as \"not explicit\" with less than 100000 streams. This can be achieved by simply supplying the function with additional arguments, as in the example below. Notice that this is equivalent to using the &amp; operator. filter(music_data, top10_track_explicit == &quot;not explicit&quot;, top10_track_streams &lt; 100000) 2.2.1.2 Select columns Another common task is to select or exclude certain columns of a data frame. The dplyr package contains the select() function for exactly this purpose. Similarly to filter() you first supply the function with the data frame you wish to apply the selection to, followed by the columns you wish to select or exclude. The following code selects the two columns top10_track_explicit and top10_track_streams from the music_data data set. select(music_data, top10_track_explicit, top10_track_streams) To remove columns from a data frame you simply put a - before the column name. select(music_data, -top10_track_explicit, -top10_track_streams) You can also select or exclude a whole range of columns through numbers or names. # Selects all columns from top10_track_explicit to top_10_track_release_date select(music_data, top10_track_explicit:top_10_track_release_date) # This is equivalent to select(music_data, 3:5) 2.2.1.3 Arrange rows If you just want to change the order of a data frame without discarding any observations or columns, you can use the arrange() function. It takes a data frame and a set of column names to order by, always in ascending order. If you provide more than one column name, each additional column will be used to break ties in the values of preceding columns. arrange(music_data, top10_artist_genre, top10_track_streams) If you wish to arrange them in descending order, you can wrap the column name in the desc() function. arrange(music_data, top10_artist_genre, desc(top10_track_streams)) 2.2.1.4 Adding and changing variables There are two functions to create new variables based on other variables in the dplyr package, mutate() and transmute(). They both do the same thing, with one key difference: mutate() returns both the newly created variables and the ones previously contained in the data frame, while transmute() only returns the newly created variables. Both functions take variables already present in the data set and apply a function to them to generate new variables. This can be something as simple as adding 1 to every entry in a column or more complex procedures, like standardizing variables. The syntax is fairly straightforward: The first argument is the data frame we wish to apply the transformation to, and all the following arguments are the new variables we want to create. An example will make this clearer. Say we (for some reason) want to add 10 to every entry of the top10_track_streams column in our data set. The code to do this would look as follows: mutate(music_data, streams_plus_10 = top10_track_streams + 10) This code essentially defines a new column named streams_plus_10, which is the column top10_track_streams + 10. As previously mentioned, we can also perform more complex operations, such as standardizing variables (i.e. subtracting the mean value and dividing by the standard deviation). Note that we are generating the mean and the standard deviation in our code by applying functions (mean() and sd()) to the entire column and then we use these values to perform the standardization on each value of the column. mutate(music_data, streams_standardised = (top10_track_streams - mean(top10_track_streams))/ sd(top10_track_streams)) Note that you could also use the scale() function to do the same: mutate(music_data, streams_standardised = scale(top10_track_streams)) You can also add other vectors to an existing data frame with mutate, given that it is the same length as the data frame you want to add it to. # create a vector of length 10 extra_column &lt;- c(1,2,3,4,5,6,7,8,9,10) mutate(music_data, new_data = extra_column) If you don’t want to add a new variable and only want to edit a variable already present in the data frame, dplyr has you covered with the mutate_at() function. The practical thing about this function is that it can also be applied to a whole range of variables if you want to perform the same operation on multiple columns. To do this, you give the function not just a single column name, but a whole vector of column names. Say we first want to change the type of a single column. For example, maybe we want top10_track_explicit to be a character column and not a factor. We again tell mutate_at() which data frame we want to change, followed by the column(s) to change and finally the function we want to apply. You may have noticed that the column name is in quotation marks here, as opposed to the other functions we have met from the dplyrpackage. This is a side effect of its ability to apply a function to a range of columns and not just a single one, as this can only be achieved via a character vector. So from a technical standpoint we are giving the mutate_at() function a character vector of length one, solely containing the element \"top10_track_explicit\". mutate_at(music_data, &quot;top10_track_explicit&quot;, as.character) Now say we want to change multiple columns to be character vectors. To do this we will first create a vector of the names of the columns we want to apply the function to and then simply give mutate_at()this vector as one of its arguments. If you do this, don’t forget the c() function to create a vector. columns &lt;- c(&quot;top10_track_explicit&quot;, &quot;top10_artist_genre&quot;, &quot;top10_track_explicit_1&quot;) mutate_at(music_data, columns, as.character) Note that if you merely want to rename a variable without changing its content, you may use the rename() function to achieve this. The syntax may seem familiar at this point, with the first argument being the data frame to apply the function to and the following arguments being the transformations to apply. The example changes the names of the top10_track_explicit and top10_artist_names columns into explicit and names, respectively. rename(music_data, explicit = top10_track_explicit, names = top10_artist_names) 2.2.1.5 Creating custom summaries The summarise() function lets you build customized summaries of your data. This can range from creating means and standard deviations of certain variables to simply counting how many observations are in a data frame. Say we want to find out the mean and standard deviation of the number of streams and also count the number of observations. With summarise() that would look as follows: summarise(music_data, n_observations = n(), mean_streams = mean(top10_track_streams), sd_streams = sd(top10_track_streams)) On its own, this function is not that impressive. After all, we could just apply the mean(), sd() and nrow() functions individually and would have gotten the same result, albeit not in such a nice format. However, when combined with the group_by() function, summarise() becomes very useful as we will see next. 2.2.1.6 Group operations The group_by() splits a data frame into groups, by the values of a column in the data frame. Say we wanted to calculate the mean and standard deviation of explicit and non-explicit songs separately. music_data &lt;- group_by(music_data, top10_track_explicit) summarise(music_data, n_observations = n(), mean_streams = mean(top10_track_streams), sd_streams = sd(top10_track_streams)) 2.2.1.7 Pipes A very practical feature of the dplyr package are so called “pipes”. Say you want to apply three of the previously mentioned functions to the same data frame. So far, the way we learned to do this would be as follows: # First use select() to take only certain columns music_data_new &lt;- select(music_data, top10_track_explicit_1, top10_artist_names, top10_track_streams) # Now use filter() to choose only rows that fulfill certain criteria music_data_new &lt;- filter(music_data_new, top10_track_streams &lt; 100000) # Then change order with arrange() music_data_new &lt;- arrange(music_data_new, top10_track_streams) # Print to console music_data_new While this does achieve our objective, it is quite tedious. With the pipes offered by dplyr, you can chain these commands together to streamline your code, while keeping it very readable. The symbol for a pipe is %&gt;%. From a technical perspective, this hands the preceding object to the next function as the first argument. This may sound complicated, but will become clear after an example. The code below will create exactly the same data frame as the example above, but in a much more compact form. music_data_new &lt;- music_data %&gt;% select(top10_track_explicit_1, top10_artist_names, top10_track_streams) %&gt;% filter(top10_track_streams &lt; 100000) %&gt;% arrange(top10_track_streams) # Print to console music_data_new Let’s unpack what happened here. The first line “pipes” music_data into the first function, select(), which is in the second line. Here we remove all columns except for top10_track_explicit_1, top10_artist_names and top10_track_streams. Then we take this data frame with the reduced columns and hand it to filter(), which only selects observations with less than 100000 streams. Finally, we pass the filtered, column reduced data frame to arrange, which sorts the rows by the number of streams per track. The assignment operator (&lt;-) at the top then saves this data frame in the environment as music_new_data. Note that, in contrast to the previous examples, we no longer have to specify which data frame we want to apply the various functions to, as the pipes take care of this for us. 2.2.2 Dealing with strings Strings (which is short for “character strings”), can be tough to deal with. They are unstructured, messy and getting them into a format that one can perform analysis with is often a task that requires a lot of time. However, seeing as they appear fairly frequently in data sets and often contain valuable information, it is definitely worth the time to learn how to deal with them. 2.2.2.1 The stringr package A very accessible package for manipulating strings is the stringr package. It is designed to be as uniform as possible, meaning that once you have understood the basic syntax of any one of its functions it is very easy to apply all of them. It sacrifices some flexibility for this simplicity, so if you ever encounter a task you can not easily solve with stringr it is worth checking out the package it is built on, stringi. For now, however, stringr will be more than sufficient. The majority of functions in stringr are built around two core arguments: a string to be worked on and a pattern. There are quite a few that aren’t, such as str_length(), but these are (for the most part) fairly self explanatory and will not be explained further here. A good overview of the included functions can be found here (downloads a pdf). The string to be worked on can either be an individual string in quotation marks or an entire vector or column of strings that the same operation should be applied to. The pattern can technically also be a vector of patterns to look for, however, in this tutorial we will only use single patterns. Recall the list of artist names in the music_data data frame. music_data$top10_artist_names ## [1] &quot;Axwell /\\\\ Ingrosso&quot; &quot;Imagine Dragons&quot; &quot;J. Balvin&quot; ## [4] &quot;Robin Schulz&quot; &quot;Jonas Blue&quot; &quot;David Guetta&quot; ## [7] &quot;French Montana&quot; &quot;Calvin Harris&quot; &quot;Liam Payne&quot; ## [10] &quot;Lauv&quot; Say, for example, we want to see which names contain a “g”. stringr contains the function str_detect, which tells you exactly that. Note that these functions are all case sensitive, i.e. the “G” in “David Guetta” is not detected. str_detect(string = music_data$top10_artist_names, pattern = &quot;g&quot;) ## [1] TRUE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE Say we want to be a bit more specific and want to know exactly where the g is located in each artist name. str_locate_all returns a list of start and end values for each entry in the character vector. str_locate_all(string = music_data$top10_artist_names, pattern = &quot;g&quot;) ## [[1]] ## start end ## [1,] 13 13 ## ## [[2]] ## start end ## [1,] 4 4 ## [2,] 12 12 ## ## [[3]] ## start end ## ## [[4]] ## start end ## ## [[5]] ## start end ## ## [[6]] ## start end ## ## [[7]] ## start end ## ## [[8]] ## start end ## ## [[9]] ## start end ## ## [[10]] ## start end This obviously also works for patterns longer than just one letter. str_locate_all(string = music_data$top10_artist_names, pattern = &quot;vin&quot;) ## [[1]] ## start end ## ## [[2]] ## start end ## ## [[3]] ## start end ## [1,] 7 9 ## ## [[4]] ## start end ## ## [[5]] ## start end ## ## [[6]] ## start end ## ## [[7]] ## start end ## ## [[8]] ## start end ## [1,] 4 6 ## ## [[9]] ## start end ## ## [[10]] ## start end Another common task is to replace all instances of one character with another character. Say, for example, we want to replace all occurrences of “a” with “b”. The function str_replace_all() does exactly that. Note that there is also a function called str_replace(), but this only replaces the first match that the pattern finds. str_replace_all(music_data$top10_artist_names, pattern = &quot;a&quot;, replacement = &quot;b&quot;) ## [1] &quot;Axwell /\\\\ Ingrosso&quot; &quot;Imbgine Drbgons&quot; &quot;J. Bblvin&quot; ## [4] &quot;Robin Schulz&quot; &quot;Jonbs Blue&quot; &quot;Dbvid Guettb&quot; ## [7] &quot;French Montbnb&quot; &quot;Cblvin Hbrris&quot; &quot;Libm Pbyne&quot; ## [10] &quot;Lbuv&quot; A common application for this function is to replace all “.” with “,” or vice versa in price data. R, for example, needs the decimal separator to be a “.”, so if you want to perform numerical analyses on price data that isn’t in a uniform format, this function can be very useful. Say we want to replace all “.” with “,” in the artist names. str_replace_all(music_data$top10_artist_names, pattern = &quot;.&quot;, replacement = &quot;,&quot;) ## [1] &quot;,,,,,,,,,,,,,,,,,,&quot; &quot;,,,,,,,,,,,,,,,&quot; &quot;,,,,,,,,,&quot; ## [4] &quot;,,,,,,,,,,,,&quot; &quot;,,,,,,,,,,&quot; &quot;,,,,,,,,,,,,&quot; ## [7] &quot;,,,,,,,,,,,,,,&quot; &quot;,,,,,,,,,,,,,&quot; &quot;,,,,,,,,,,&quot; ## [10] &quot;,,,,&quot; This result may be somewhat surprising. Why did the function replace every character with “,” even though we specified pattern = \".\"? The explanation for this can be found in how the function matches patterns. 2.2.2.2 A crash course in regex Behind the scenes stringr uses something called regex (short for “regular expressions”) to match patterns ( see cheat sheet ). Regex allows you to match not just individual letters, but more abstract patterns. It does this by using special characters that do not match literally. You have already met the first such special character “.”, which is regex’s version of a wildcard, meaning that it matches any other character. That’s why, when we told stringr to replace all dots with commas, it replaced every character with a comma. This raises the question of what to do if we want to literally match a dot and not any character. There are two ways to go about this. First, we could escape the wildcard character. What this means is that we use another special character to tell regex that the next character is meant to be taken literally and not as a wildcard. The symbol for this is a double back slash “\\\\”. So, if we wanted to change only the dot to a comma we would write str_replace_all(music_data$top10_artist_names, pattern = &quot;\\\\.&quot;, replacement = &quot;,&quot;) ## [1] &quot;Axwell /\\\\ Ingrosso&quot; &quot;Imagine Dragons&quot; &quot;J, Balvin&quot; ## [4] &quot;Robin Schulz&quot; &quot;Jonas Blue&quot; &quot;David Guetta&quot; ## [7] &quot;French Montana&quot; &quot;Calvin Harris&quot; &quot;Liam Payne&quot; ## [10] &quot;Lauv&quot; The second way to achieve this would be by passing the pattern string through the fixed() function, which tells stringr that it should take the entire string literally. This means that the following code achieves the same result as using “\\\\”. str_replace_all(music_data$top10_artist_names, pattern = fixed(&quot;.&quot;), replacement = &quot;,&quot;) ## [1] &quot;Axwell /\\\\ Ingrosso&quot; &quot;Imagine Dragons&quot; &quot;J, Balvin&quot; ## [4] &quot;Robin Schulz&quot; &quot;Jonas Blue&quot; &quot;David Guetta&quot; ## [7] &quot;French Montana&quot; &quot;Calvin Harris&quot; &quot;Liam Payne&quot; ## [10] &quot;Lauv&quot; 2.2.2.2.1 Other special characters in regex We will quickly go through the most important regex special characters. Be aware that this list is by no means exhaustive and is only meant to give you some basic tools that can help you with string manipulation. Keep this in mind if a regex is displaying unexpected behavior, as it could be due to some wildcard you are not aware of. If all else fails you can always used the fixed() function to just match literal strings. 2.2.2.2.1.1 Square brackets [] Square brackets can be used to match from a set of different letters. This means that [abc] will match a, b or c. The following code will replace a, b or c with a capital X. str_replace_all(music_data$top10_artist_names, pattern = &quot;[abc]&quot;, replacement = &quot;X&quot;) ## [1] &quot;Axwell /\\\\ Ingrosso&quot; &quot;ImXgine DrXgons&quot; &quot;J. BXlvin&quot; ## [4] &quot;RoXin SXhulz&quot; &quot;JonXs Blue&quot; &quot;DXvid GuettX&quot; ## [7] &quot;FrenXh MontXnX&quot; &quot;CXlvin HXrris&quot; &quot;LiXm PXyne&quot; ## [10] &quot;LXuv&quot; Note again that this is case sensitive, meaning that A, B and C are not replaced. However, square brackets are a great way to replace both capitalized and non-capitalized occurrences at once. str_replace_all(music_data$top10_artist_names, pattern = &quot;[ABCabc]&quot;, replacement = &quot;X&quot;) ## [1] &quot;Xxwell /\\\\ Ingrosso&quot; &quot;ImXgine DrXgons&quot; &quot;J. XXlvin&quot; ## [4] &quot;RoXin SXhulz&quot; &quot;JonXs Xlue&quot; &quot;DXvid GuettX&quot; ## [7] &quot;FrenXh MontXnX&quot; &quot;XXlvin HXrris&quot; &quot;LiXm PXyne&quot; ## [10] &quot;LXuv&quot; If you include a ^ in the beginning of a square bracket pattern, regex will interpret that to mean any character except the ones in brackets. This means that if we take the same code as before and include a ^, all letters except for A, B and C (and their non-capitalized counterparts) will be replaced by a capital X. str_replace_all(music_data$top10_artist_names, pattern = &quot;[^ABCabc]&quot;, replacement = &quot;X&quot;) ## [1] &quot;AXXXXXXXXXXXXXXXXX&quot; &quot;XXaXXXXXXXaXXXX&quot; &quot;XXXBaXXXX&quot; ## [4] &quot;XXbXXXXcXXXX&quot; &quot;XXXaXXBXXX&quot; &quot;XaXXXXXXXXXa&quot; ## [7] &quot;XXXXcXXXXXXaXa&quot; &quot;CaXXXXXXaXXXX&quot; &quot;XXaXXXaXXX&quot; ## [10] &quot;XaXX&quot; 2.2.2.2.1.2 Repetition operators: *,+ and {} Repetition operators can be used to match the same character (or set of characters) multiple times. + matches a character one or more times, * matches a character zero or more times and with {} you can specify the range that matches can occur in. vector &lt;- c(&quot;&quot;, &quot;a&quot;, &quot;aa&quot;, &quot;aaa&quot;, &quot;aaaa&quot;) # Replace one or more a with an X str_replace(vector, pattern = &quot;a+&quot;, replacement = &quot;X&quot;) ## [1] &quot;&quot; &quot;X&quot; &quot;X&quot; &quot;X&quot; &quot;X&quot; # replace zero or more a with an X str_replace(vector, pattern = &quot;a*&quot;, replacement = &quot;X&quot;) ## [1] &quot;X&quot; &quot;X&quot; &quot;X&quot; &quot;X&quot; &quot;X&quot; # replace exactly two a with an X str_replace(vector, pattern = &quot;a{2}&quot;, replacement = &quot;X&quot;) ## [1] &quot;&quot; &quot;a&quot; &quot;X&quot; &quot;Xa&quot; &quot;Xaa&quot; # replace two to three a with an X str_replace(vector, pattern = &quot;a{2,3}&quot;, replacement = &quot;X&quot;) ## [1] &quot;&quot; &quot;a&quot; &quot;X&quot; &quot;X&quot; &quot;Xa&quot; Note that the + and * operators are “greedy”, meaning that they try to match as much as possible, which can often lead to unintended consequences. It is often a good practice with regex to be as specific as possible while remaining as general as needed. 2.2.2.2.1.3 Parentheses: () Parentheses are used to create groups. Groups always match in their entirety and can be combined with other operators. vector &lt;- c(&quot;abc&quot;, &quot;abcabc&quot;, &quot;123abc&quot;, &quot;abcabcabc&quot;) str_replace_all(vector, pattern = &quot;(abc){2}&quot;, replacement = &quot;X&quot;) ## [1] &quot;abc&quot; &quot;X&quot; &quot;123abc&quot; &quot;Xabc&quot; The pattern \"(abc){2}\" will match only \"abcabc\", seeing as it looks for matches that repeat the group \"(abc)\" twice. 2.2.2.2.1.4 Optional characters: ? The question mark tells regex that the preceding character is optional for a match. vector &lt;- c(&quot;abc&quot;, &quot;ac&quot;) str_replace_all(vector, pattern = &quot;abc&quot;, replacement = &quot;X&quot;) ## [1] &quot;X&quot; &quot;ac&quot; As expected, this only replaces the first element of the vector, as the second (\"ac\") is not an exact match. str_replace_all(vector, pattern = &quot;ab?c&quot;, replacement = &quot;X&quot;) ## [1] &quot;X&quot; &quot;X&quot; By including ? after the b, we tell regex that it is optional, i.e. that both \"abc\" and \"ac\" are correct matches. This can also be applied to groups and sets. 2.2.2.2.1.5 Anchors: ^ and $ Anchors can be used to specify that a match should only occur at the very beginning or end of a character string, with ^ and $ standing for the beginning and the end, respectively. Note that the ^ operator has a different meaning inside square brackets ([]), as discussed above. vector &lt;- c(&quot;abc123&quot;, &quot;123abc&quot;) str_replace_all(vector, pattern = &quot;^abc&quot;, replacement = &quot;X&quot;) ## [1] &quot;X123&quot; &quot;123abc&quot; This code only replaces the \"abc\" in \"abc123\" because it appears at the beginning of the string. str_replace_all(vector, pattern = &quot;abc$&quot;, replacement = &quot;X&quot;) ## [1] &quot;abc123&quot; &quot;123X&quot; abc$, on the other hand, only matches the \"abc\" in \"123abc\" because it appears at the end of the string. regex can do a lot more than shown here, but these basic tools already enable you to do a lot of things that would take much more time when done by hand. 2.2.3 Case study Let’s take everything we have learned in this chapter and apply it to a practical example. We will be using survey data from Qualtrics, which was created by a group of students for this course in 2017. We will only be looking at a small subsection of the variables available, to keep things from becoming unwieldy. As always, let’s first load all the required libraries and the data set and take a look at it. library(dplyr) library(stringr) data &lt;- read.csv(&quot;https://raw.githubusercontent.com/IMSMWU/MRDA2018/master/Survey_data.csv?token=AVa281hYEyEQbqaSBcQZcYU-da4rv9xkks5bXyUMwA%3D%3D&quot;, stringsAsFactors = FALSE) data This data frame consists of 11 variables and 305 observations. Progress: How much of the survey (in percent) was completed. ResponseId: A unique ID for each participant Q39: A multiple choice question on supermarket recognition. Participants were presented with 8 supermarket brands and asked which of them they were familiar with. A value of 1,2,3,4,5,6,7,8 means that the participant knew all eight, while, e.g., 3,5 means that the person only knew supermarkets 3 and 5. Q18_1 to Q18_1_8:_ A series of questions on willingness to pay for various products. The data has a couple problems we need to take care of before we can start analyzing it properly. The first two rows: Qualtrics data comes with two rows that contain no useful information. Additionally, these force all columns to be of type “character”, which we can’t perform all types of analyses on. Not all respondents finished the survey: As you can tell by the progress column, not all respondents finished the survey. To be able to perform proper analysis, we only want those that completed the survey. Multiple choice question: The multiple choice question is currently in a format that is very hard to work with. It would be best to have eight individual columns that each correspond to an individual supermarket. The price data is a mess: The price data is arguably the hardest challenge. The data is not uniform, with the decimal separator symbol varying and some rows containing additional text and symbols that we do not need. We need to filter out only the relevant parts and then transform it from a character to a numeric column to work on it. As a first step, we will filter out all observations where the progress column is unequal 100. This has the added advantage of removing the first two rows as well. data &lt;- data %&gt;% filter(Progress == 100) Next we want to turn the multiple choice question into something a bit more useful. We will create eight new columns, called Q39_A1 to Q39_A8, which contain a 1 if the respective number appears in Q39 and a 0 otherwise. So, for example, if Q39is equal to 1,2,5 then Q39_A1, Q39_A2 and Q39_A5 will be set to 1 and all others will be set to 0. We will achieve this with mutate and str_detect. data &lt;- data %&gt;% mutate(Q39_A1 = str_detect(Q39, pattern = &quot;1&quot;), Q39_A2 = str_detect(Q39, pattern = &quot;2&quot;), Q39_A3 = str_detect(Q39, pattern = &quot;3&quot;), Q39_A4 = str_detect(Q39, pattern = &quot;4&quot;), Q39_A5 = str_detect(Q39, pattern = &quot;5&quot;), Q39_A6 = str_detect(Q39, pattern = &quot;6&quot;), Q39_A7 = str_detect(Q39, pattern = &quot;7&quot;), Q39_A8 = str_detect(Q39, pattern = &quot;8&quot;)) First, we always define the name of the new column we want to create. Then we tell mutate the function with which to create the new column. In this case we use str_detect() to check if the correct number appears in the column Q39. You can read the first line in mutate() as: “If a 1 appears in the field Q39 set the column Q39_A1 to TRUE and otherwise set it to FALSE”. Now is a good time to take a look at the columns we have created and see if it did what we wanted. data %&gt;% select(Q39, Q39_A1:Q39_A8) Everything seems to have worked! In a later step we will change the values TRUE and FALSE to 1 and 0, respectively, simply because it is easier to work with. The last big task is to clean up the price data. Let’s take a look at it. data %&gt;% select(Q18_1_1:Q18_1_8) The problems can be grouped roughly into two categories: Additional symbols: Some respondents added additional symbols or text that we need to get rid of. Wrong decimal separator: R needs all decimal separators to be . and not ,, so we need to make sure all of them are dots. We will tackle both of these problems with mutate_at() and the stringr package. First we will strip out any character that isn’t a number, a comma or a dot. str_remove_all removes all instances of characters that are matched by the expression given in “pattern”. Note that the function str_remove_all is called without parentheses. Its argument(s) are passed directly to mutate_at(), separated by commas. The pattern we use here is [^0-9,\\\\.], which can be read as “match all characters that are not (remember that a ^ in square brackets means ‘everything but’) 0-9 a comma (,) or a dot (\\\\.)”. Remember that the dot is a special character and therefore has to be escaped with the double backslash. data &lt;- data %&gt;% mutate_at(vars(Q18_1_1:Q18_1_8), str_remove_all, pattern = &quot;[^0-9,\\\\.]&quot;) Let’s take a look and see if the code did what we wanted it to. data %&gt;% select(Q18_1_1:Q18_1_8) So far, so good. To complete the clean up of the price data, we simply want to replace any instances of a comma with a dot. The code for this is quite simple. data &lt;- data %&gt;% mutate_at(vars(Q18_1_1:Q18_1_8), str_replace_all, pattern = &quot;,&quot;, replacement = &quot;.&quot;) # Print variables we just mutated to see if everything worked data %&gt;% select(Q18_1_1:Q18_1_8) As a final step we want to convert all variables that we want to work with into numeric variables. Again, we use mutate_at(), this time coupled with the as.numeric() function. Values that cannot be converted to numbers such as empty strings or strings that contain characters will be set to NA and the warning “NAs introduced by coercion” is shown. This is not a big deal, but you might have to exclude those observations from the dataset depending on your analysis. data &lt;- data %&gt;% mutate_at(vars(Q18_1_1:Q39_A8), as.numeric) Finally, let’s have a look at the finished data frame. All the relevant data is now in a format that we can perform further analysis with. # Print entire data frame data 2.3 Data import and export Before you can start your analysis in R, you first need to import the data you wish to perform the analysis on. You will often be faced with different types of data formats (usually produced by some other statistical software like SPSS or Excel or a text editor). Fortunately, R is fairly flexible with respect to the sources from which data may be imported and you can import the most common data formats into R with the help of a few packages. R can, among others, handle data from the following sources: In the previous chapter, we saw how we may use the keyboard to input data in R. In the following sections, we will learn how to import data from text files and other statistical software packages. 2.3.1 Getting data for this course Most of the data sets we will be working with in this course will be stored in text files (i.e., .dat, .txt, .csv). There are two ways for you to obtain access to the data sets: You can download the corresponding R-Code here 2.3.1.1 Directly import datasets from GitHub (recommended) All data sets we will be working with are stored in a repository on GitHub (similar to other cloud storage services such as Dropbox). If you know the location, where the files are stored, you may conveniently load the data directly from GitHub into R using the read.table() function. The header=TRUE argument indicates that the first line of data represents the header, i.e., it contains the names of the columns. The sep=\"\\t\"-argument specifies the delimiter (the character used to separate the columns), which is a TAB in this case. test_data &lt;- read.table(&quot;https://raw.githubusercontent.com/IMSMWU/Teaching/master/MRDA2017/test_data.dat&quot;, sep = &quot;\\t&quot;, header = TRUE) 2.3.1.2 Download and import datasets from “Learn@WU” It is also possible to download the data from the respective folder on the “Learn@WU” platform, placing it in the working directory and importing it from there. However, this requires an additional step to download the file manually first. If you chose this option, please remember to put the data file in the working directory first. If the import is not working, check your working directory setting using getwd(). Once you placed the file in the working directory, you can import it using the same command as above. Note that the file must be given as a character string (i.e., in quotation marks) and has to end with the file extension (e.g., .csv, .tsv, etc.). music_data &lt;- read.table(&quot;music.data.extension&quot;, header=TRUE) 2.3.2 Import data created by other software packages Sometimes, you may need to import data files created by other software packages, such as Excel or SPSS. In this section we will use the readxl and haven packages to do this. To import a certain file you should first make sure that the file is stored in your current working directory. You can list all file names in your working directory using the list.files() function. If the file is not there, either copy it to your current working directory, or set your working directory to the folder where the file is located using setwd(\"/path/to/file\"). This tells R the folder you are working in. Remember that you have to use / instead of \\ to specify the path (if you use Windows paths copied from the explorer they will not work). When your file is in your working directory you can simply enter the filename into the respective import command. The import commands offer various options. For more details enter ?read_excel, ?read_spss after loading the packages. list.files() #lists all files in the current working directory #setwd(&quot;/path/to/file&quot;) #may be used to change the working directory to the folder that contains the desired file #import excel files library(readxl) #load package to import Excel files excel_sheets(&quot;music_data.xlsx&quot;) music_data_excel &lt;- read_excel(&quot;music_data.xlsx&quot;, sheet = &quot;mrda_2016_survey&quot;) # &quot;sheet=x&quot;&quot; specifies which sheet to import head(music_data_excel) library(haven) #load package to import SPSS files #import SPSS files music_data_spss &lt;- read_sav(&quot;music_data.sav&quot;) head(music_data_spss) The import of other file formats works in a very similar way (e.g., Stata, SAS). Please refer to the respective help-files (e.g., ?read_dta, ?read_sas …) if you wish to import data created by other software packages. 2.3.3 Export data Exporting to different formats is also easy, as you can just replace “read” with “write” in many of the previously discussed functions (e.g. write.table(object, \"file_name\")). This will save the data file to the working directory. To check what the current working directory is you can use getwd(). By default, the write.table(object, \"file_name\")function includes the row number as the first variable. By specifying row.names = FALSE, you may exclude this variable since it doesn’t contain any useful information. write.table(music_data, &quot;musicData.dat&quot;, row.names = FALSE, sep = &quot;\\t&quot;) #writes to a tab-delimited text file write.table(music_data, &quot;musicData.csv&quot;, row.names = FALSE, sep = &quot;,&quot;) #writes to a comma-separated value file write_sav(music_data, &quot;my_file.sav&quot;) 2.3.4 Import data from the Web 2.3.4.1 Scraping data from websites Sometimes you may come across interesting data on websites that you would like to analyse. Reading data from websites is possible in R, e.g., using the rvest package. Let’s assume you would like to read a table that lists the population of different countries from this Wikipedia page. It helps to first inspect the structure of the website (e.g., using tools like SelectorGadget), so you know which elements you would like to extract. In this case it is fairly obvious that the data are stored in a table for which the associated html-tag is &lt;table&gt;. So let’s read the entire website using read_html(url) and filter all tables using read_html(html_nodes(...,\"table\")). library(rvest) url &lt;- &quot;https://en.wikipedia.org/wiki/List_of_countries_and_dependencies_by_population&quot; population &lt;- read_html(url) population &lt;- html_nodes(population, &quot;table.wikitable&quot;) print(population) ## {xml_nodeset (1)} ## [1] &lt;table class=&quot;wikitable sortable&quot; style=&quot;text-align:right&quot;&gt;&lt;tbody&gt;\\n&lt;tr&gt;\\ ... The output shows that there are two tables on the website and the first one appears to contain the relevant information. So let’s read the first table using the html_table() function. Note that population is of class “list”. A list is a vector that has other R objects (e.g., other vectors, data frames, matrices, etc.) as its elements. If we want to access the data of one of the elements, we have to use two square brackets on each side instead of just one (e.g., population[[1]] gets us the first table from the list of tables on the website; the argument fill = TRUE ensures that empty cells are replaced with missing values when reading the table). population &lt;- population[[1]] %&gt;% html_table(fill = TRUE) head(population) #checks if we scraped the desired data You can see that population is read as a character variable because of the commas. class(population$Population) ## [1] &quot;character&quot; If we wanted to use this variable for some kind of analysis, we would first need to convert it to numeric format using the as.numeric() function. However, before we can do this, we can use the str_replace_all() function from the stringr package, which replaces all matches of a string. In our case, we would like to replace the commas (\",\") with nothing (\"\"). library(stringr) population$Population &lt;- as.numeric(str_replace_all(population$Population, pattern = &quot;,&quot;, replacement = &quot;&quot;)) #convert to numeric head(population) #checks if we scraped the desired data Now the variable is of type “numeric” and could be used for analysis. class(population$Population) ## [1] &quot;numeric&quot; 2.3.4.2 Scraping data from APIs 2.3.4.2.1 Scraping data from APIs directly Reading data from websites can be tricky since you need to analyze the page structure first. Many web-services (e.g., Facebook, Twitter, YouTube) actually have application programming interfaces (API’s), which you can use to obtain data in a pre-structured format. JSON (JavaScript Object Notation) is a popular lightweight data-interchange format in which data can be obtained. The process of obtaining data is visualized in the following graphic: Obtaining data from APIs The process of obtaining data from APIs consists of the following steps: Identify an API that has enough data to be relevant and reliable (e.g., www.programmableweb.com has &gt;12,000 open web APIs in 63 categories). Request information by calling (or, more technically speaking, creating a request to) the API (e.g., R, python, php or JavaScript). Receive response messages, which is usually in JavaScript Object Notation (JSON) or Extensible Markup Language (XML) format. Write a parser to pull out the elements you want and put them into a of simpler format Store, process or analyze data according the marketing research question. Let’s assume that you would like to obtain population data again. The World Bank has an API that allows you to easily obtain this kind of data. The details are usually provided in the API reference, e.g., here. You simply “call” the API for the desired information and get a structured JSON file with the desired key-value pairs in return. For example, the population for Austria from 1960 to 2016 can be obtained using this call. The file can be easily read into R using the fromJSON()-function from the jsonlite-package. Again, the result is a list and the second element ctrydata[[2]] contains the desired data, from which we select the “value” and “data” columns using the square brackets as usual [,c(\"value\",\"date\")] library(jsonlite) url &lt;- &quot;http://api.worldbank.org/v2/countries/AT/indicators/SP.POP.TOTL/?date=1960:2016&amp;format=json&amp;per_page=100&quot; #specifies url ctrydata &lt;- fromJSON(url) #parses the data str(ctrydata) ## List of 2 ## $ :List of 6 ## ..$ page : int 1 ## ..$ pages : int 1 ## ..$ per_page : int 100 ## ..$ total : int 57 ## ..$ sourceid : chr &quot;2&quot; ## ..$ lastupdated: chr &quot;2020-09-08&quot; ## $ :&#39;data.frame&#39;: 57 obs. of 8 variables: ## ..$ indicator :&#39;data.frame&#39;: 57 obs. of 2 variables: ## .. ..$ id : chr [1:57] &quot;SP.POP.TOTL&quot; &quot;SP.POP.TOTL&quot; &quot;SP.POP.TOTL&quot; &quot;SP.POP.TOTL&quot; ... ## .. ..$ value: chr [1:57] &quot;Population, total&quot; &quot;Population, total&quot; &quot;Population, total&quot; &quot;Population, total&quot; ... ## ..$ country :&#39;data.frame&#39;: 57 obs. of 2 variables: ## .. ..$ id : chr [1:57] &quot;AT&quot; &quot;AT&quot; &quot;AT&quot; &quot;AT&quot; ... ## .. ..$ value: chr [1:57] &quot;Austria&quot; &quot;Austria&quot; &quot;Austria&quot; &quot;Austria&quot; ... ## ..$ countryiso3code: chr [1:57] &quot;AUT&quot; &quot;AUT&quot; &quot;AUT&quot; &quot;AUT&quot; ... ## ..$ date : chr [1:57] &quot;2016&quot; &quot;2015&quot; &quot;2014&quot; &quot;2013&quot; ... ## ..$ value : int [1:57] 8736668 8642699 8546356 8479823 8429991 8391643 8363404 8343323 8321496 8295487 ... ## ..$ unit : chr [1:57] &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; ... ## ..$ obs_status : chr [1:57] &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; ... ## ..$ decimal : int [1:57] 0 0 0 0 0 0 0 0 0 0 ... head(ctrydata[[2]][,c(&quot;value&quot;,&quot;date&quot;)]) #checks if we scraped the desired data 2.3.4.2.2 Scraping data from APIs via R packages An even more convenient way to obtain data from web APIs is to use existing R packages that someone else has already created. There are R packages available for various web-services. For example, the gtrendsR package can be used to conveniently obtain data from the Google Trends page. The gtrends() function is easy to use and returns a list of elements (e.g., “interest over time”, “interest by city”, “related topics”), which can be inspected using the ls() function. The following example can be used to obtain data for the search term “data science” in the US between September 1 and October 6: library(gtrendsR) index = 1 success = FALSE while(!(success | index == 10)){ google_trends &lt;- try(gtrends(&quot;data science&quot;, geo = c(&quot;US&quot;), gprop = c(&quot;web&quot;), time = &quot;2017-09-01 2017-10-06&quot;), silent = TRUE) if(!is(google_trends, &quot;try-error&quot;)){ ls(google_trends) head(google_trends$interest_over_time) success = TRUE }else{ index = index + 1 Sys.sleep(runif(1,0,3))} } if(success == FALSE){ warning(&quot;Google Trends has exited unsuccessfully&quot;) } ## Warning: Google Trends has exited unsuccessfully "],
["summarizing-data.html", "3 Summarizing data 3.1 Summary statistics 3.2 Data visualization 3.3 Writing reports using R-Markdown", " 3 Summarizing data 3.1 Summary statistics This section discusses how to produce and analyse basic summary statistics. We make a distinction between categorical and continuous variables, for which different statistics are permissible. You can download the corresponding R-Code here OK to compute…. Nominal Ordinal Interval Ratio frequency distribution Yes Yes Yes Yes median and percentiles No Yes Yes Yes mean, standard deviation, standard error of the mean No No Yes Yes ratio, or coefficient of variation No No No Yes As an example data set, we will be using the Spotify music data. Let’s load and inspect the data first. Students of the course can get the data via Learn@WU. If you are not enrolled in the course please contact Daniel Winkler (https://www.wu.ac.at/en/imsm/about-us/team/daniel-winkler). readRDS(&quot;music_data.rds&quot;) 3.1.1 Categorical variables Categorical variables contain a finite number of categories or distinct groups and are also known as qualitative variables. There are different types of categorical variables: Nominal variables: variables that have two or more categories but no logical order (e.g., music genres). A dichotomous variables is simply a nominal variable that only has two categories (e.g., gender). Ordinal variables: variables that have two or more categories that can also be ordered or ranked (e.g., income groups). For this example, we are interested in the following two variables “genre”: the music genre the song is associated with, subsetted for the most frequent genres. “explicit”: whether the lyrics of the tracks are explicit or not (0 = not explicit, 1 = explicit) You can find a full description of the variables here: In a first step, we convert the variables to factor variables using the factor() function to assign appropriate labels according to the scale points: s.genre &lt;- c(&quot;pop&quot;,&quot;hip hop&quot;,&quot;rock&quot;,&quot;rap&quot;,&quot;indie&quot;) music_data &lt;- subset(music_data, top.genre %in% s.genre) music_data$genre_cat &lt;- as.factor(music_data$top.genre) music_data$explicit_cat &lt;- factor(music_data$explicit, levels = c(0:1), labels = c(&quot;not explicit&quot;, &quot;explicit&quot;)) music_data$adv_spending &lt;- music_data$adv_spending / 10^111 # e^111 = scaling factor for specific outliers music_data$adv_spending2 &lt;- music_data$adv_spending The table() function creates a frequency table. Let’s start with the number of occurrences of the categories associated with the genre and explicitness variables separately: table(music_data[,c(&quot;genre_cat&quot;)]) #absolute frequencies ## ## hip hop indie pop rap rock ## 9907 528 28448 1180 1839 table(music_data[,c(&quot;explicit_cat&quot;)]) #absolute frequencies ## ## not explicit explicit ## 30204 11698 It is obvious that there are more tracks with non-explicit lyrics than songs with explicit lyrics. For variables with more categories, it might be less obvious and we might use the summary() function, which produces further statistics. summary(music_data$explicit) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.0000 0.0000 0.0000 0.2792 1.0000 1.0000 Often, we are interested in the relative frequencies, which can be obtained by using the prop.table() function. prop.table(table(music_data[,c(&quot;genre_cat&quot;)])) #relative frequencies ## ## hip hop indie pop rap rock ## 0.23643263 0.01260083 0.67891747 0.02816095 0.04388812 prop.table(table(music_data[,c(&quot;explicit_cat&quot;)])) #relative frequencies ## ## not explicit explicit ## 0.7208248 0.2791752 Now let’s investigate if the genre differs by expliciteness. To do this, we simply apply the table() function to both variables: table(music_data[,c(&quot;genre_cat&quot;, &quot;explicit_cat&quot;)]) #absolute frequencies ## explicit_cat ## genre_cat not explicit explicit ## hip hop 3515 6392 ## indie 495 33 ## pop 23881 4567 ## rap 536 644 ## rock 1777 62 Again, it might be more meaningful to look at the relative frequencies using prop.table(): prop.table(table(music_data[,c(&quot;genre_cat&quot;, &quot;explicit_cat&quot;)])) #relative frequencies ## explicit_cat ## genre_cat not explicit explicit ## hip hop 0.0838862107 0.1525464178 ## indie 0.0118132786 0.0007875519 ## pop 0.5699250632 0.1089924109 ## rap 0.0127917522 0.0153691948 ## rock 0.0424084769 0.0014796430 Note that the above output shows the overall relative frequencies when explicit and non-explicit songs are considered together. In this context, it might be even more meaningful to look at the conditional relative frequencies. This can be achieved by adding a ,2 to the prop.table() command, which tells R to compute the relative frequencies by the columns (which is in our case the explicitness variable): prop.table(table(music_data[,c(&quot;genre_cat&quot;, &quot;explicit_cat&quot;)]),2) #conditional relative frequencies ## explicit_cat ## genre_cat not explicit explicit ## hip hop 0.116375315 0.546418191 ## indie 0.016388558 0.002820995 ## pop 0.790656867 0.390408617 ## rap 0.017745994 0.055052146 ## rock 0.058833267 0.005300051 3.1.2 Continuous variables 3.1.2.1 Descriptive statistics Continuous variables are numeric variables that can take on any value on a measurement scale (i.e., there is an infinite number of values between any two values). There are different types of continuous variables: Interval variables: while the zero point is arbitrary, equal intervals on the scale represent equal differences in the property being measured. E.g., on a temperature scale measured in Celsius the difference between a temperature of 15 degrees and 25 degrees is the same difference as between 25 degrees and 35 degrees but the zero point is arbitrary. Ratio variables: has all the properties of an interval variable, but also has an absolute zero point. When the variable equals 0.0, it means that there is none of that variable (e.g., number of products sold, willingness-to-pay, mileage a car gets). Computing descriptive statistics in R is easy and there are many functions from different packages that let you calculate summary statistics (including the summary() function from the base package). In this tutorial, we will use the describe() function from the psych package: library(psych) psych::describe(music_data[,c(&quot;trackPopularity&quot;, &quot;duration_ms&quot;)]) ## vars n mean sd median trimmed mad min ## trackPopularity 1 41902 42.19 21.93 46.0 43.32 20.76 0 ## duration_ms 2 41892 215476.95 59067.55 209990.5 212072.35 39194.75 1000 ## max range skew kurtosis se ## trackPopularity 100 100 -0.46 -0.57 0.11 ## duration_ms 3924548 3923548 14.95 755.65 288.59 In the above command, we used the psych:: prefix to avoid confusion and to make sure that R uses the describe() function from the psych package since there are many other packages that also contain a desribe() function. Note that you could also compute these statistics separately by using the respective functions (e.g., mean(), sd(), median(), min(), max(), etc.). The psych package also contains the describeBy() function, which lets you compute the summary statistics by sub-group separately. For example, we could easily compute the summary statistics by expliciteness as follows: describeBy(music_data[,c(&quot;trackPopularity&quot;, &quot;duration_ms&quot;)], music_data$explicit_cat) ## ## Descriptive statistics by group ## group: not explicit ## vars n mean sd median trimmed mad min ## trackPopularity 1 30204 41.08 21.48 44 42.02 20.76 0 ## duration_ms 2 30195 219629.07 58949.53 213567 216310.72 38765.54 1000 ## max range skew kurtosis se ## trackPopularity 100 100 -0.39 -0.61 0.12 ## duration_ms 3924548 3923548 18.08 986.56 339.24 ## ------------------------------------------------------------ ## group: explicit ## vars n mean sd median trimmed mad min ## trackPopularity 1 11698 45.07 22.81 50 46.71 19.27 0 ## duration_ms 2 11697 204758.53 58015.74 198987 200790.11 38528.33 33422 ## max range skew kurtosis se ## trackPopularity 98 98 -0.67 -0.38 0.21 ## duration_ms 2147004 2113582 7.54 184.82 536.42 Note that you could just as well use other packages to compute the descriptive statistics. For example, you could have used the stat.desc() function from the pastecs package: library(pastecs) stat.desc(music_data[,c(&quot;trackPopularity&quot;, &quot;duration_ms&quot;)]) ## trackPopularity duration_ms ## nbr.val 41902.0000000 41892.0000000 ## nbr.null 2718.0000000 0.0000000 ## nbr.na 0.0000000 10.0000000 ## min 0.0000000 1000.0000000 ## max 100.0000000 3924548.0000000 ## range 100.0000000 3923548.0000000 ## sum 1767874.0000000 9026760380.0000000 ## median 46.0000000 209990.5000000 ## mean 42.1906830 215476.9497756 ## SE.mean 0.1071435 288.5914072 ## CI.mean.0.95 0.2100034 565.6451076 ## var 481.0231842 3488975232.0832715 ## std.dev 21.9322407 59067.5480453 ## coef.var 0.5198361 0.2741247 Computing statistics by group is also possible by using the wrapper function by(). Within the function, you first specify the data on which you would like to perform the grouping music_data[,c(\"trackPopularity\", \"adv_spending\")], followed by the grouping variable music_data$explicit_cat and the function that you would like to execute (e.g., stat.desc()): library(pastecs) by(music_data[,c(&quot;trackPopularity&quot;, &quot;duration_ms&quot;)],music_data$explicit_cat,stat.desc) ## music_data$explicit_cat: not explicit ## trackPopularity duration_ms ## nbr.val 30204.0000000 30195.000000 ## nbr.null 1777.0000000 0.000000 ## nbr.na 0.0000000 9.000000 ## min 0.0000000 1000.000000 ## max 100.0000000 3924548.000000 ## range 100.0000000 3923548.000000 ## sum 1240696.0000000 6631699851.000000 ## median 44.0000000 213567.000000 ## mean 41.0772083 219629.072727 ## SE.mean 0.1235926 339.244534 ## CI.mean.0.95 0.2422467 664.933723 ## var 461.3697298 3475047544.433871 ## std.dev 21.4795188 58949.533878 ## coef.var 0.5229060 0.268405 ## ------------------------------------------------------------ ## music_data$explicit_cat: explicit ## trackPopularity duration_ms ## nbr.val 11698.0000000 11697.0000000 ## nbr.null 941.0000000 0.0000000 ## nbr.na 0.0000000 1.0000000 ## min 0.0000000 33422.0000000 ## max 98.0000000 2147004.0000000 ## range 98.0000000 2113582.0000000 ## sum 527178.0000000 2395060529.0000000 ## median 50.0000000 198987.0000000 ## mean 45.0656522 204758.5303069 ## SE.mean 0.2109063 536.4245115 ## CI.mean.0.95 0.4134115 1051.4815357 ## var 520.3441548 3365826447.7986922 ## std.dev 22.8110533 58015.7431030 ## coef.var 0.5061738 0.2833374 These examples are meant to exemplify that there are often many different ways to reach a goal in R. Which one you choose depends on what type of information you seek (the results provide slightly different information) and on personal preferences. 3.1.2.2 Creating subsets From the above statistics it is clear that the data set contains some severe outliers on some variables. For example, the maximum amount of spending on advertisment is 5656214468105730048 units. You might want to investigate these cases and delete them if they would turn out to indeed induce a bias in your analyses. For normally distributed data, any absolute standardized deviations larger than 3 standard deviations from the mean are suspicious. Let’s check if potential outliers exist in the data: library(dplyr) music_data %&gt;% mutate(adv_spending_std = as.vector(scale(adv_spending2))) %&gt;% filter(abs(adv_spending_std) &gt; 3) %&gt;% select(id, trackName, adv_spending, adv_spending_std) ## # A tibble: 2 x 4 ## id trackName adv_spending adv_spending_std ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0bAkKNCQfWkexHFn7fIKns In My Feelings 5.66e18 62.6 ## 2 14msK75pk3pA33pzPVNtBF 7 rings 1.01e18 11.1 Indeed, there appear to be two potential outliers, which we may wish to exclude before we start fitting models to the data. You could easily create a subset of the original data, which you would then use for estimation using the filter() function from the dplyr() package. For example, the following code creates a subset that excludes all cases with a standardized duration of more than 3: library(dplyr) estimation_sample &lt;- music_data %&gt;% mutate(adv_spending_std = as.vector(scale(adv_spending))) %&gt;% filter(abs(adv_spending_std) &lt; 3) psych::describe(estimation_sample[,c(&quot;trackPopularity&quot;, &quot;adv_spending&quot;)]) ## vars n mean sd median trimmed ## trackPopularity 1 4038 56.1 25.75 65.00 59.04 ## adv_spending 2 4038 14299852780120.5 514033938921804.06 0.01 14.09 ## mad min max range skew kurtosis ## trackPopularity 16.31 0 100 100 -0.98 -0.27 ## adv_spending 0.01 0 23007897832533400 23007897832533400 41.45 1773.85 ## se ## trackPopularity 0.41 ## adv_spending 8089257063071.86 3.2 Data visualization This section discusses how to produce appropriate graphics to describe our data visually. While R includes tools to build plots, we will be using the ggplot2 package by Hadley Wickham. It has the advantage of being fairly straightforward to learn but being very flexible when it comes to building more complex plots. For a more in depth discussion you can refer to chapter 4 of the book “Discovering Statistics Using R” by Andy Field et al. or read the following chapter from the book “R for Data science” by Hadley Wickham as well as “R Graphics Cookbook” by Winston Chang. You can download the corresponding R-Code here ggplot2 is built around the idea of constructing plots by stacking layers on top of one another. Every plot starts with the ggplot(data) function, after which layers can be added with the “+” symbol. The following figures show the layered structure of creating plots with ggplot.     3.2.1 Categorical variables 3.2.1.1 Bar plot To give you an example of how the graphics are composed, let’s go back to the frequency table from the previous chapter, where we created this table: readRDS(&quot;music_data.rds&quot;) s.genre &lt;- c(&quot;pop&quot;,&quot;hip hop&quot;,&quot;rock&quot;,&quot;rap&quot;,&quot;indie&quot;) music_data &lt;- subset(music_data, top.genre %in% s.genre) music_data$genre_cat &lt;- as.factor(music_data$top.genre) music_data$explicit_cat &lt;- factor(music_data$explicit, levels = c(0:1), labels = c(&quot;not explicit&quot;, &quot;explicit&quot;)) head(music_data) How can we plot this kind of data? Since we have a categorical variable, we will use a bar plot. However, to be able to use the table for your plot, you first need to assign it to an object as a data frame using the as.data.frame()-function. table_plot_rel &lt;- as.data.frame(prop.table(table(music_data[,c(&quot;genre_cat&quot;)]))) #relative frequencies #relative frequencies head(table_plot_rel) Since Var1 is not a very descriptive name, let’s rename the variable to something more meaningful library(plyr) table_plot_rel &lt;- plyr::rename(table_plot_rel, c(Var1=&quot;Genre&quot;)) head(table_plot_rel) Once we have our data set we can begin constructing the plot. As mentioned previously, we start with the ggplot() function, with the argument specifying the data set to be used. Within the function, we further specify the scales to be used using the aesthetics argument, specifying which variable should be plotted on which axis. In our example, we would like to plot the categories on the x-axis (horizontal axis) and the relative frequencies on the y-axis (vertical axis). library(ggplot2) bar_chart &lt;- ggplot(table_plot_rel, aes(x = Genre,y = Freq)) bar_chart Figure 3.1: Bar chart (step 1) You can see that the coordinate system is empty. This is because so far, we have told R merely which variables we would like to plot but we haven’t specified which geometric figures (points, bars, lines, etc.) we would like to use. This is done using the geom_xxx function. ggplot includes many different geoms, for a wide range of plots (e.g., geom_line, geom_histogram, geom_boxplot, etc.). A good overview of the various geom functions can be found here. In our case, we would like to use a bar chart for which geom_col is appropriate. bar_chart + geom_col() Figure 3.2: Bar chart (step 2) Note that the same could be achieved using geom_bar. However, by default geom_bar counts the number of observations within each category of a variable. This is not required in our case because we have already used the prop.table() function to compute the relative frequencies. The argument stat = \"identity\" prevents geom_bar from performing counting operations and uses it “as it is”. bar_chart + geom_bar(stat = &quot;identity&quot;) Figure 3.3: Bar chart (alternative specification) Now we have specified the data, the scales and the shape. Specifying this information is essential for plotting data using ggplot. Everything that follows now just serves the purpose of making the plot look nicer by modifying the appearance of the plot. How about some more meaningful axis labels? We can specify the axis labels using the ylab() and xlab() functions: bar_chart + geom_col() + ylab(&quot;Relative frequency&quot;) + xlab(&quot;Genre&quot;) Figure 3.4: Bar chart (step 3) How about adding some value labels to the bars? This can be done using geom_text(). Note that the sprintf() function is not mandatory and is only added to format the numeric labels here. The function takes two arguments: the first specifies the format wrapped in two % sings. Thus, %.0f means to format the value as a fixed point value with no digits after the decimal point, and %% is a literal that prints a “%” sign. The second argument is simply the numeric value to be used. In this case, the relative frequencies multiplied by 100 to obtain the percentage values. Using the vjust = argument, we can adjust the vertical alignment of the label. In this case, we would like to display the label slightly above the bars. bar_chart + geom_col() + ylab(&quot;Relative frequency&quot;) + xlab(&quot;Genre&quot;) + geom_text(aes(label = sprintf(&quot;%.0f%%&quot;, Freq/sum(Freq) * 100)), vjust=-0.2) Figure 3.5: Bar chart (step 4) We could go ahead and specify the appearance of every single element of the plot now. However, there are also pre-specified themes that include various formatting steps in one singe function. For example theme_bw() would make the plot appear like this: bar_chart + geom_col() + ylab(&quot;Relative frequency&quot;) + xlab(&quot;Genre&quot;) + geom_text(aes(label = sprintf(&quot;%.0f%%&quot;, Freq/sum(Freq) * 100)), vjust=-0.2) + theme_bw() Figure 3.6: Bar chart (step 5) and theme_minimal() looks like this: bar_chart + geom_col() + ylab(&quot;Relative frequency&quot;) + xlab(&quot;Genre&quot;) + geom_text(aes(label = sprintf(&quot;%.0f%%&quot;, Freq/sum(Freq) * 100)), vjust=-0.2) + theme_minimal() Figure 3.7: Bar chart (options 1) These were examples of built-in formations of ggolot(), where the default is theme_classic(). For even more options, check out the ggthemes package, which includes formats for specific publications. You can check out the different themes here. For example theme_economist() uses the formatting of the journal “The Economist”: library(ggthemes) bar_chart + geom_col() + ylab(&quot;Relative frequency&quot;) + xlab(&quot;Genre&quot;) + theme_economist() Figure 3.8: Bar chart (options 2) Summary To create a plot with ggplot we give it the appropriate data (in the ggplot() function), tell it which shape to use (via a function of the geom family), assign variables to the correct axis (by using the the aes() function) and define the appearance of the plot. Now we would like to investigate whether the distribution differs between explicit and non-explicit songs. For this purpose we first construct the conditional relative frequency table from the previous chapter again. Recall that the latter gives us the relative frequency within a group (in our case explicit and non-explicit), as compared to the relative frequency within the entire sample. table_plot_cond_rel &lt;- as.data.frame(prop.table(table(music_data[,c(&quot;genre_cat&quot;, &quot;explicit_cat&quot;)]),2)) #conditional relative frequencies We can now take these tables to construct plots grouped by explicitness. To achieve this we simply need to add the facet_wrap() function, which replicates a plot multiple times, split by a specified grouping factor. Note that the grouping factor has to be supplied in R’s formula notation, hence it is preceded by a “~” symbol. ggplot(table_plot_cond_rel, aes(x = genre_cat, y = Freq)) + geom_col() + facet_wrap(~ explicit_cat) + ylab(&quot;Conditional relative frequency&quot;) + xlab(&quot;Genre&quot;) + theme_bw() Figure 3.9: Grouped bar chart (conditional relative frequencies) To plot the relative frequencies for each response category by group in a slightly different way, we can also use the fill argument, which tells ggplot to fill the bars by a specified variable (in our case “explicit”). The position = \"dodge\" argument causes the bars to be displayed next to each other (as opposed to stacked on top of one another). ggplot(table_plot_cond_rel, aes(x = genre_cat, y = Freq, fill = explicit_cat)) + #use &quot;fill&quot; argument for different colors geom_col(position = &quot;dodge&quot;) + #use &quot;dodge&quot; to display bars next to each other (instead of stacked on top) geom_text(aes(label = sprintf(&quot;%.0f%%&quot;, Freq/sum(Freq) * 100)),position=position_dodge(width=0.9), vjust=-0.25) + ylab(&quot;Conditional relative frequency&quot;) + xlab(&quot;Genre&quot;) + theme_bw() Figure 3.10: Grouped bar chart (conditional relative frequencies) (2) 3.2.1.2 Pie plot We could also create a pie plot with ggplot. ggplot(subset(table_plot_rel,Freq &gt; 0), aes(x=&quot;&quot;, y=Freq, fill=Genre)) + # Create a basic bar geom_bar(stat=&quot;identity&quot;, width=1) + coord_polar(&quot;y&quot;, start=0) + #Convert to pie (polar coordinates) geom_text(aes(label = paste0(round(Freq*100), &quot;%&quot;)), position = position_stack(vjust = 0.5)) + #add labels #scale_fill_manual(values=c(&quot;#55DDE0&quot;, &quot;#33658A&quot;, &quot;#2F4858&quot;, &quot;#F6AE2D&quot;)) # Add color scale (hex colors) scale_fill_brewer(palette = &quot;Blues&quot;) + labs(x = NULL, y = NULL, fill = NULL, title = &quot;Spotify tracks by Genre&quot;) + #remove labels and add title theme_classic() + theme(axis.line = element_blank(), # Tidy up the theme axis.text = element_blank(), axis.ticks = element_blank(), plot.title = element_text(hjust = 0.5, color = &quot;#666666&quot;)) Alternatively we could use ggstatplot, which has a smiliar syntax: ggstatsplot::ggpiestats( data = music_data, # use raw data because we need disaggregated factors for x x = genre_cat, title = &quot;Spotify tracks by Genre&quot;, messages = FALSE, bf.message = FALSE, legend.title = element_blank(), #palette = &quot;Blues&quot;, results.subtitle = FALSE ) 3.2.1.3 Covariation plots To visualize the covariation between categorical variables, you’ll need to count the number of observations for each combination stored in the frequency table. Say, we wanted to investigate the association between genre and popularity. First, we need to make sure that the respective variables are coded as factors. music_data$genre_cat &lt;- as.factor(music_data$top.genre) music_data$popularity_factor &lt;- cut(music_data$trackPopularity, breaks=c(-Inf, 40, 60, Inf), labels=c(&quot;low&quot;,&quot;middle&quot;,&quot;high&quot;)) There are multiple ways to visualize such a relationship with ggplot. One option would be to use a variation of the scatterplot which counts how many points overlap at any given point and increases the dot size accordingly. This can be achieved with geom_count(). From the bar charts above, we know that the categories in genre differ in size. To account for that we set the parameters size and group in geom_count, which gives us the conditional relative frequencies. This is equivalent to the conditional relative frequency table from above, only now the stat(prop) argument assures that we get relative frequencies and with the group argument we tell R to compute the relative frequencies by genre. ggplot(data = music_data) + geom_count(aes(x = genre_cat, y = popularity_factor, size = stat(prop), group = genre_cat)) + ylab(&quot;Popularity&quot;) + xlab(&quot;Genre&quot;) + labs(size = &quot;Proportion&quot;) + theme_bw() Figure 3.11: Covariation between categorical data (1) Another option would be to use a tile plot that changes the color of the tile based on the frequency of the combination of factors. To achieve this we first have to create a dataframe that contains the relative frequencies of all combinations of factors. Then we can take this dataframe and supply it to geom_tile(), while specifying that the fill of each tile should be dependent on the observed frequency of the factor combination, which is done by specifying the fill in the aes() function. table_plot_rel &lt;- prop.table(table(music_data[,c(&quot;genre_cat&quot;, &quot;popularity_factor&quot;)]),1) table_plot_rel &lt;- as.data.frame(table_plot_rel) ggplot(table_plot_rel, aes(x = genre_cat, y = popularity_factor)) + geom_tile(aes(fill = Freq)) + ylab(&quot;Popularity&quot;) + xlab(&quot;Genre&quot;) + theme_bw() Figure 3.12: Covariation between categorical data (2) 3.2.2 Continuous variables 3.2.2.1 Histogram Histograms can be plotted for continuous data using the geom_histogram() function. Note that the aes() function only needs one argument here, since a histogram is a plot of the distribution of only one variable. As an example, let’s consider our data set containing the advertising expenditures and product sales of a company selling products in two different stores: head(music_data) Now we can create the histogram using geom_histogram(). The argument binwidth specifies the range that each bar spans, col = \"black\" specifies the border to be black and fill = \"darkblue\" sets the inner color of the bars to dark blue. For brevity, we have now also started naming the x and y axis with the single function labs(), instead of using the two distinct functions xlab() and ylab(). ggplot(music_data,aes(mstreams)) + geom_histogram(binwidth = 3000, col = &quot;black&quot;, fill = &quot;darkblue&quot;) + labs(x = &quot;Number of streams&quot;, y = &quot;Frequency&quot;) + theme_bw() Figure 3.13: Histogram Alternatively we could use ggstatplot, which has a smiliar syntax: gghistostats( data = music_data, # dataframe from which variable is to be taken x = mstreams, # numeric variable whose distribution is of interest title = &quot;Distribution of the number of streams&quot;, # title for the plot caption = substitute(paste(italic(&quot;Source:&quot;), &quot;Spotify&quot;)), centrality.para = &quot;mean&quot;, # which measure of central tendency is to be plotted centrality.color = &quot;darkred&quot;, # decides color for central tendency line binwidth = 3000, # binwidth value messages = FALSE, # turn off the messages ggtheme = theme_bw(), # choosing a different theme ggstatsplot.layer = FALSE, # turn off ggstatsplot theme layer bf.message = FALSE, results.subtitle = FALSE ) 3.2.2.2 Boxplot Another common way to display the distribution of continuous variables is through boxplots. ggplot will construct a boxplot if given the geom geom_boxplot(). In our case we want to show the difference in distribution between the two stores in our sample, which is why the aes() function contains both an x and a y variable. ggplot(music_data,aes(x = explicit_cat, y = mstreams)) + geom_boxplot(coef = 3) + labs(x = &quot;Explicit&quot;, y = &quot;Number of streams&quot;) + theme_bw() Figure 3.14: Boxplot by group The following graphic shows you how to interpret the boxplot: Information contained in a Boxplot You may also augment the boxplot with the data points using geom_jitter(): ggplot(music_data,aes(x = explicit_cat, y = mstreams)) + geom_boxplot(coef = 3) + geom_jitter(colour=&quot;red&quot;, alpha = 0.2) + labs(x = &quot;Explicit&quot;, y = &quot;Number of streams&quot;) + theme_bw() Figure 3.15: Boxplot with augmented data points In case you would like to create the boxplot on the total data (i.e., not by group), just leave the x = argument within the aes() function empty: ggplot(music_data,aes(x = &quot;&quot;, y = mstreams)) + geom_boxplot(coef = 3) + labs(x = &quot;Total&quot;, y = &quot;Number of streams&quot;) + theme_bw() Figure 3.16: Single Boxplot Alternatively we could use ggstatplot, which has a smiliar syntax: ggbetweenstats( data = music_data, title = &quot;Number of streams by explicitness&quot;, # title for the plot plot.type = &quot;box&quot;, x = explicit_cat, # 2 groups y = mstreams, type = &quot;p&quot;, # default messages = FALSE, bf.message = FALSE ) 3.2.2.3 Plot of means Another quick way to get an overview of the difference between two groups is to plot their respective means with confidence intervals. Two things about this plot are new. First, there are now two geoms included in the same plot. This is one of the big advantages of ggplot’s layered approach to graphs, the fact that new elements can be drawn by simply adding a new line with a new geom function. In this case we want to add confidence bounds to our plot, which we achieve by adding a geom_pointrange() layer. Recall that if the interval is small, the sample must be very close to the population and when the interval is wide, the sample mean is likely very different from the population mean and therefore a bad representation of the population. Second, we are using an additional argument in geom_bar(), namely stat =, which is short for statistical transformation. Every geom uses such a transformation in the background to adapt the data to be able to create the desired plot. geom_bar() typically uses the count stat, which would create a similar plot to the one we saw at the very beginning, counting how often a certain value of a variable appears. By telling geom_bar() explicitly that we want to use a different stat we can override its behavior, forcing it to create a bar plot of the means. ggplot(music_data,aes(explicit_cat, duration_ms)) + geom_bar(stat = &quot;summary&quot;, color = &quot;black&quot;, fill = &quot;white&quot;, width = 0.7, na.rm=T) + geom_pointrange(stat = &quot;summary&quot;, fun.ymin = function(x)mean(x) - sd(x), fun.ymax = function(x)mean(x) + sd(x), fun.y= mean, na.rm = T) + labs(x = &quot;Explicit&quot;, y = &quot;Average number of streams&quot;)+ theme_bw() Figure 3.17: Plot of means 3.2.2.4 Scatter plot The most common way to show the relationship between two continuous variables is a scatterplot. The following code creates a scatterplot with some additional components. The geom_smooth() function creates a smoothed line from the data provided. In this particular example we tell the function to draw the best possible straight line (i.e., minimizing the distance between the line and the points) through the data (via the argument method = \"lm\"). The “fill” and “alpha” arguments solely affect appearance, in our case the color and the opacity of the confidence interval, respectively. ggplot(music_data, aes(log(adv_spending), mstreams)) + geom_point() + geom_smooth(method = &quot;lm&quot;, fill = &quot;blue&quot;, alpha = 0.1) + labs(x = &quot;Advertising expenditures (EUR)&quot;, y = &quot;Number of streams&quot;) + theme_bw() Figure 3.18: Scatter plot As you can see, there appears to be a positive relationship between advertising and sales. 3.2.2.4.1 Grouped scatter plot It could be that customers from different store respond differently to advertising. We can visually capture such differences with a grouped scatter plot. By adding the argument colour = store to the aesthetic specification, ggplot automatically treats the two stores as distinct groups and plots accordingly. ggplot(music_data, aes(log(adv_spending), mstreams, colour = explicit_cat)) + geom_point() + geom_smooth(method=&quot;lm&quot;, alpha = 0.1) + labs(x = &quot;Advertising expenditures (EUR)&quot;, y = &quot;Number of streams&quot;, colour=&quot;Explicit&quot;) + theme_bw() Figure 3.19: Grouped scatter plot It appears from the plot that explicit tracks are more responsive to advertising. 3.2.2.4.2 Combination of scatter plot and histogram Using the ggExtra() package, you may also augment the scatterplot with a histogram: library(ggExtra) p &lt;- ggplot(music_data, aes(log(adv_spending), mstreams)) + geom_point() + labs(x = &quot;Advertising expenditures (EUR)&quot;, y = &quot;Number of strams&quot;, colour = &quot;store&quot;) + theme_bw() ggExtra::ggMarginal(p, type = &quot;histogram&quot;) Figure 3.20: Scatter plot with histogram In this case, the type = \"histogram\" argument specifies that we would like to plot a histogram. However, you could also opt for type = \"boxplot\" or type = \"density\" to use a boxplot or density plot instead. 3.2.2.5 Line plot Another important type of plot is the line plot used if, for example, you have a variable that changes over time and you want to plot how it develops over time. To demonstrate this we first gather the population of Austria from the world bank API (as we did previously). library(jsonlite) #specifies url url &lt;- &quot;http://api.worldbank.org/v2/country/at/indicator/SP.POP.TOTL?date=1960:2016&amp;format=json&quot; ctrydata_at &lt;- fromJSON(url) #parses the data head(ctrydata_at[[2]][, c(&quot;value&quot;, &quot;date&quot;)]) #checks if we scraped the desired data ctrydata_at &lt;- ctrydata_at[[2]][, c(&quot;date&quot;,&quot;value&quot;)] ctrydata_at$value &lt;- as.numeric(ctrydata_at$value) ctrydata_at$date &lt;- as.integer(ctrydata_at$date) str(ctrydata_at) ## &#39;data.frame&#39;: 50 obs. of 2 variables: ## $ date : int 2016 2015 2014 2013 2012 2011 2010 2009 2008 2007 ... ## $ value: num 8736668 8642699 8546356 8479823 8429991 ... As you can see doing this is very straightforward. Given the correct aes() and geom specification ggplot constructs the correct plot for us. ggplot(ctrydata_at, aes(x = date, y = value)) + geom_line() + labs(x = &quot;Year&quot;, y = &quot;Population of Austria&quot;) + theme_bw() Figure 3.21: Line plot 3.2.3 Saving plots To save the last displayed plot, simply use the function ggsave(), and it will save the plot to your working directory. Use the arguments heightand width to specify the size of the file. You may also choose the file format by adjusting the ending of the file name. E.g., file_name.jpg will create a file in JPG-format, whereas file_name.png saves the file in PNG-format, etc.. ggplot(table_plot_rel, aes(x = genre_cat, y = popularity_factor)) + geom_tile(aes(fill = Freq)) + ylab(&quot;Popularity&quot;) + xlab(&quot;Genre&quot;) + theme_bw() ggsave(&quot;popularity_genre_regression.jpg&quot;, height = 5, width = 7.5) 3.2.4 Additional options Now that we have covered the most important plots, we can look at what other type of data you may come across. One type of data that is increasingly available is the geo-location of customers and users (e.g., from app usage data). The following data set contains the app usage data of Shazam users from Germany. The data contains the latitude and longitude information where a music track was “shazamed”. library(ggmap) library(dplyr) geo_data &lt;- read.table(&quot;https://raw.githubusercontent.com/IMSMWU/Teaching/master/MRDA2017/geo_data.dat&quot;, sep = &quot;\\t&quot;, header = TRUE) head(geo_data) There is a package called “ggmap”, which is an augmentation for the ggplot packages. It lets you load maps from different web services (e.g., Google maps) and maps the user location within the coordination system of ggplot. With this information, you can create interesting plots like heat maps. We won’t go into detail here but you may go through the following code on your own if you are interested. However, please note that you need to register an API with Google in order to make use of this package. #register_google(key = &quot;your_api_key&quot;) # Download the base map de_map_g_str &lt;- get_map(location=c(10.018343,51.133481), zoom=6, scale=2) # results in below map (wohoo!) # Draw the heat map ggmap(de_map_g_str, extent = &quot;device&quot;) + geom_density2d(data = geo_data, aes(x = lon, y = lat), size = 0.3) + stat_density2d(data = geo_data, aes(x = lon, y = lat, fill = ..level.., alpha = ..level..), size = 0.01, bins = 16, geom = &quot;polygon&quot;) + scale_fill_gradient(low = &quot;green&quot;, high = &quot;red&quot;) + scale_alpha(range = c(0, 0.3), guide = FALSE) 3.3 Writing reports using R-Markdown This page will guide you through creating and editing R-Markdown documents. This is a useful tool for reporting your analysis (e.g. for homework assignments). Of course, there is also a cheat sheet for R-Markdown 3.3.1 Creating a new R-Markdown document If an R-Markdown file was provided to you, open it with R-Studio and skip to step 4 after adding your answers. Open R-Studio Create a new R-Markdown document Save with appropriate name 3.1. Add your answers 3.2. Save again “Knit” to HTML Hand in appropriate file (ending in .html) on learn@WU 3.3.2 Text and Equations R-Markdown documents are plain text files that include both text and R-code. Using RStudio they can be converted (‘knitted’) to HTML or PDF files that include both the text and the results of the R-code. In fact this website is written using R-Markdown and RStudio. In order for RStudio to be able to interpret the document you have to use certain characters or combinations of characters when formatting text and including R-code to be evaluated. By default the document starts with the options for the text part. You can change the title, date, author and a few more advanced options. First lines of an R-Markdown document The default is text mode, meaning that lines in an Rmd document will be interpreted as text, unless specified otherwise. 3.3.2.1 Headings Usually you want to include some kind of heading to structure your text. A heading is created using # signs. A single # creates a first level heading, two ## a second level and so on. It is important to note here that the # symbol means something different within the code chunks as opposed to outside of them. If you continue to put a # in front of all your regular text, it will all be interpreted as a first level heading, making your text very large. 3.3.2.2 Lists Bullet point lists are created using *, + or -. Sub-items are created by indenting the item using 4 spaces or 2 tabs. * First Item * Second Item + first sub-item - first sub-sub-item + second sub-item First Item Second Item first sub-item first sub-sub-item second sub-item Ordered lists can be created using numbers and letters. If you need sub-sub-items use A) instead of A. on the third level. 1. First item a. first sub-item A) first sub-sub-item b. second sub-item 2. Second item First item first sub-item first sub-sub-item second sub-item Second item 3.3.2.3 Text formatting Text can be formatted in italics (*italics*) or bold (**bold**). In addition, you can ad block quotes with &gt; &gt; Lorem ipsum dolor amet chillwave lomo ramps, four loko green juice messenger bag raclette forage offal shoreditch chartreuse austin. Slow-carb poutine meggings swag blog, pop-up salvia taxidermy bushwick freegan ugh poke. Lorem ipsum dolor amet chillwave lomo ramps, four loko green juice messenger bag raclette forage offal shoreditch chartreuse austin. Slow-carb poutine meggings swag blog, pop-up salvia taxidermy bushwick freegan ugh poke. 3.3.3 R-Code R-code is contained in so called “chunks”. These chunks always start with three backticks and r in curly braces ({r} ) and end with three backticks ( ). Optionally, parameters can be added after the r to influence how a chunk behaves. Additionally, you can also give each chunk a name. Note that these have to be unique, otherwise R will refuse to knit your document. 3.3.3.1 Global and chunk options The first chunk always looks as follows ```{r setup, include = FALSE} knitr::opts_chunk$set(echo = TRUE) ``` It is added to the document automatically and sets options for all the following chunks. These options can be overwritten on a per-chunk basis. Keep knitr::opts_chunk$set(echo = TRUE) to print your code to the document you will hand in. Changing it to knitr::opts_chunk$set(echo = FALSE) will not print your code by default. This can be changed on a per-chunk basis. ```{r cars, echo = FALSE} summary(cars) plot(dist~speed, cars) ``` ## speed dist ## Min. : 4.0 Min. : 2.00 ## 1st Qu.:12.0 1st Qu.: 26.00 ## Median :15.0 Median : 36.00 ## Mean :15.4 Mean : 42.98 ## 3rd Qu.:19.0 3rd Qu.: 56.00 ## Max. :25.0 Max. :120.00 ```{r cars2, echo = TRUE} summary(cars) plot(dist~speed, cars) ``` summary(cars) ## speed dist ## Min. : 4.0 Min. : 2.00 ## 1st Qu.:12.0 1st Qu.: 26.00 ## Median :15.0 Median : 36.00 ## Mean :15.4 Mean : 42.98 ## 3rd Qu.:19.0 3rd Qu.: 56.00 ## Max. :25.0 Max. :120.00 plot(dist~speed, cars) A good overview of all available global/chunk options can be found here. 3.3.4 LaTeX Math Writing well formatted mathematical formulae is done the same way as in LaTeX. Math mode is started and ended using $$. $$ f_1(\\omega) = \\frac{\\sigma^2}{2 \\pi},\\ \\omega \\in[-\\pi, \\pi] $$ \\[ f_1(\\omega) = \\frac{\\sigma^2}{2 \\pi},\\ \\omega \\in[-\\pi, \\pi] \\] (for those interested this is the spectral density of white noise) Including inline mathematical notation is done with a single $ symbol. ${2\\over3}$ of my code is inline. \\({2\\over3}\\) of my code is inline. Take a look at this wikibook on Mathematics in LaTeX and this list of Greek letters and mathematical symbols if you are not familiar with LaTeX. In order to write multi-line equations in the same math environment, use \\\\ after every line. In order to insert a space use a single \\. To render text inside a math environment use \\text{here is the text}. In order to align equations start with \\begin{align} and place an &amp; in each line at the point around which it should be aligned. Finally end with \\end{align} $$ \\begin{align} \\text{First equation: }\\ Y &amp;= X \\beta + \\epsilon_y,\\ \\forall X \\\\ \\text{Second equation: }\\ X &amp;= Z \\gamma + \\epsilon_x \\end{align} $$ \\[ \\begin{align} \\text{First equation: }\\ Y &amp;= X \\beta + \\epsilon_y,\\ \\forall X \\\\ \\text{Second equation: }\\ X &amp;= Z \\gamma + \\epsilon_x \\end{align} \\] 3.3.4.1 Important symbols Symbol Code \\(a^{2} + b\\) a^{2} + b \\(a^{2+b}\\) a^{2+b} \\(a_{1}\\) a_{1} \\(a \\leq b\\) a \\leq b \\(a \\geq b\\) a \\geq b \\(a \\neq b\\) a \\neq b \\(a \\approx b\\) a \\approx b \\(a \\in (0,1)\\) a \\in (0,1) \\(a \\rightarrow \\infty\\) a \\rightarrow \\infty \\(\\frac{a}{b}\\) \\frac{a}{b} \\(\\frac{\\partial a}{\\partial b}\\) \\frac{\\partial a}{\\partial b} \\(\\sqrt{a}\\) \\sqrt{a} \\(\\sum_{i = 1}^{b} a_i\\) \\sum_{i = 1}^{b} a_i \\(\\int_{a}^b f(c) dc\\) \\int_{a}^b f(c) dc \\(\\prod_{i = 0}^b a_i\\) \\prod_{i = 0}^b a_i \\(c \\left( \\sum_{i=1}^b a_i \\right)\\) c \\left( \\sum_{i=1}^b a_i \\right) The {} after _ and ^ are not strictly necessary if there is only one character in the sub-/superscript. However, in order to place multiple characters in the sub-/superscript they are necessary. e.g. Symbol Code \\(a^b = a^{b}\\) a^b = a^{b} \\(a^b+c \\neq a^{b+c}\\) a^b+c \\neq a^{b+c} \\(\\sum_i a_i = \\sum_{i} a_{i}\\) \\sum_i a_i = \\sum_{i} a_{i} \\(\\sum_{i=1}^{b+c} a_i \\neq \\sum_i=1^b+c a_i\\) \\sum_{i=1}^{b+c} a_i \\neq \\sum_i=1^b+c a_i 3.3.4.2 Greek letters Greek letters are preceded by a \\ followed by their name ($\\beta$ = \\(\\beta\\)). In order to capitalize them simply capitalize the first letter of the name ($\\Gamma$ = \\(\\Gamma\\)). "],
["introduction-to-statistical-inference.html", "4 Introduction to Statistical Inference 4.1 If we knew it all 4.2 The Central Limit Theorem 4.3 Using what we actually know 4.4 Summary", " 4 Introduction to Statistical Inference This chapter will provide you with a basic intuition on statistical inference. As marketing researchers we are usually faced with “imperfect” data in the sense that we cannot collect all the data we would like. Imagine you are interested in the average amount of time WU students spend listening to music every month. Ideally, we could force all WU students to fill out our survey. Realistically we will only be able to observe a small fraction of students (maybe 500 out of the \\(25.000+\\)). With the data from this small fraction at hand, we want to make an inference about the true average listening time of all WU students. We are going to start with the assumption that we know everything. That is, we first assume that we know all WU students’ listening times and analyze the distribution of the listening time in the entire population. Subsequently, we are going to look at the uncertainty that is introduced by only knowing some of the students’ listening times (i.e., a sample from the population) and how that influences our analysis. You can download the corresponding R-Code here 4.1 If we knew it all Assume there are \\(25,000\\) students at WU and every single one has kindly provided us with the hours they listened to music in the past month. In this case, we know the true mean (\\(49.93\\) hours) and the true standard deviation (SD = \\(10.02\\)) and thus we can easily summarize the entire distribution. Since the data follows a normal distribution, roughly 95% of the values lie within 2 standard deviations from the mean, as the following plot shows: library(tidyverse) library(ggplot2) library(latex2exp) set.seed(321) hours &lt;- rnorm(25000, 50, 10) ggplot(data.frame(hours)) + geom_histogram(aes(hours), bins = 50, fill = &#39;white&#39;, color = &#39;black&#39;) + labs(title = &quot;Histogram of listening times&quot;, subtitle = TeX(sprintf(&quot;Population mean ($\\\\mu$) = %.2f; population standard deviation ($\\\\sigma$) = %.2f&quot;,round(mean(hours),2),round(sd(hours),2))), y = &#39;Number of students&#39;, x = &#39;Hours&#39;) + theme_bw() + geom_vline(xintercept = mean(hours), size = 1) + geom_vline(xintercept = mean(hours)+2*sd(hours), colour = &quot;red&quot;, size = 1) + geom_vline(xintercept = mean(hours)-2*sd(hours), colour = &quot;red&quot;, size = 1) + geom_segment(aes(x = mean(hours), y = 1100, yend = 1100, xend = (mean(hours) - 2*sd(hours))), lineend = &quot;butt&quot;, linejoin = &quot;round&quot;, size = 0.5, arrow = arrow(length = unit(0.2, &quot;inches&quot;))) + geom_segment(aes(x = mean(hours), y = 1100, yend = 1100, xend = (mean(hours) + 2*sd(hours))), lineend = &quot;butt&quot;, linejoin = &quot;round&quot;, size = 0.5, arrow = arrow(length = unit(0.2, &quot;inches&quot;))) + annotate(&quot;text&quot;, x = mean(hours) + 28, y = 1100, label = &quot;Mean + 2 * SD&quot; )+ annotate(&quot;text&quot;, x = mean(hours) -28, y = 1100, label = &quot;Mean - 2 * SD&quot; ) In this case, we refere to all WU students as the population. In general, the population is the entire group we are interested in. This group does not have to necessarily consist of people, but could also be companies, stores, animals, etc.. The parameters of the distribution of population values (in hour case: “hours”) are called population parameters. As already mentioned, we do not usually know population parameters but use inferential statistics to infer them based on our sample from the population, i.e., we measure statistics from a sample (e.g., the sample mean \\(\\bar x\\)) to estimate population parameters (the population mean \\(\\mu\\)). Here, we will use the following notation to refer to either the population parameters or the sample statistic: Variable Sample statistic Population parameter Size n N Mean \\(\\bar{x} = {1 \\over n}\\sum_{i=1}^n x_i\\) \\(\\mu = {1 \\over N}\\sum_{i=1}^N x_i\\) Variance \\(s^2 = {1 \\over n-1}\\sum_{i=1}^n (x_i-\\bar{x})^2\\) \\(\\sigma^2 = {1 \\over N}\\sum_{i=1}^N (x_i-\\mu)^2\\) Standard deviation \\(s = \\sqrt{s^2}\\) \\(\\sigma = \\sqrt{\\sigma^2}\\) Standard error \\(SE_{\\bar x} = {s \\over \\sqrt{n}}\\) \\(\\sigma_{\\bar x} = {\\sigma \\over \\sqrt{n}}\\) 4.1.1 Sampling from a known population In the first step towards a realistic research setting, let us take one sample from this population and calculate the mean listening time. We can simply sample the row numbers of students and then subset the hours vector with the sampled rownumbers. The sample() function will be used to draw a sample of size 100 from the population of 25,000 students, and one student can only be drawn once (i.e., replace = FALSE). The following plot shows the distribution of listening times for our sample. Observe that in this first draw the mean (\\(\\bar x =\\) 49.67) is quite close to the actual mean ($= $ 49.93). It seems like the sample mean is a decent estimate of the population mean. However, we could just be lucky this time and the next sample could turn out to have a different mean. Let us continue by looking at four additional random samples, consisting of 100 students each. The following plot shows the distribution of listening times for the four different samples from the population. It becomes clear that the mean is slightly different for each sample. This is referred to as sampling variation and it is completely fine to get a slightly different mean every time we take a sample. We just need to find a way of expressing the uncertainty associated with the fact that we only have data from one sample, because in a realistic setting you are most likely only going to have access to a single sample. So in order to make sure that the first draw is not just pure luck and the sample mean is in fact a good estimate for the population mean, let us take many (e.g., \\(20,000\\)) different samples from the population. That is, we repeatedly draw 100 students randomly from the population without replacement (that is, once a student has been drawn she or he is removed from the pool and cannot be drawn again) and calculate the mean of each sample. This will show us a range within which the sample mean of any sample we take is likely going to be. We are going to store the means of all the samples in a matrix and then plot a histogram of the means to observe the likely values. As you can see, on average the sample mean (“mean of sample means”) is extremely close to the population mean, despite only sampling \\(100\\) people at a time. This distribution of sample means is also referred to as sampling distribution of the sample mean. However, there is some uncertainty, and the means are slightly different for the different samples and range from 45.95 to 54.31. 4.1.2 Standard error of the mean Due to the variation in the sample means shown in our simulation, it is never possible to say exactly what the population mean is based on a single sample. However, even with a single sample we can infer a range of values within which the population mean is likely contained. In order to do so, notice that the sample means are approximately normally distributed. Another interesting fact is that the mean of sample means (i.e., 49.94) is roughly equal to the population mean (i.e., 49.93). This tells us already that generally the sample mean is a good approximation of the population mean. However, in order to make statements about the expected range of possible values, we would need to know the standard deviation of the sampling distribution. The formal representation of the standard deviation of the sample means is \\[ \\sigma_{\\bar x} = {\\sigma \\over \\sqrt{n}} \\] where \\(\\sigma\\) is the population SD and \\(n\\) is the sample size. \\(\\sigma_{\\bar{x}}\\) is referred to as the Standard Error of the mean and it expresses the variation in sample means we should expect given the number of observations in our sample and the population SD. That is, it provides a measure of how precisely we can estimate the population mean from the sample mean. 4.1.2.1 Sample size The first thing to notice here is that an increase in the number of observations per sample \\(n\\) decreases the range of possible sample means (i.e., the standard error). This makes intuitive sense. Think of the two extremes: sample size \\(1\\) and sample size \\(25,000\\). With a single person in the sample we do not gain a lot of information and our estimate is very uncertain, which is expressed through a larger standard deviation. Looking at the histogram at the beginning of this chapter showing the number of students for each of the listening times, clearly we would get values below \\(25\\) or above \\(75\\) for some samples. This is way farther away from the population mean than the minimum and the maximum of our \\(100\\) person samples. On the other hand, if we sample every student we get the population mean every time and thus we do not have any uncertainty (assuming the population does not change). Even if we only sample, say \\(24,000\\) people every time, we gain a lot of information about the population and the sample means would not be very different from each other since only up to \\(1,000\\) people are potentially different in any given sample. Thus, with larger (smaller) samples, there is less (more) uncertainty that the sample is a good approximation of the entire population. The following plot shows the relationship between the sample size and the standard error. Samples of increasing size are randomly drawn from the population of WU students. You can see that the standard error is decreasing with the number of observations. Figure 4.1: Relationship between the sample size and the standard error The following plots show the relationship between the sample size and the standard error in a slightly different way. The plots show the range of sample means resulting from the repeated sampling process for different sample sizes. Notice that the more students are contained in the individual samples, the less uncertainty there is when estimating the population mean from a sample (i.e., the possible values are more closely centered around the mean). So when the sample size is small, the sample mean can expected to be very different the next time we take a sample. When the sample size is large, we can expect the sample means to be more similar every time we take a sample. As you can see, the standard deviation of the sample means (\\(\\sigma_{\\bar x}\\)) decreases as the sample size increases as a consequence of the reduced uncertainty about the true sample mean when we take larger samples. 4.1.2.2 Population standard deviation A second factor determining the standard deviation of the distribution of sample means (\\(\\sigma_{\\bar x}\\)) is the standard deviation associated with the population parameter (\\(\\sigma\\)). Again, looking at the extremes illustrates this well. If all WU students listened to music for approximately the same amount of time, the samples would not differ much from each other. In other words, if the standard deviation in the population is lower, we expect the standard deviation of the sample means to be lower as well. This is illustrated by the following plots. In the first plot (panel A), we assume a much smaller population standard deviation (e.g., \\(\\sigma\\) = 1 instead of \\(\\sigma\\) = 10). Notice how the smaller (larger) the population standard deviation, the less (more) uncertainty there is when estimating the population mean from a sample (i.e., the possible values are more closely centered around the mean). So when the population SD is large, the sample mean can expected to be very different the next time we take a sample. When the population SD is small, we can expect the sample means to be more similar. 4.2 The Central Limit Theorem The attentive reader might have noticed that the population above was generated using a normal distribution function. It would be very restrictive if we could only analyze populations whose values are normally distributed. Furthermore, we are unable in reality to check whether the population values are normally distributed since we do not know the entire population. However, it turns out that the results generalize to many other distributions. This is described by the Central Limit Theorem. The central limit theorem states that if (1) the population distribution has a mean (there are examples of distributions that don’t have a mean , but we will ignore these here), and (2) we take a large enough sample, then the sampling distribution of the sample mean is approximately normally distributed. What exactly “large enough” means depends on the setting, but the interactive element at the end of this chapter illustrates how the sample size influences how accurately we can estimate the population parameters from the sample statistics. To illustrate this, let’s repeat the analysis above with a population from a gamma distribution. In the previous example, we assumed a normal distribution so it was more likely for a given student to spend around 50 hours per week listening to music. The following example depicts the case in which most students spend a similar amount of time listening to music, but there are a few students who very rarely listen to music, and some music enthusiasts with a very high level of listening time. Here is a histogram of the listening times in the population: set.seed(321) hours &lt;- rgamma(25000, shape = 2, scale = 10) ggplot(data.frame(hours)) + geom_histogram(aes(x = hours), bins = 30, fill=&#39;white&#39;, color=&#39;black&#39;) + geom_vline(xintercept = mean(hours), size = 1) + theme_bw() + labs(title = &quot;Histogram of listening times&quot;, subtitle = TeX(sprintf(&quot;Population mean ($\\\\mu$) = %.2f; population standard deviation ($\\\\sigma$) = %.2f&quot;,round(mean(hours),2),round(sd(hours),2))), y = &#39;Number of students&#39;, x = &#39;Hours&#39;) The vertical black line represents the population mean (\\(\\mu\\)), which is 19.98 hours. The following plot depicts the histogram of listening times of four random samples from the population, each consisting of 100 students: As in the previous example, the mean is slightly different every time we take a sample due to sampling variation. Also note that the distribution of listening times no longer follows a normal distribution as a result of the fact that we now assume a gamma distribution for the population with a positive skew (i.e., lower values more likely, higher values less likely). Let’s see what happens to the distribution of sample means if we take an increasing number of samples, each drawn from the same gamma population: Two things are worth noting: (1) The more (hypothetical) samples we take, the more the sampling distribution approaches a normal distribution. (2) The mean of the sampling distribution of the sample mean (\\(\\mu_{\\bar x}\\)) is very similar to the population mean (\\(\\mu\\)). From this we can see that the mean of a sample is a good estimate of the population mean. In summary, it is important to distinguish two types of variation: (1) For each individual sample that we may take in real life, the standard deviation (\\(s\\)) is used to describe the natural variation in the data and the data may follow a non-normal distribution. (2) If we would (hypothetically!) repeat the study many times, the sampling distribution of the sample mean follows a normal distribution for large samples sizes (even if data from each individual study are non-normal), and the standard error (\\(\\sigma_{\\bar x}\\)) is used to describe the variation between study results. This is an important feature, since many statistical tests assume that the sampling distribution is normally distributed. As we have seen, this does not mean that the data from one particular sample needs to follow a normal distribution. 4.3 Using what we actually know So far we have assumed to know the population standard deviation (\\(\\sigma\\)). This an unrealistic assumption since we do not know the entire population. The best guess for the population standard deviation we have is the sample standard deviation, denoted \\(s\\). Thus, the standard error of the mean is usually estimated from the sample standard deviation: \\[ \\sigma_{\\bar x} \\approx SE_{\\bar x}={s \\over \\sqrt{n}} \\] Note that \\(s\\) itself is a sample estimate of the population parameter \\(\\sigma\\). This additional estimation introduces further uncertainty. You can see in the interactive element below that the sample SD, on average, provides a good estimate of the population SD. That is, the distribution of sample SDs that we get by drawing many samples is centered around the population value. Again, the larger the sample, the closer any given sample SD is going to be to the population parameter and we introduce less uncertainty. One conclusion is that your sample needs to be large enough to provide a reliable estimate of the population parameters. What exactly “large enough” means depends on the setting, but the interactive element illustrates how the remaining values change as a function of the sample size. We will not go into detail about the importance of random samples but basically the correctness of your estimate depends crucially on having a sample at hand that actually represents the population. Unfortunately, we will usually not notice if the sample is non-random. Our statistics are still a good approximation of “a” population parameter, namely the one for the population that we actually sampled but not the one we are interested in. To illustrate this uncheck the “Random Sample” box below. The new sample will be only from the top \\(50\\%\\) music listeners (but this generalizes to different types of non-random samples). 4.3.1 Confidence Intervals for the Sample Mean When we try to estimate parameters of populations (e.g., the population mean \\(\\mu\\)) from a sample, the average value from a sample (e.g., the sample mean \\(\\bar x\\)) only provides an estimate of what the real population parameter is. The next time you collect a sample of the same size, you could get a different average. This is sampling variation and it is completely fine to get a slightly different sample mean every time we take a sample as we have seen above. However, this inherent uncertainty about the true population parameter means that coming up with an exact estimate (i.e., a point estimate) for a particular population parameter is really difficult. That is why it is often informative to construct a range around that statistic (i.e., an interval estimate) that likely contains the population parameter with a certain level of confidence. That is, we construct an interval such that for a large share (say 95%) of the sample means we could potentially get, the population mean is within that interval. Let us consider one random sample of 100 students from our population above. set.seed(321) hours &lt;- rgamma(25000, shape = 2, scale = 10) set.seed(6789) sample_size &lt;- 100 student_sample &lt;- sample(1:25000, size = sample_size, replace = FALSE) hours_s &lt;- hours[student_sample] plot2 &lt;- ggplot(data.frame(hours_s)) + geom_histogram(aes(x = hours_s), bins = 30, fill=&#39;white&#39;, color=&#39;black&#39;) + theme_bw() + xlab(&quot;Hours&quot;) + geom_vline(aes(xintercept = mean(hours_s)), size=1) + ggtitle(TeX(sprintf(&quot;Random sample; $n$ = %d; $\\\\bar{x}$ = %.2f; $s$ = %.2f&quot;,sample_size,round(mean(hours_s),2),round(sd(hours_s),2)))) plot2 From the central limit theorem we know that the sampling distribution of the sample mean is approximately normal and we know that for the normal distribution, 95% of the values lie within about 2 standard deviations from the mean. Actually, it is not exactly 2 standard deviations from the mean. To get the exact number, we can use the quantile function for the normal distribution qnorm(): qnorm(0.975) ## [1] 1.959964 We use 0.975 (and not 0.95) to account for the probability at each end of the distribution (i.e., 2.5% at the lower end and 2.5% at the upper end). We can see that 95% of the values are roughly within 1.96 standard deviations from the mean. Since we know the sample mean (\\(\\bar x\\)) and we can estimate the standard deviation of the sampling distribution (\\(\\sigma_{\\bar x} \\approx {s \\over \\sqrt{n}}\\)), we can now easily calculate the lower and upper boundaries of our confidence interval as: \\[ CI_{lower} = {\\bar x} - z_{1-{\\alpha \\over 2}} * \\sigma_{\\bar x} \\\\ CI_{upper} = {\\bar x} + z_{1-{\\alpha \\over 2}} * \\sigma_{\\bar x} \\] Here, \\(\\alpha\\) refers to the significance level. You can find a detailed discussion of this point at the end of the next chapter. For now, we will adopt the widely accepted significance level of 5% and set \\(\\alpha\\) to 0.05. Thus, \\(\\pm z_{1-{\\alpha \\over 2}}\\) gives us the z-scores (i.e., number of standard deviations from the mean) within which range 95% of the probability density lies. Plugging in the values from our sample, we get: sample_mean &lt;- mean(hours_s) se &lt;- sd(hours_s)/sqrt(sample_size) ci_lower &lt;- sample_mean - qnorm(0.975)*se ci_upper &lt;- sample_mean + qnorm(0.975)*se ci_lower ## [1] 17.67089 ci_upper ## [1] 23.1592 such that if we collected 100 samples and computed the mean and confidence interval for each of them, in \\(95\\%\\) of the cases, the true population mean is going to be within this interval between 17.67 and 23.16. This is illustrated in the plot below that shows the mean of the first 100 samples and their confidence intervals: set.seed(12) samples &lt;- 100 hours &lt;- rgamma(25000, shape = 2, scale = 10) means &lt;- matrix(NA, nrow = samples) for (i in 1:samples){ student_sample &lt;- sample(1:25000, size = 100, replace = FALSE) means[i,] &lt;- mean(hours[student_sample]) } means_sd &lt;- data.frame(means, lower = means - qnorm(0.975) * (sd(hours)/sqrt(100)), upper = means + qnorm(0.975) * (sd(hours)/sqrt(100)), y = 1:100) means_sd$diff &lt;- factor(ifelse(means_sd$lower &gt; mean(hours) | means_sd$upper &lt; mean(hours), &#39;No&#39;, &#39;Yes&#39;)) ggplot2::ggplot(means_sd, aes(y = y)) + scale_y_continuous(breaks = seq(1, 100, by = 1), expand = c(0.005,0.005)) + geom_point(aes(x = means, color = diff)) + geom_errorbarh(aes(xmin = lower, xmax = upper, color = diff)) + geom_vline(xintercept = mean(hours)) + scale_color_manual(values = c(&quot;red&quot;, &quot;black&quot;)) + guides(color=guide_legend(title=&quot;True mean in CI&quot;)) + theme_bw() Note that this does not mean that for a specific sample there is a \\(95\\%\\) chance that the population mean lies within its confidence interval. The statement depends on the large number of samples we do not actually draw in a real setting. You can view the set of all possible confidence intervals similarly to the sides of a coin or a die. If we throw a coin many times, we are going to observe head roughly half of the times. This does not, however, exclude the possibility of observing tails for the first 10 throws. Similarly, any specific confidence interval might or might not include the population mean but if we take many samples on average \\(95\\%\\) of the confidence intervals are going to include the population mean. 4.4 Summary When conducting research, we are (almost) always faced with the problem of not having access to the entire group we are interested in. Therefore, we use sample properties that we have derived in this chapter to do statistical inference based on a single sample. We use statistics of the sample as well as the sample size to calculate the confidence interval of our choice (e.g. \\(95\\%\\)). In practice most of this is done for us, so you won’t need to do this by hand. With a sample at hand, we need to choose the appropriate test and a null hypothesis. How this is done will be discussed in the next chapter. "],
["hypothesis-testing.html", "5 Hypothesis testing 5.1 Introduction 5.2 One sample t-test 5.3 Comparing two means 5.4 Comparing several means 5.5 Non-parametric tests 5.6 Categorical data 5.7 Summary of hypothesis testing", " 5 Hypothesis testing This chapter is primarily based on Field, A., Miles J., &amp; Field, Z. (2012): Discovering Statistics Using R. Sage Publications, chapters 5, 9, 15, 18. You can download the corresponding R-Code here 5.1 Introduction We test hypotheses because we are confined to taking samples – we rarely work with the entire population. In the previous chapter, we introduced the standard error (i.e., the standard deviation of a large number of hypothetical samples) as an estimate of how well a particular sample represents the population. We also saw how we can construct confidence intervals around the sample mean \\(\\bar x\\) by computing \\(SE_{\\bar x}\\) as an estimate of \\(\\sigma_{\\bar x}\\) using \\(s\\) as an estimate of \\(\\sigma\\) and calculating the 95% CI as \\(\\bar x \\pm 1.96 * SE_{\\bar x}\\). Although we do not know the true population mean (\\(\\mu\\)), we might have an hypothesis about it and this would tell us how the corresponding sampling distribution looks like. Based on the sampling distribution of the hypothesized population mean, we could then determine the probability of a given sample assuming that the hypothesis is true. Let us again begin by assuming we know the entire population using the example of music listening times among students from the previous example. As a reminder, the following plot shows the distribution of music listening times in the population of WU students. library(tidyverse) library(ggplot2) library(latex2exp) set.seed(321) hours &lt;- rgamma(25000, shape = 2, scale = 10) ggplot(data.frame(hours)) + geom_histogram(aes(x = hours), bins = 30, fill=&#39;white&#39;, color=&#39;black&#39;) + geom_vline(xintercept = mean(hours), size = 1) + theme_bw() + labs(title = &quot;Histogram of listening times&quot;, subtitle = TeX(sprintf(&quot;Population mean ($\\\\mu$) = %.2f; population standard deviation ($\\\\sigma$) = %.2f&quot;,round(mean(hours),2),round(sd(hours),2))), y = &#39;Number of students&#39;, x = &#39;Hours&#39;) In this example, the population mean (\\(\\mu\\)) is equal to 19.98, and the population standard deviation \\(\\sigma\\) is equal to 14.15. 5.1.1 The null hypothesis Let us assume that we were planning to take a random sample of 50 students from this population and our hypothesis was that the mean listening time is equal to some specific value \\(\\mu_0\\), say \\(10\\). This would be our null hypothesis. The null hypothesis refers to the statement that is being tested and is usually a statement of the status quo, one of no difference or no effect. In our example, the null hypothesis would state that there is no difference between the true population mean \\(\\mu\\) and the hypothesized value \\(\\mu_0\\) (in our example \\(10\\)), which can be expressed as follows: \\[ H_0: \\mu = \\mu_0 \\] When conducting research, we are usually interested in providing evidence against the null hypothesis. If we then observe sufficient evidence against it and our estimate is said to be significant. If the null hypothesis is rejected, this is taken as support for the alternative hypothesis. The alternative hypothesis assumes that some difference exists, which can be expressed as follows: \\[ H_1: \\mu \\neq \\mu_0 \\] Accepting the alternative hypothesis in turn will often lead to changes in opinions or actions. Note that while the null hypothesis may be rejected, it can never be accepted based on a single test. If we fail to reject the null hypothesis, it means that we simply haven’t collected enough evidence against the null hypothesis to disprove it. In classical hypothesis testing, there is no way to determine whether the null hypothesis is true. Hypothesis testing provides a means to quantify to what extent the data from our sample is in line with the null hypothesis. In order to quantify the concept of “sufficient evidence” we look at the theoretical distribution of the sample means given our null hypothesis and the sample standard error. Using the available information we can infer the sampling distribution for our null hypothesis. Recall that the standard deviation of the sampling distribution (i.e., the standard error of the mean) is given by \\(\\sigma_{\\bar x}={\\sigma \\over \\sqrt{n}}\\), and thus can be computed as follows: mean_pop &lt;- mean(hours) sigma &lt;- sd(hours) #population standard deviation n &lt;- 50 #sample size standard_error &lt;- sigma/sqrt(n) #standard error standard_error ## [1] 2.001639 Since we know from the central limit theorem that the sampling distribution is normal for large enough samples, we can now visualize the expected sampling distribution if our null hypothesis was in fact true (i.e., if the was no difference between the true population mean and the hypothesized mean of 10). We also know that 95% of the probability is within 1.96 standard deviations from the mean. Values higher than that are rather unlikely, if our hypothesis about the population mean was indeed true. This is shown by the shaded area, also known as the “rejection region”. To test our hypothesis that the population mean is equal to \\(10\\), let us take a random sample from the population. set.seed(12567) H_0 &lt;- 10 student_sample &lt;- sample(1:25000, size = 50, replace = FALSE) student_sample &lt;- hours[student_sample] mean_sample &lt;- mean(student_sample) ggplot(data.frame(student_sample)) + geom_histogram(aes(x = student_sample), fill = &#39;white&#39;, color = &#39;black&#39;, bins = 20) + theme_bw() + geom_vline(xintercept = mean(student_sample), color = &#39;black&#39;, size=1) + labs(title = TeX(sprintf(&quot;Distribution of values in the sample ($n =$ %.0f, $\\\\bar{x] = $ %.2f, s = %.2f)&quot;,n,mean(student_sample),sd(student_sample))),x = &quot;Hours&quot;, y = &quot;Frequency&quot;) The mean listening time in the sample (black line) \\(\\bar x\\) is 18.59. We can already see from the graphic above that such a value is rather unlikely under the hypothesis that the population mean is \\(10\\). Intuitively, such a result would therefore provide evidence against our null hypothesis. But how could we quantify specifically how unlikely it is to obtain such a value and decide whether or not to reject the null hypothesis? Significance tests can be used to provide answers to these questions. 5.1.2 Statistical inference on a sample 5.1.2.1 Test statistic 5.1.2.1.1 z-scores Let’s go back to the sampling distribution above. We know that 95% of all values will fall within 1.96 standard deviations from the mean. So if we could express the distance between our sample mean and the null hypothesis in terms of standard deviations, we could make statements about the probability of getting a sample mean of the observed magnitude (or more extreme values). Essentially, we would like to know how many standard deviations (\\(\\sigma_{\\bar x}\\)) our sample mean (\\(\\bar x\\)) is away from the population mean if the null hypothesis was true (\\(\\mu_0\\)). This can be formally expressed as follows: \\[ \\bar x- \\mu_0 = z \\sigma_{\\bar x} \\] In this equation, z will tell us how many standard deviations the sample mean \\(\\bar x\\) is away from the null hypothesis \\(\\mu_0\\). Solving for z gives us: \\[ z = {\\bar x- \\mu_0 \\over \\sigma_{\\bar x}}={\\bar x- \\mu_0 \\over \\sigma / \\sqrt{n}} \\] This standardized value (or “z-score”) is also referred to as a test statistic. Let’s compute the test statistic for our example above: z_score &lt;- (mean_sample - H_0)/(sigma/sqrt(n)) z_score ## [1] 4.292454 To make a decision on whether the difference can be deemed statistically significant, we now need to compare this calculated test statistic to a meaningful threshold. In order to do so, we need to decide on a significance level \\(\\alpha\\), which expresses the probability of finding an effect that does not actually exist (i.e., Type I Error). You can find a detailed discussion of this point at the end of this chapter. For now, we will adopt the widely accepted significance level of 5% and set \\(\\alpha\\) to 0.05. The critical value for the normal distribution and \\(\\alpha\\) = 0.05 can be computed using the qnorm() function as follows: z_crit &lt;- qnorm(0.975) z_crit ## [1] 1.959964 We use 0.975 and not 0.95 since we are running a two-sided test and need to account for the rejection region at the other end of the distribution. Recall that for the normal distribution, 95% of the total probability falls within 1.96 standard deviations of the mean, so that higher (absolute) values provide evidence against the null hypothesis. Generally, we speak of a statistically significant effect if the (absolute) calculated test statistic is larger than the (absolute) critical value. We can easily check if this is the case in our example: abs(z_score) &gt; abs(z_crit) ## [1] TRUE Since the absolute value of the calculated test statistic is larger than the critical value, we would reject \\(H_0\\) and conclude that the true population mean \\(\\mu\\) is significantly different from the hypothesized value \\(\\mu_0 = 10\\). 5.1.2.1.2 t-statistic You may have noticed that the formula for the z-score above assumes that we know the true population standard deviation (\\(\\sigma\\)) when computing the standard deviation of the sampling distribution (\\(\\sigma_{\\bar x}\\)) in the denominator. However, the population standard deviation is usually not known in the real world and therefore represents another unknown population parameter which we have to estimate from the sample. We saw in the previous chapter that we usually use \\(s\\) as an estimate of \\(\\sigma\\) and \\(SE_{\\bar x}\\) as and estimate of \\(\\sigma_{\\bar x}\\). Intuitively, we should be more conservative regarding the critical value that we used above to assess whether we have a significant effect to reflect this uncertainty about the true population standard deviation. That is, the threshold for a “significant” effect should be higher to safeguard against falsely claiming a significant effect when there is none. If we replace \\(\\sigma_{\\bar x}\\) by it’s estimate \\(SE_{\\bar x}\\) in the formula for the z-score, we get a new test statistic (i.e, the t-statistic) with its own distribution (the t-distribution): \\[ t = {\\bar x- \\mu_0 \\over SE_{\\bar x}}={\\bar x- \\mu_0 \\over s / \\sqrt{n}} \\] Here, \\(\\bar X\\) denotes the sample mean and \\(s\\) the sample standard deviation. The t-distribution has more probability in its “tails”, i.e. farther away from the mean. This reflects the higher uncertainty introduced by replacing the population standard deviation by its sample estimate. Intuitively, this is particularly relevant for small samples, since the uncertainty about the true population parameters decreases with increasing sample size. This is reflected by the fact that the exact shape of the t-distribution depends on the degrees of freedom, which is the sample size minus one (i.e., \\(n-1\\)). To see this, the following graph shows the t-distribution with different degrees of freedom for a two-tailed test and \\(\\alpha = 0.05\\). The grey curve shows the normal distribution. Notice that as \\(n\\) gets larger, the t-distribution gets closer and closer to the normal distribution, reflecting the fact that the uncertainty introduced by \\(s\\) is reduced. To summarize, we now have an estimate for the standard deviation of the distribution of the sample mean (i.e., \\(SE_{\\bar x}\\)) and an appropriate distribution that takes into account the necessary uncertainty (i.e., the t-distribution). Let us now compute the t-statistic according to the formula above: SE &lt;- (sd(student_sample)/sqrt(n)) t_score &lt;- (mean_sample - H_0)/SE t_score ## [1] 4.84204 Notice that the value of the t-statistic is higher compared to the z-score (4.29). This can be attributed to the fact that by using the \\(s\\) as and estimate of \\(\\sigma\\), we underestimate the true population standard deviation. Hence, the critical value would need to be larger to adjust for this. This is what the t-distribution does. Let us compute the critical value from the t-distribution with n - 1degrees of freedom. df = n - 1 t_crit &lt;- qt(0.975, df = df) t_crit ## [1] 2.009575 Again, we use 0.975 and not 0.95 since we are running a two-sided test and need to account for the rejection region at the other end of the distribution. Notice that the new critical value based on the t-distributionis larger, to reflect the uncertainty when estimating \\(\\sigma\\) from \\(s\\). Now we can see that the calculated test statistic is still larger than the critical value. abs(t_score) &gt; abs(t_crit) ## [1] TRUE The following graphics shows that the calculated test statistic (red line) falls into the rejection region so that in our example, we would reject the null hypothesis that the true population mean is equal to \\(10\\). Decision: Reject \\(H_0\\), given that the calculated test statistic is larger than critical value. Something to keep in mind here is the fact the test statistic is a function of the sample size. This, as \\(n\\) gets large, the test statistic gets larger as well and we are more likely to find a significant effect. This reflects the decrease in uncertainty about the true population mean as our sample size increases. 5.1.2.2 P-values In the previous section, we computed the test statistic, which tells us how close our sample is to the null hypothesis. The p-value corresponds to the probability that the test statistic would take a value as extreme or more extreme than the one that we actually observed, assuming that the null hypothesis is true. It is important to note that this is a conditional probability: we compute the probability of observing a sample mean (or a more extreme value) conditional on the assumption that the null hypothesis is true. The pnorm()function can be used to compute this probability. It is the cumulative probability distribution function of the `normal distribution. Cumulative probability means that the function returns the probability that the test statistic will take a value less than or equal to the calculated test statistic given the degrees of freedom. However, we are interested in obtaining the probability of observing a test statistic larger than or equal to the calculated test statistic under the null hypothesis (i.e., the p-value). Thus, we need to subtract the cumulative probability from 1. In addition, since we are running a two-sided test, we need to multiply the probability by 2 to account for the rejection region at the other side of the distribution. p_value &lt;- 2*(1-pt(abs(t_score), df = df)) p_value ## [1] 0.00001326885 This value corresponds to the probability of observing a mean equal to or larger than the one we obtained from our sample, if the null hypothesis was true. As you can see, this probability is very low. A small p-value signals that it is unlikely to observe the calculated test statistic under the null hypothesis. To decide whether or not to reject the null hypothesis, we would now compare this value to the level of significance (\\(\\alpha\\)) that we chose for our test. For this example, we adopt the widely accepted significance level of 5%, so any test results with a p-value &lt; 0.05 would be deemed statistically significant. Note that the p-value is directly related to the value of the test statistic. The relationship is such that the higher (lower) the value of the test statistic, the lower (higher) the p-value. Decision: Reject \\(H_0\\), given that the p-value is smaller than 0.05. 5.1.2.3 Confidence interval For a given statistic calculated for a sample of observations (e.g., listening times), a 95% confidence interval can be constructed such that in 95% of samples, the true value of the true population mean will fall within its limits. If the parameter value specified in the null hypothesis (here \\(10\\)) does not lie within the bounds, we reject \\(H_0\\). Building on what we learned about confidence intervals in the previous chapter, the 95% confidence interval based on the t-distribution can be computed as follows: \\[ CI_{lower} = {\\bar x} - t_{1-{\\alpha \\over 2}} * SE_{\\bar x} \\\\ CI_{upper} = {\\bar x} + t_{1-{\\alpha \\over 2}} * SE_{\\bar x} \\] It is easy to compute this interval manually: ci_lower &lt;- (mean_sample)-qt(0.975, df = df)*SE ci_upper &lt;- (mean_sample)+qt(0.975, df = df)*SE ci_lower ## [1] 15.02606 ci_upper ## [1] 22.15783 The interpretation of this interval is as follows: if we would (hypothetically) take 100 samples and calculated the mean and confidence interval for each of them, then the true population mean would be included in 95% of these intervals. The CI is informative when reporting the result of your test, since it provides an estimate of the uncertainty associated with the test result. From the test statistic or the p-value alone, it is not easy to judge in which range the true population parameter is located. The CI provides an estimate of this range. Decision: Reject \\(H_0\\), given that the parameter value from the null hypothesis (\\(10\\)) is not included in the interval. To summarize, you can see that we arrive at the same conclusion (i.e., reject \\(H_0\\)), irrespective if we use the test statistic, the p-value, or the confidence interval. However, keep in mind that rejecting the null hypothesis does not prove the alternative hypothesis (we can merely provide support for it). Rather, think of the p-value as the chance of obtaining the data we’ve collected assuming that the null hypothesis is true. You should report the confidence interval to provide an estimate of the uncertainty associated with your test results. 5.1.3 Choosing the right test The test statistic, as we have seen, measures how close the sample is to the null hypothesis and often follows a well-known distribution (e.g., normal, t, or chi-square). To select the correct test, various factors need to be taken into consideration. Some examples are: On what scale are your variables measured (categorical vs. continuous)? Do you want to test for relationships or differences? If you test for differences, how many groups would you like to test? For parametric tests, are the assumptions fulfilled? The previous discussion used a one sample t-test as an example, which requires that variable is measured on an interval or ratio scale. If you are confronted with other settings, the following flow chart provides a rough guideline on selecting the correct test: Flowchart for selecting an appropriate test (source: McElreath, R. (2016): Statistical Rethinking, p. 2) For a detailed overview over the different type of tests, please also refer to this overview by the UCLA. 5.1.3.1 Parametric vs. non-parametric tests A basic distinction can be made between parametric and non-parametric tests. Parametric tests require that variables are measured on an interval or ratio scale and that the sampling distribution follows a known distribution. Non-Parametric tests on the other hand do not require the sampling distribution to be normally distributed (a.k.a. “assumption free tests”). These tests may be used when the variable of interest is measured on an ordinal scale or when the parametric assumptions do not hold. They often rely on ranking the data instead of analyzing the actual scores. By ranking the data, information on the magnitude of differences is lost. Thus, parametric tests are more powerful if the sampling distribution is normally distributed. In this chapter, we will first focus on parametric tests and cover non-parametric tests later. 5.1.3.2 One-tailed vs. two-tailed test For some tests you may choose between a one-tailed test versus a two-tailed test. The choice depends on the hypothesis you specified, i.e., whether you specified a directional or a non-directional hypotheses. In the example above, we used a non-directional hypothesis. That is, we stated that the mean is different from the comparison value \\(\\mu_0\\), but we did not state the direction of the effect. A directional hypothesis states the direction of the effect. For example, we might test whether the population mean is smaller than a comparison value: \\[ H_0: \\mu \\ge \\mu_0 \\\\ H_1: \\mu &lt; \\mu_0 \\] Similarly, we could test whether the population mean is larger than a comparison value: \\[ H_0: \\mu \\le \\mu_0 \\\\ H_1: \\mu &gt; \\mu_0 \\] Connected to the decision of how to phrase the hypotheses (directional vs. non-directional) is the choice of a one-tailed test versus a two-tailed test. Let’s first think about the meaning of a one-tailed test. Using a significance level of 0.05, a one-tailed test means that 5% of the total area under the probability distribution of our test statistic is located in one tail. Thus, under a one-tailed test, we test for the possibility of the relationship in one direction only, disregarding the possibility of a relationship in the other direction. In our example, a one-tailed test could test either if the mean listening time is significantly larger or smaller compared to the control condition, but not both. Depending on the direction, the mean listening time is significantly larger (smaller) if the test statistic is located in the top (bottom) 5% of its probability distribution. The following graph shows the critical values that our test statistic would need to surpass so that the difference between the population mean and the comparison value would be deemed statistically significant. It can be seen that under a one-sided test, the rejection region is at one end of the distribution or the other. In a two-sided test, the rejection region is split between the two tails. As a consequence, the critical value of the test statistic is smaller using a one-tailed test, meaning that it has more power to detect an effect. Having said that, in most applications, we would like to be able catch effects in both directions, simply because we can often not rule out that an effect might exist that is not in the hypothesized direction. For example, if we would conduct a one-tailed test for a mean larger than some specified value but the mean turns out to be substantially smaller, then testing a one-directional hypothesis ($H_0: _0 $) would not allow us to conclude that there is a significant effect because there is not rejection at this end of the distribution. 5.1.4 Summary As we have seen, the process of hypothesis testing consists of various steps: Formulate null and alternative hypotheses Select an appropriate test Choose the level of significance (\\(\\alpha\\)) Descriptive statistics and data visualization Conduct significance test Report results and draw a marketing conclusion In the following, we will go through the individual steps using examples for different tests. 5.2 One sample t-test The example we used in the introduction was an example of the one sample t-test and we computed all statistics by hand to explain the underlying intuition. When you conduct hypothesis tests using R, you do not need to calculate these statistics by hand, since there are build-in routines to conduct the steps for you. Let us use the same example again to see how you would conduct hypothesis tests in R. 1. Formulate null and alternative hypotheses The null hypothesis states that there is no difference between the true population mean \\(\\mu\\) and the hypothesized value (i.e., \\(10\\)), while the alternative hypothesis states the opposite: \\[ H_0: \\mu = 10 \\\\ H_1: \\mu \\neq 10 \\] 2. Select an appropriate test Because we would like to test if the mean of a variable is different from a specified threshold, the one-sample t-test is appropriate. The assumptions of the test are 1) that the variable is measured using an interval or ratio scale, and 2) that the sampling distribution is normal. Both assumptions are met since 1) listening time is a ratio scale, and 2) we deem the sample size (n = 50) large enough to assume a normal sampling distribution according to the central limit theorem. 3. Choose the level of significance We choose the conventional 5% significance level. 4. Descriptive statistics and data visualization Provide descriptive statistics using the stat.desc() function: library(pastecs) stat.desc(student_sample) ## nbr.val nbr.null nbr.na min max range ## 50.0000000 0.0000000 0.0000000 1.9365049 64.9843737 63.0478688 ## sum median mean SE.mean CI.mean.0.95 var ## 929.5971917 16.2246844 18.5919438 1.7744470 3.5658848 157.4331152 ## std.dev coef.var ## 12.5472354 0.6748749 From this, we can already see that the mean is different from the hypothesized value. The question however remains, whether this difference is significantly different, given the sample size and the variability in the data. Since we only have one continuous variable, we can visualize the distribution in a histogram. ggplot(data.frame(student_sample)) + geom_histogram(aes(x = student_sample), fill = &#39;white&#39;, color = &#39;black&#39;, bins = 20) + theme_bw() + labs(title = &quot;Distribution of values in the sample&quot;,x = &quot;Hours&quot;, y = &quot;Frequency&quot;) 5. Conduct significance test In the beginning of the chapter, we saw, how you could conduct significance test by hand. However, R has built-in routines that you can use to conduct the analyses. The t.test() function can be used to conduct the test. To test if the listening time among WU students was 10, you can use the following code: H_0 &lt;- 10 t.test(student_sample, mu = H_0, alternative = &#39;two.sided&#39;) ## ## One Sample t-test ## ## data: student_sample ## t = 4.842, df = 49, p-value = 0.00001327 ## alternative hypothesis: true mean is not equal to 10 ## 95 percent confidence interval: ## 15.02606 22.15783 ## sample estimates: ## mean of x ## 18.59194 Note that if you would have stated a directional hypothesis (i.e., the mean is either greater or smaller than 10 hours), you could easily amend the code to conduct a one sided test by changing the argument alternativefrom 'two.sided' to either 'less' or 'greater'. 6. Report results and draw a marketing conclusion Note that the results are the same as above, when we computed the test by hand. You could summarize the results as follows: On average, the listening times in our sample were different form 10 hours per month (Mean = 18.99 hours, SE = 1.78). This difference was significant t(49) = 5.058, p &lt; .05 (95% CI = [15.42; 22.56]). Based on this evidence, we can conclude that the mean in our sample is significantly lower compared to the hypothesized population mean of \\(10\\) hours, providing evidence against the null hypothesis. Note that in the reporting above, the number 49 in parenthesis refers to the degrees of freedom that are available from the output. 5.3 Comparing two means In the one-sample test above, we tested the hypothesis that the population mean has some specific value \\(\\mu_0\\) using data from only one sample. In marketing (as in many other disciplines), you will often be confronted with a situation where you wish to compare the means of two groups. For example, you may conduct an experiment and randomly split your sample into two groups, one of which receives a treatment (experimental group) while the other doesn’t (control group). In this case, the units (e.g., participants, products) in each group are different (‘between-subjects design’) and the samples are said to be independent. Hence, we would use a independent-means t-test. If you run an experiment with two experimental conditions and the same units (e.g., participants, products) were observed in both experimental conditions, the sample is said to be dependent in the sense that you have the same units in each group (‘within-subjects design’). In this case, we would need to conduct an dependent-means t-test. Both tests are described in the following sections, beginning with the independent-means t-test. 5.3.1 Independent-means t-test Using an independent-means t-test, we can compare the means of two possibly different populations. It is, for example, quite common for online companies to test new service features by running an experiment and randomly splitting their website visitors into two groups: one is exposed to the website with the new feature (experimental group) and the other group is not exposed to the new feature (control group). This is a typical A/B-Test scenario. As an example, imagine that a music streaming service would like to introduce a new playlist feature that let’s their users access playlists created by other users. The goal is to analyse how the new service feature impacts the listening time of users. The service randomly splits a representative subset of their users into two groups and collects data about their listening times over one month. Let’s create a data set to simulate such a scenario. set.seed(321) hours_population_1 &lt;- rgamma(25000, shape = 2, scale = 10) set.seed(12567) sample_1 &lt;- sample(1:25000, size = 98, replace = FALSE) sample_1_hours &lt;- hours_population_1[sample_1] sample_1_df &lt;- data.frame(hours = round(sample_1_hours,0), group = &quot;A&quot;) set.seed(321) hours_population_2 &lt;- rgamma(25000, shape = 2.5, scale = 11) set.seed(12567) sample_2 &lt;- sample(1:25000, size = 112, replace = FALSE) sample_2_hours &lt;- hours_population_2[sample_2] sample_2_df &lt;- data.frame(hours = round(sample_2_hours,0), group = &quot;B&quot;) hours_a_b &lt;- rbind(sample_1_df,sample_2_df) head(hours_a_b) This data set contains two variables: the variable hours indicates the music listening times (in hours) and the variable group indicates from which group the observation comes, where ‘A’ refers to the control group (with the standard service) and ‘B’ refers to the experimental group (with the new playlist feature). Let’s first look at the descriptive statistics by group using the describeBy function: library(psych) describeBy(hours_a_b$hours, hours_a_b$group) ## ## Descriptive statistics by group ## group: A ## vars n mean sd median trimmed mad min max range skew kurtosis se ## X1 1 98 18.11 12.1 15 16.88 10.38 2 65 63 1.08 1.21 1.22 ## ------------------------------------------------------------ ## group: B ## vars n mean sd median trimmed mad min max range skew kurtosis se ## X1 1 112 28.5 17.97 24.5 26.56 15.57 1 83 82 0.96 0.82 1.7 From this, we can already see that there is a difference in means between groups A and B. We can also see that the number of observations is different, as is the standard deviation. The question that we would like to answer is whether there is a significant difference in mean listening times between the groups. Remember that different users are contained in each group (‘between-subjects design’) and that the observations in one group are independent of the observations in the other group. Before we will see how you can easily conduct an independent-means t-test, let’s go over some theory first. 5.3.1.1 Theory As a starting point, let us label the unknown population mean of group A (control group) in our experiment \\(\\mu_1\\), and that of group B (experimental group) \\(\\mu_2\\). In this setting, the null hypothesis would state that the mean in group A is equal to the mean in group B: \\[ H_0: \\mu_1=\\mu_2 \\] This is equivalent to stating that the difference between the two groups (\\(\\delta\\)) is zero: \\[ H_0: \\mu_1 - \\mu_2=0=\\delta \\] That is, \\(\\delta\\) is the new unknown population parameter, so that the null and alternative hypothesis become: \\[ H_0: \\delta = 0 \\\\ H_1: \\delta \\ne 0 \\] Remember that we usually don’t have access to the entire population so that we can not observe \\(\\delta\\) and have to estimate is from a sample statistic, which we define as \\(d = \\bar x_1-\\bar x_2\\), i.e., the difference between the sample means from group a (\\(\\bar x_1\\)) and group b (\\(\\bar x_2\\)). But can we really estimate \\(d\\) from \\(\\delta\\)? Remember from the previous chapter, that we could estimate \\(\\mu\\) from \\(\\bar x\\), because if we (hypothetically) take a larger number of samples, the distribution of the means of these samples (the sampling distribution) will be normally distributed and its mean will be (in the limit) equal to the population mean. It turns out that we can use the same underlying logic here. The above samples were drawn from two different populations with \\(\\mu_1\\) and \\(\\mu_2\\). Let us compute the difference in means between these two populations: delta_pop &lt;- mean(hours_population_1)-mean(hours_population_2) delta_pop ## [1] -7.422855 This means that the true difference between the mean listening times of groups a and b is -7.42. Let us now repeat the exercise from the previous chapter: let us repeatedly draw a large number of \\(20,000\\) random samples of 100 users from each of these populations, compute the difference (i.e., \\(d\\), our estimate of \\(\\delta\\)), store the difference for each draw and create a histogram of \\(d\\). set.seed(321) hours_population_1 &lt;- rgamma(25000, shape = 2, scale = 10) hours_population_2 &lt;- rgamma(25000, shape = 2.5, scale = 11) samples &lt;- 20000 mean_delta &lt;- matrix(NA, nrow = samples) for (i in 1:samples){ student_sample &lt;- sample(1:25000, size = 100, replace = FALSE) mean_delta[i,] &lt;- mean(hours_population_1[student_sample])-mean(hours_population_2[student_sample]) } ggplot(data.frame(mean_delta)) + geom_histogram(aes(x = mean_delta), bins = 30, fill=&#39;white&#39;, color=&#39;black&#39;) + theme_bw() + theme(legend.title = element_blank()) + geom_vline(aes(xintercept = mean(mean_delta)), size=1) + xlab(&quot;d&quot;) + ggtitle(TeX(sprintf(&quot;%d samples; $d_{\\\\bar{x}}$ = %.2f&quot;,samples, round(mean(mean_delta),2)))) This gives us the sampling distribution of the mean differences between the samples. You will notice that this distribution follows a normal distribution and is centered around the true difference between the populations. This means that, on average, the difference between two sample means \\(d\\) is a good estimate of \\(\\delta\\). In our example, the difference between \\(\\bar x_1\\) and \\(\\bar x_2\\) is: mean_x1 &lt;- mean(hours_a_b[hours_a_b$group==&quot;A&quot;,&quot;hours&quot;]) mean_x1 ## [1] 18.11224 mean_x2 &lt;- mean(hours_a_b[hours_a_b$group==&quot;B&quot;,&quot;hours&quot;]) mean_x2 ## [1] 28.5 d &lt;- mean_x1-mean_x2 d ## [1] -10.38776 Now that we have \\(d\\) as an estimate of \\(\\delta\\), how can we find out if the observed difference is significantly different from the null hypothesis (i.e., \\(\\delta = 0\\))? Recall from the previous section, that the standard deviation of the sampling distribution \\(\\sigma_{\\bar x}\\) (i.e., the standard error) gives us indication about the precision of our estimate. Further recall that the standard error can be calculated as \\(\\sigma_{\\bar x}={\\sigma \\over \\sqrt{n}}\\). So how can we calculate the standard error of the difference between two population means? According to the variance sum law, to find the variance of the sampling distribution of differences, we merely need to add together the variances of the sampling distributions of the two populations that we are comparing. To find the standard error, we only need to take the square root of the variance (because the standard error is the standard deviation of the sampling distribution and the standard deviation is the square root of the variance), so that we get: \\[ \\sigma_{\\bar x_1-\\bar x_2} = \\sqrt{{\\sigma_1^2 \\over n_1}+{\\sigma_2^2 \\over n_2}} \\] But recall that we don’t actually know the true population standard deviation, so we use \\(SE_{\\bar x_1-\\bar x_2}\\) as an estimate of \\(\\sigma_{\\bar x_1-\\bar x_2}\\): \\[ SE_{\\bar x_1-\\bar x_2} = \\sqrt{{s_1^2 \\over n_1}+{s_2^2 \\over n_2}} \\] Hence, for our example, we can calculate the standard error as follows: n1 &lt;- 98 n2 &lt;- 112 s1 &lt;- var(hours_a_b[hours_a_b$group==&quot;A&quot;,&quot;hours&quot;]) s1 ## [1] 146.4924 s2 &lt;- var(hours_a_b[hours_a_b$group==&quot;B&quot;,&quot;hours&quot;]) s2 ## [1] 322.9189 SE_x1_x2 &lt;- sqrt(s1/n1+s2/n2) SE_x1_x2 ## [1] 2.092373 Recall from above that we can calculate the t-statistic as: \\[ t= {\\bar x - \\mu_0 \\over {s \\over \\sqrt{n}}} \\] Exchanging \\(\\bar x\\) for \\(d\\), we get \\[ t= {(\\bar{x}_1 - \\bar{x}_2) - (\\mu_1 - \\mu_2) \\over {\\sqrt{{s_1^2 \\over n_1}+{s_2^2 \\over n_2}}}} \\] Note that according to our hypothesis \\(\\mu_1-\\mu_2=0\\), so that we can calculate the t-statistic as: t_score &lt;- d/SE_x1_x2 t_score ## [1] -4.964581 Following the example of our one sample t-test above, we would now need to compare this calculated test statistic to a critical value in order to assess if \\(d\\) is sufficiently far away from the null hypothesis to be statistically significant. To do this, we would need to know the exact t-distribution, which depends on the degrees of freedom. The problem is that deriving the degrees of freedom in this case is not that obvious. If we were willing to assume that \\(\\sigma_1=\\sigma_2\\), the correct t-distribution has \\(n_1 -1 + n_2-1\\) degrees of freedom (i.e., the sum of the degrees of freedom of the two samples). However, because in real life we don not know if \\(\\sigma_1=\\sigma_2\\), we need to account for this additional uncertainty. We will not go into detail here, but R automatically uses a sophisticated approach to correct the degrees of freedom called the Welch’s correction, as we will see in the subsequent application. 5.3.1.2 Application The section above explained the theory behind the independent-means t-test and showed how to compute the statistics manually. Obviously you don’t have to compute these statistics by hand in this section shows you how to conduct an independent-means t-test in R using the example from above. 1. Formulate null and alternative hypotheses We wish to analyze whether there is a significant difference in music listening times between groups A and B. So our null hypothesis is that the means from the two populations are the same (i.e., there is no difference), while the alternative hypothesis states the opposite: \\[ H_0: \\mu_1=\\mu_2\\\\ H_1: \\mu_1 \\ne \\mu_2 \\] 2. Select an appropriate test Since we have a ratio scaled variable (i.e., listening times) and two independent groups, where the mean of one sample is independent of the group of the second sample (i.e., the groups contain different units), the independent-means t-test is appropriate. 3. Choose the level of significance We choose the conventional 5% significance level. 4. Descriptive statistics and data visualization We can compute the descriptive statistics for each group separately, using the describeBy() function: library(psych) describeBy(hours_a_b$hours, hours_a_b$group) ## ## Descriptive statistics by group ## group: A ## vars n mean sd median trimmed mad min max range skew kurtosis se ## X1 1 98 18.11 12.1 15 16.88 10.38 2 65 63 1.08 1.21 1.22 ## ------------------------------------------------------------ ## group: B ## vars n mean sd median trimmed mad min max range skew kurtosis se ## X1 1 112 28.5 17.97 24.5 26.56 15.57 1 83 82 0.96 0.82 1.7 This already shows us that the mean between groups A and B are different. We can visualize the data using a plot of means, boxplot, and a histogram. ggplot(hours_a_b,aes(group, hours)) + geom_bar(stat = &quot;summary&quot;, color = &quot;black&quot;, fill = &quot;white&quot;, width = 0.7) + geom_pointrange(stat = &quot;summary&quot;) + labs(x = &quot;Group&quot;, y = &quot;Listening time (hours)&quot;) + ggtitle(&quot;Means and standard errors of listening times&quot;) + theme_bw() ggplot(hours_a_b, aes(x = group, y = hours)) + geom_boxplot() + labs(x = &quot;Group&quot;, y = &quot;Listening time (hours)&quot;) + ggtitle(&quot;Boxplot of listening times&quot;) + theme_bw() ggplot(hours_a_b,aes(hours)) + geom_histogram(col = &quot;black&quot;, fill = &quot;darkblue&quot;) + labs(x = &quot;Listening time (hours)&quot;, y = &quot;Frequency&quot;) + ggtitle(&quot;Histogram of listening times&quot;) + facet_wrap(~group) + theme_bw() 5. Conduct significance test To conduct the independent means t-test, we can use the t.test() function: t.test(hours ~ group, data = hours_a_b, mu = 0, alternative = &quot;two.sided&quot;, conf.level = 0.95, var.equal = FALSE) ## ## Welch Two Sample t-test ## ## data: hours by group ## t = -4.9646, df = 195.73, p-value = 0.000001494 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -14.514246 -6.261264 ## sample estimates: ## mean in group A mean in group B ## 18.11224 28.50000 6. Report results and draw a marketing conclusion The results showed that listening times were higher in the experimental group (Mean = 26.43, SE = 1.54) compared to the control group (Mean = 20.91, SE = 1.37). This means that the listening times were 10.39 hours higher on average in the experimental group, compared to the control group. An independent-means t-test showed that this difference is significant t(207) = 2.684, p &lt; .05 (95% CI = [1.46,9.58]). 5.3.2 Dependent-means t-test While the independent-means t-test is used when different units (e.g., participants, products) were assigned to the different condition, the dependent-means t-test is used when there are two experimental conditions and the same units (e.g., participants, products) were observed in both experimental conditions. Imagine, for example, a slightly different experimental setup for the above experiment. Imagine that we do not assign different users to the groups, but that a sample of 100 users gets to use the music streaming service with the new feature for one month and we compare the music listening times of these users during the month of the experiment with the listening time in the previous month. Let us generate data for this example: set.seed(321) hours_population_1 &lt;- rgamma(25000, shape = 2, scale = 10) set.seed(12567) sample_1 &lt;- sample(1:25000, size = 100, replace = FALSE) sample_1_hours &lt;- hours_population_1[sample_1] set.seed(321) hours_population_2 &lt;- rgamma(25000, shape = 2.5, scale = 11) set.seed(12567) sample_2 &lt;- sample(1:25000, size = 100, replace = FALSE) sample_2_hours &lt;- hours_population_2[sample_2] hours_a_b_paired &lt;- data.frame(hours_a = round(sample_1_hours,0),hours_b = round(sample_2_hours,0)) head(hours_a_b_paired) Note that the data set has almost the same structure as before only that we know have two variables representing the listening times of each user in the month before the experiment and during the month of the experiment when the new feature was tested. 5.3.2.1 Theory In this case, we want to test the hypothesis that there is no difference in mean the mean listening times between the two months. This can be expressed as follows: \\[ H_0: \\mu_D = 0 \\\\ \\] Note that the hypothesis only refers to one population, since both observations come from the same units (i.e., users). To use consistent notation, we replace \\(\\mu_D\\) with \\(\\delta\\) and get: \\[ H_0: \\delta = 0 \\\\ H_1: \\delta \\neq 0 \\] where \\(\\delta\\) denotes the difference between the observed listening times from the two consecutive months of the same users. As is the previous example, since we do not observe the entire population, we estimate \\(\\delta\\) based on the sample using \\(d\\), which is the difference in mean listening time between the two months for our sample. Note that we assume that everything else (e.g., number of new releases) remained constant over the two month to keep it simple. We can show as above that the sampling distribution follows a normal distribution with a mean that is (in the limit) the same as the population mean. This means, again, that the difference in sample means is a good estimate for the difference in population means. Let’s compute a new variable \\(d\\), which is the difference between two month. hours_a_b_paired$d &lt;- hours_a_b_paired$hours_a - hours_a_b_paired$hours_b head(hours_a_b_paired) Note that we now have a new variable, which is the difference in listening times (in hours) between the two months. The mean of this difference is: mean_d &lt;- mean(hours_a_b_paired$d) mean_d ## [1] -11.65 Again, we use \\(SE_{\\bar x}\\) as an estimate of \\(\\sigma_{\\bar x}\\): \\[ SE_{\\bar d}={s \\over \\sqrt{n}} \\] Hence, we can compute the standard error as: n &lt;- nrow(hours_a_b_paired) SE_d &lt;- sd(hours_a_b_paired$d)/sqrt(n) SE_d ## [1] 2.151503 The test statistic is therefore: \\[ t = {\\bar d- \\mu_0 \\over SE_{\\bar d}} \\] on 99 (i.e., n-1) degrees of freedom. Now we can compute the t-statistic as follows: t_score &lt;- mean_d/SE_d t_score ## [1] -5.41482 qt(0.975,df=99) ## [1] 1.984217 Note that in the case of the dependent-means t-test, we only base our hypothesis on one population and hence there is only one population variance. This is because in the dependent sample test, the observations come from the same observational units (i.e., users). Hence, there is no unsystematic variation due to potential differences between users that were assigned to the experimental groups. This means that the influence of unobserved factors (unsystematic variation) relative to the variation due to the experimental manipulation (systematic variation) is not as strong in the dependent-means test compared to the independent-means test and we don’t need to correct for differences in the population variances. 5.3.2.2 Application Again, we don’t have to compute all this by hand since the t.test(...) function can be used to do it for us. Now we have to use the argument paired=TRUE to let R know that we are working with dependent observations. 1. Formulate null and alternative hypotheses We would like to the test if there is a difference in music listening times between the two consecutive months, so our null hypothesis is that there is no difference, while the alternative hypothesis states the opposite: \\[ H_0: \\mu_D = 0 \\\\ H_0: \\mu_D \\ne 0 \\] 2. Select an appropriate test Since we have a ratio scaled variable (i.e., listening times) and two observations of the same group of users (i.e., the groups contain the same units), the dependent-means t-test is appropriate. 3. Choose the level of significance We choose the conventional 5% significance level. 4. Descriptive statistics and data visualization We can compute the descriptive statistics for each month separately, using the describe() function: library(psych) describe(hours_a_b_paired) ## hours_a_b_paired ## ## 3 Variables 100 Observations ## -------------------------------------------------------------------------------- ## hours_a ## n missing distinct Info Mean Gmd .05 .10 ## 100 0 38 0.998 17.93 13.16 3.00 5.90 ## .25 .50 .75 .90 .95 ## 8.00 15.00 25.25 34.30 39.05 ## ## lowest : 2 3 4 5 6, highest: 40 41 46 50 65 ## -------------------------------------------------------------------------------- ## hours_b ## n missing distinct Info Mean Gmd .05 .10 ## 100 0 48 0.999 29.58 20.3 5.95 8.00 ## .25 .50 .75 .90 .95 ## 15.75 25.00 40.00 54.20 63.30 ## ## lowest : 3 4 5 6 8, highest: 63 69 78 81 83 ## -------------------------------------------------------------------------------- ## d ## n missing distinct Info Mean Gmd .05 .10 ## 100 0 52 0.999 -11.65 23.75 -54.15 -35.20 ## .25 .50 .75 .90 .95 ## -23.25 -9.00 2.00 12.00 21.05 ## ## lowest : -76 -64 -63 -60 -57, highest: 22 23 24 34 51 ## -------------------------------------------------------------------------------- This already shows us that the mean between the two months are different. We can visiualize the data using a plot of means, boxplot, and a histogram. To plot the data, we need to do some restructuring first, since the variables are now stored in two different columns (“hours_a” and “hours_b”). This is also known as the “wide” format. To plot the data we need all observations to be stored in one variable. This is also known as the “long” format. We can use the melt(...) function from the reshape2package to “melt” the two variable into one column to plot the data. library(reshape2) hours_a_b_paired_long &lt;- melt(hours_a_b_paired[, c(&quot;hours_a&quot;, &quot;hours_b&quot;)]) names(hours_a_b_paired_long) &lt;- c(&quot;group&quot;,&quot;hours&quot;) head(hours_a_b_paired_long) Now we are ready to plot the data: ggplot(hours_a_b_paired_long,aes(group, hours)) + geom_bar(stat = &quot;summary&quot;, color = &quot;black&quot;, fill = &quot;white&quot;, width = 0.7) + geom_pointrange(stat = &quot;summary&quot;) + labs(x = &quot;Group&quot;, y = &quot;Listening time (hours)&quot;) + ggtitle(&quot;Means and standard errors of listining times&quot;) + theme_bw() ggplot(hours_a_b_paired_long, aes(x = group, y = hours)) + geom_boxplot() + labs(x = &quot;Group&quot;, y = &quot;Listening time (hours)&quot;) + ggtitle(&quot;Boxplot of listening times&quot;) + theme_bw() ggplot(hours_a_b_paired_long,aes(hours)) + geom_histogram(col = &quot;black&quot;, fill = &quot;darkblue&quot;) + labs(x = &quot;Listening time (hours)&quot;, y = &quot;Frequency&quot;) + ggtitle(&quot;Histogram of listening times&quot;) + facet_wrap(~group) + theme_bw() 5. Conduct significance test To conduct the independent means t-test, we can use the t.test() function with the argument paired = TRUE: t.test(hours_a_b_paired$hours_a, hours_a_b_paired$hours_b, mu = 0, alternative = &quot;two.sided&quot;, conf.level = 0.95, paired=TRUE) ## ## Paired t-test ## ## data: hours_a_b_paired$hours_a and hours_a_b_paired$hours_b ## t = -5.4148, df = 99, p-value = 0.00000043 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -15.919048 -7.380952 ## sample estimates: ## mean of the differences ## -11.65 6. Report results and draw a marketing conclusion On average, the same users used the service more when it included the new feature (M = 25.96, SE = 1.68) compared to the service without the feature (M = 20.99, SE = 1.34). This difference was significant t(99) = 2.3781, p &lt; .05 (95% CI = [0.82, 9.12]). 5.3.3 Further considerations 5.3.3.1 Type I and Type II Errors When choosing the level of significance (\\(\\alpha\\)). It is important to note that the choice of the significance level affects the type 1 and type 2 error: Type I error: When we believe there is a genuine effect in our population, when in fact there isn’t. Probability of type I error (\\(\\alpha\\)) = level of significance. Type II error: When we believe that there is no effect in the population, when in fact there is. This following table shows the possible outcomes of a test (retain vs. reject \\(H_0\\)), depending on whether \\(H_0\\) is true or false in reality.   Retain H0 Reject H0 H0 is true Correct decision:1-α (probability of correct retention); Type 1 error: α (level of significance) H0 is false Type 2 error:β (type 2 error rate) Correct decision:1-β (power of the test) 5.3.3.2 Significance level, sample size, power, and effect size When you plan to conduct an experiment, there are some factors that are under direct control of the researcher: Significance level (\\(\\alpha\\)): The probability of finding an effect that does not genuinely exist. Sample size (n): The number of observations in each group of the experimental design. Unlike α and n, which are specified by the researcher, the magnitude of β depends on the actual value of the population parameter. In addition, β is influenced by the effect size (e.g., Cohen’s d), which can be used to determine a standardized measure of the magnitude of an observed effect. The following parameters are affected more indirectly: Power (1-β): The probability of finding an effect that does genuinely exists. Effect size (d): Standardized measure of the effect size under the alternate hypothesis. Although β is unknown, it is related to α. For example, if we would like to be absolutely sure that we do not falsely identify an effect which does not exist (i.e., make a type I error), this means that the probability of identifying an effect that does exist (i.e., 1-β) decreases and vice versa. Thus, an extremely low value of α (e.g., α = 0.0001) will result in intolerably high β errors. A common approach is to set α=0.05 and β=0.80. Unlike the t-value of our test, the effect size (d) is unaffected by the sample size and can be categorized as follows (see Cohen, J. 1988): 0.2 (small effect) 0.5 (medium effect) 0.8 (large effect) In order to test more subtle effects (smaller effect sizes), you need a larger sample size compared to the test of more obvious effects. In this paper, you can find a list of examples for different effect sizes and the number of observations you need to reliably find an effect of that magnitude. Although the exact effect size is unknown before the experiment, you might be able to make a guess about the effect size (e.g., based on previous studies). If you wish to obtain a standardized measure of the effect, you may compute the effect size (Cohen’s d) using the cohensD() function from the lsr package. Using the examples from the independent-means t-test above, we would use: library(lsr) cohensD(hours ~ group, data = hours_a_b) ## [1] 0.6696301 According to the thresholds defined above, this effect would be judged to be a small-medium effect. For the dependent-means t-test, we would use: cohensD(hours_a_b_paired$hours_a, hours_a_b_paired$hours_b, method=&quot;paired&quot;) ## [1] 0.541482 According to the thresholds defined above, this effect would also be judged to be a small-medium effect. When constructing an experimental design, your goal should be to maximize the power of the test while maintaining an acceptable significance level and keeping the sample as small as possible. To achieve this goal, you may use the pwr package, which let’s you compute n, d, alpha, and power. You only need to specify three of the four input variables to get the fourth. For example, what sample size do we need (per group) to identify an effect with d = 0.6, α = 0.05, and power = 0.8: library(pwr) pwr.t.test(d = 0.6, sig.level = 0.05, power = 0.8, type = c(&quot;two.sample&quot;), alternative = c(&quot;two.sided&quot;)) ## ## Two-sample t test power calculation ## ## n = 44.58577 ## d = 0.6 ## sig.level = 0.05 ## power = 0.8 ## alternative = two.sided ## ## NOTE: n is number in *each* group Or we could ask, what is the power of our test with 51 observations in each group, d = 0.6, and α = 0.05: pwr.t.test(n = 51, d = 0.6, sig.level = 0.05, type = c(&quot;two.sample&quot;), alternative = c(&quot;two.sided&quot;)) ## ## Two-sample t test power calculation ## ## n = 51 ## d = 0.6 ## sig.level = 0.05 ## power = 0.850985 ## alternative = two.sided ## ## NOTE: n is number in *each* group 5.3.3.3 P-values, stopping rules and p-hacking From my experience, students tend to place a lot of weight on p-values when interpreting their research findings. It is therefore important to note some points that hopefully help to put the meaning of a “significant” vs. “insignificant” test result into perspective. Significant result Even if the probability of the effect being a chance result is small (e.g., less than .05) it doesn’t necessarily mean that the effect is important. Very small and unimportant effects can turn out to be statistically significant if the sample size is large enough. Insignificant result If the probability of the effect occurring by chance is large (greater than .05), the alternative hypothesis is rejected. However, this does not mean that the null hypothesis is true. Although an effect might not be large enough to be anything other than a chance finding, it doesn’t mean that the effect is zero. In fact, two random samples will always have slightly different means that would deemed to be statistically significant if the samples were large enough. Thus, you should not base your research conclusion on p-values alone! It is also crucial to determine the sample size before you run the experiment or before you start your analysis. Why? Consider the following example: You run an experiment After each respondent you analyze the data and look at the mean difference between the two groups with a t-test You stop when you have a significant effect This is called p-hacking and should be avoided at all costs. Assuming that both groups come from the same population (i.e., there is no difference in the means): What is the likelihood that the result will be significant at some point? In other words, what is the likelihood that you will draw the wrong conclusion from your data that there is an effect, while there is none? This is shown in the following graph using simulated data - the color red indicates significant test results that arise although there is no effect (i.e., false positives). Figure 5.1: p-hacking (red indicates false positives) 5.4 Comparing several means This chapter is primarily based on Field, A., Miles J., &amp; Field, Z. (2012): Discovering Statistics Using R. Sage Publications, chapters 10 &amp; 12. You can download the corresponding R-Code here 5.4.1 Introduction In the previous section we learned how to compare means using a t-test. The t-test has some limitations since it only lets you compare 2 means and you can only use it with one independent variable. However, often we would like to compare means from 3 or more groups. In addition, there may be instances in which you manipulate more than one independent variable. For these applications, ANOVA (ANalysis Of VAriance) can be used. Hence, to conduct ANOVA you need: A metric dependent variable (i.e., measured using an interval or ratio scale) One or more non-metric (categorical) independent variables (also called factors) A treatment is a particular combination of factor levels, or categories. One-way ANOVA is used when there is only one categorical variable (factor). In this case, a treatment is the same as a factor level. N-way ANOVA is used with two or more factors. Note that we are only going to talk about a single independent variable in the context of ANOVA. If you have multiple independent variables please refere to the chapter on Regression. Let’s use an example to see how ANOVA works. Similar to the previous example it is also imaginable that the music streaming service experiments with a recommendation system for user created playlists. We now have three groups, the control group “A” with the current system, treatment group “B” who have access to playlists created by other users but are not shown recommendations and treatment group “C” who are shown recommendations for user created playlists. As always, we load and inspect the data first: set.seed(321) hours_population_1 &lt;- rnorm(25000, 15, 5) set.seed(125671) sample_1 &lt;- sample(1:25000, size = 100, replace = FALSE) sample_1_hours &lt;- hours_population_1[sample_1] sample_1_df &lt;- data.frame(hours = round(sample_1_hours,0), group = &quot;A&quot;) sample_1_df$index &lt;- 1:100 set.seed(321) hours_population_2 &lt;- rnorm(25000, 25, 6) set.seed(125672) sample_2 &lt;- sample(1:25000, size = 100, replace = FALSE) sample_2_hours &lt;- hours_population_2[sample_2] sample_2_df &lt;- data.frame(hours = round(sample_2_hours,0), group = &quot;B&quot;) sample_2_df$index &lt;- 1:100 set.seed(321) hours_population_3 &lt;- rnorm(25000, 35, 6) set.seed(125678) sample_3 &lt;- sample(1:25000, size = 100, replace = FALSE) sample_3_hours &lt;- hours_population_3[sample_3] sample_3_df &lt;- data.frame(hours = round(sample_3_hours,0), group = &quot;C&quot;) sample_3_df$index &lt;- 1:100 hours_abc &lt;- rbind(sample_1_df, sample_2_df, sample_3_df) head(hours_abc) tail(hours_abc) The null hypothesis, typically, is that all means are equal (non-directional hypothesis). Hence, in our case: \\[H_0: \\mu_1 = \\mu_2 = \\mu_3\\] The alternative hypothesis is simply that the means are not all equal, i.e., \\[H_1: \\textrm{Means are not all equal}\\] If you wanted to put this in mathematical notation, you could also write: \\[H_1: \\exists {i,j}: {\\mu_i \\ne \\mu_j} \\] To get a first impression if there are any differences in listening times across the experimental groups, we use the describeBy(...) function from the psych package: library(psych) describeBy(hours_abc$hours, hours_abc$group) #inspect data ## ## Descriptive statistics by group ## group: A ## vars n mean sd median trimmed mad min max range skew kurtosis se ## X1 1 100 14.34 4.62 14 14.36 4.45 3 25 22 -0.03 -0.32 0.46 ## ------------------------------------------------------------ ## group: B ## vars n mean sd median trimmed mad min max range skew kurtosis se ## X1 1 100 24.7 5.81 25 24.79 5.93 12 42 30 0.05 0.2 0.58 ## ------------------------------------------------------------ ## group: C ## vars n mean sd median trimmed mad min max range skew kurtosis se ## X1 1 100 34.99 6.42 34.5 35.02 6.67 17 50 33 -0.05 -0.23 0.64 In addition, you should visualize the data using appropriate plots: #Plot of means library(plyr) library(ggplot2) ggplot(hours_abc, aes(group, hours)) + stat_summary(fun.y = mean, geom = &quot;bar&quot;, fill = &quot;White&quot;, colour = &quot;Black&quot;) + stat_summary(fun.data = mean_cl_normal, geom = &quot;pointrange&quot;) + labs(x = &quot;Experimental group &quot;, y = &quot;Listening time&quot;) + theme_bw() Figure 5.2: Plot of means Note that ANOVA is an omnibus test, which means that we test for an overall difference between groups. Hence, the test will only tell you if the group means are different, but it won’t tell you exactly which groups are different from another. So why don’t we then just conduct a series of t-tests for all combinations of groups (i.e., A vs. B, A vs. C, B vs. C)? The reason is that if we assume each test to be independent, then there is a 5% probability of falsely rejecting the null hypothesis (Type I error) for each test. In our case: A vs. B (α = 0.05) A vs. C (α = 0.05) B vs. C (α = 0.05) This means that the overall probability of making a Type I error is 1-(0.953) = 0.143, since the probability of no Type I error is 0.95 for each of the three tests. Consequently, the Type I error probability would be 14.3%, which is above the conventional standard of 5%. This is also known as the family-wise or experiment-wise error. 5.4.2 Decomposing variance The basic concept underlying ANOVA is the decomposition of the variance in the data. There are three variance components which we need to consider: We calculate how much variability there is between scores: Total sum of squares (SST) We then calculate how much of this variability can be explained by the model we fit to the data (i.e., how much variability is due to the experimental manipulation): Model sum of squares (SSM) … and how much cannot be explained (i.e., how much variability is due to individual differences in performance): Residual sum of squares (SSR) The following figure shows the different variance components using a generalized data matrix: Decomposing variance The total variation is determined by the variation between the categories (due to our experimental manipulation) and the within-category variation that is due to extraneous factors (e.g., promotion of artists on a social network): \\[SS_T= SS_M+SS_R\\] To get a better feeling how this relates to our data set, we can look at the data in a slightly different way. Specifically, we can use the dcast(...) function from the reshape2 package to convert the data to wide format: library(reshape2) dcast(hours_abc, index ~ group, value.var = &quot;hours&quot;) In this example, X1 from the generalized data matrix above would refer to the factor level “A”, X2 to the level “B”, and X3 to the level “C”. Y11 refers to the first data point in the first row (i.e., “13”), Y12 to the second data point in the first row (i.e., “21”), etc.. The grand mean (\\(\\overline{Y}\\)) and the category means (\\(\\overline{Y}_c\\)) can be easily computed: mean(hours_abc$hours) #grand mean ## [1] 24.67667 by(hours_abc$hours, hours_abc$group, mean) #category mean ## hours_abc$group: A ## [1] 14.34 ## ------------------------------------------------------------ ## hours_abc$group: B ## [1] 24.7 ## ------------------------------------------------------------ ## hours_abc$group: C ## [1] 34.99 To see how each variance component can be derived, let’s look at the data again. The following graph shows the individual observations by experimental group: Figure 5.3: Sum of Squares 5.4.2.1 Total sum of squares To compute the total variation in the data, we consider the difference between each observation and the grand mean. The grand mean is the mean over all observations in the data set. The vertical lines in the following plot measure how far each observation is away from the grand mean: Figure 5.4: Total Sum of Squares The formal representation of the total sum of squares (SST) is: \\[ SS_T= \\sum_{i=1}^{N} (Y_i-\\bar{Y})^2 \\] This means that we need to subtract the grand mean from each individual data point, square the difference, and sum up over all the squared differences. Thus, in our example, the total sum of squares can be calculated as: \\[ \\begin{align} SS_T =&amp;(13−24.67)^2 + (14−24.67)^2 + … + (2−24.67)^2\\\\ &amp;+(21−24.67)^2 + (18-24.67)^2 + … + (17−24.67)^2\\\\ &amp;+(30−24.67)^2 + (37−24.67)^2 + … + (28−24.67)^2\\\\ &amp;=30855.64 \\end{align} \\] You could also compute this in R using: SST &lt;- sum((hours_abc$hours - mean(hours_abc$hours))^2) SST ## [1] 30855.64 For the subsequent analyses, it is important to understand the concept behind the degrees of freedom. Remember that in order to estimate a population value from a sample, we need to hold something in the population constant. In ANOVA, the df are generally one less than the number of values used to calculate the SS. For example, when we estimate the population mean from a sample, we assume that the sample mean is equal to the population mean. Then, in order to estimate the population mean from the sample, all but one scores are free to vary and the remaining score needs to be the value that keeps the population mean constant. In our example, we used all 300 observations to calculate the sum of square, so the total degrees of freedom (dfT) are: \\[\\begin{equation} \\begin{split} df_T = N-1=300-1=299 \\end{split} \\tag{5.1} \\end{equation}\\] 5.4.2.2 Model sum of squares Now we know that there are 26646.33 units of total variation in our data. Next, we compute how much of the total variation can be explained by the differences between groups (i.e., our experimental manipulation). To compute the explained variation in the data, we consider the difference between the values predicted by our model for each observation (i.e., the group mean) and the grand mean. The group mean refers to the mean value within the experimental group. The vertical lines in the following plot measure how far the predicted value for each observation (i.e., the group mean) is away from the grand mean: Figure 5.5: Model Sum of Squares The formal representation of the model sum of squares (SSM) is: \\[ SS_M= \\sum_{j=1}^{c} n_j(\\bar{Y}_j-\\bar{Y})^2 \\] where c denotes the number of categories (experimental groups). This means that we need to subtract the grand mean from each group mean, square the difference, and sum up over all the squared differences. Thus, in our example, the model sum of squares can be calculated as: \\[ \\begin{align} SS_M &amp;= 100*(15.47−24.67)^2 + 100*(24.88−24.67)^2 + 100*(33.66−24.67)^2 \\\\ &amp;= 21321.21 \\end{align} \\] You could also compute this manually in R using: SSM &lt;- sum(100*(by(hours_abc$hours, hours_abc$group, mean) - mean(hours_abc$hours))^2) SSM ## [1] 21321.21 In this case, we used the three group means to calculate the sum of squares, so the model degrees of freedom (dfM) are: \\[ df_M= c-1=3-1=2 \\] 5.4.2.3 Residual sum of squares Lastly, we calculate the amount of variation that cannot be explained by our model. In ANOVA, this is the sum of squared distances between what the model predicts for each data point (i.e., the group means) and the observed values. In other words, this refers to the amount of variation that is caused by extraneous factors, such as differences between product characteristics of the products in the different experimental groups. The vertical lines in the following plot measure how far each observation is away from the group mean: Figure 5.6: Residual Sum of Squares The formal representation of the residual sum of squares (SSR) is: \\[ SS_R= \\sum_{j=1}^{c} \\sum_{i=1}^{n} ({Y}_{ij}-\\bar{Y}_{j})^2 \\] This means that we need to subtract the group mean from each individual observation, square the difference, and sum up over all the squared differences. Thus, in our example, the model sum of squares can be calculated as: \\[ \\begin{align} SS_R =&amp; (13−14.34)^2 + (14−14.34)^2 + … + (2−14.34)^2 \\\\ +&amp;(21−24.7)^2 + (18−24.7)^2 + … + (17−24.7)^2 \\\\ +&amp; (30−34.99)^2 + (37−34.99)^2 + … + (28−34.99)^2 \\\\ =&amp; 9534.43 \\end{align} \\] You could also compute this in R using: SSR &lt;- sum((hours_abc$hours - rep(by(hours_abc$hours, hours_abc$group, mean), each = 100))^2) SSR ## [1] 9534.43 In this case, we used the 10 values for each of the SS for each group, so the residual degrees of freedom (dfR) are: \\[ \\begin{align} df_R=&amp; (n_1-1)+(n_2-1)+(n_3-1) \\\\ =&amp;(100-1)+(100-1)+(100-1)=297 \\end{align} \\] 5.4.2.4 Effect strength Once you have computed the different sum of squares, you can investigate the effect strength. \\(\\eta^2\\) is a measure of the variation in Y that is explained by X: \\[ \\eta^2= \\frac{SS_M}{SS_T}=\\frac{21321.21}{30855.64}=0.69 \\] To compute this in R: eta &lt;- SSM/SST eta ## [1] 0.6909988 The statistic can only take values between 0 and 1. It is equal to 0 when all the category means are equal, indicating that X has no effect on Y. In contrast, it has a value of 1 when there is no variability within each category of X but there is some variability between categories. 5.4.2.5 Test of significance How can we determine whether the effect of X on Y is significant? First, we calculate the fit of the most basic model (i.e., the grand mean) Then, we calculate the fit of the “best” model (i.e., the group means) A good model should fit the data significantly better than the basic model The F-statistic or F-ratio compares the amount of systematic variance in the data to the amount of unsystematic variance The F-statistic uses the ratio of mean square related to X (explained variation) and the mean square related to the error (unexplained variation): \\(\\frac{SS_M}{SS_R}\\) However, since these are summed values, their magnitude is influenced by the number of scores that were summed. For example, to calculate SSM we only used the sum of 3 values (the group means), while we used 30 and 27 values to calculate SST and SSR, respectively. Thus, we calculate the average sum of squares (“mean square”) to compare the average amount of systematic vs. unsystematic variation by dividing the SS values by the degrees of freedom associated with the respective statistic. Mean square due to X: \\[ MS_M= \\frac{SS_M}{df_M}=\\frac{SS_M}{c-1}=\\frac{21321.21}{(3-1)} \\] Mean square due to error: \\[ MS_R= \\frac{SS_R}{df_R}=\\frac{SS_R}{N-c}=\\frac{9534.43}{(300-3)} \\] Now, we compare the amount of variability explained by the model (experiment), to the error in the model (variation due to extraneous variables). If the model explains more variability than it can’t explain, then the experimental manipulation has had a significant effect on the outcome (DV). The F-radio can be derived as follows: \\[ F= \\frac{MS_M}{MS_R}=\\frac{\\frac{SS_M}{c-1}}{\\frac{SS_R}{N-c}}=\\frac{\\frac{21321.21}{(3-1)}}{\\frac{9534.43}{(300-3)}}=332.08 \\] You can easily compute this in R: f_ratio &lt;- (SSM/2)/(SSR/297) f_ratio ## [1] 332.0806 This statistic follows the F distribution with (m = c – 1) and (n = N – c) degrees of freedom. This means that, like the \\(\\chi^2\\) distribution, the shape of the F-distribution depends on the degrees of freedom. In this case, the shape depends on the degrees of freedom associated with the numerator and denominator used to compute the F-ratio. The following figure shows the shape of the F-distribution for different degrees of freedom: The F distribution The outcome of the test is one of the following: If the null hypothesis of equal category means is not rejected, then the independent variable does not have a significant effect on the dependent variable If the null hypothesis is rejected, then the effect of the independent variable is significant For 2 and 297 degrees of freedom, the critical value of F is 3.026 for α=0.05. As usual, you can either look up these values in a table or use the appropriate function in R: f_crit &lt;- qf(.95, df1 = 2, df2 = 297) #critical value f_crit ## [1] 3.026153 f_ratio &gt; f_crit #test if calculated test statistic is larger than critical value ## [1] TRUE The output tells us that the calculated test statistic exceeds the critical value. We can also show the test result visually: Visual depiction of the test result Thus, we conclude that because FCAL = 332.08 &gt; FCR = 3.03, H0 is rejected! Interpretation: one or more of the differences between means are statistically significant. Reporting: There was a significant effect of promotion on sales levels, F(2,297) = 332.08, p &lt; 0.05, \\(\\eta^2\\) = 0.69. Remember: This doesn’t tell us where the differences between groups lie. To find out which group means exactly differ, we need to use post-hoc procedures (see below). You don’t have to compute these statistics manually! Luckily, there is a function for ANOVA in R, which does the above calculations for you as we will see in the next section. 5.4.3 One-way ANOVA 5.4.3.1 Basic ANOVA As already indicated, one-way ANOVA is used when there is only one categorical variable (factor). Before conducting ANOVA, you need to check if the assumptions of the test are fulfilled. The assumptions of ANOVA are discussed in the following sections. Independence of observations The observations in the groups should be independent. Because we randomly assigned the listeners to the experimental conditions, this assumption can be assumed to be met. Distributional assumptions ANOVA is relatively immune to violations to the normality assumption when sample sizes are large due to the Central Limit Theorem. However, if your sample is small (i.e., n &lt; 30 per group) you may nevertheless want to check the normality of your data, e.g., by using the Shapiro-Wilk test or QQ-Plot. In our example, we have 100 observations in each group which is plenty but let’s create another example with only 10 observations in each group. In the latter case we cannot rely on the Central Limit Theorem and we should test the normality of our data. This can be done using the Shapiro-Wilk Test, which has the Null Hypothesis that the data is normally distributed. Hence, an insignificant test results means that the data can be assumed to be approximately normally distributed: set.seed(321) hours_fewobs &lt;- data.frame(hours = c(rnorm(10, 20, 5), rnorm(10, 40, 5), rnorm(10, 60, 5)), group = c(rep(&#39;A&#39;, 10), rep(&#39;B&#39;, 10), rep(&#39;C&#39;, 10))) by(hours_fewobs$hours, hours_fewobs$group, shapiro.test) ## hours_fewobs$group: A ## ## Shapiro-Wilk normality test ## ## data: dd[x, ] ## W = 0.91625, p-value = 0.3267 ## ## ------------------------------------------------------------ ## hours_fewobs$group: B ## ## Shapiro-Wilk normality test ## ## data: dd[x, ] ## W = 0.91486, p-value = 0.3161 ## ## ------------------------------------------------------------ ## hours_fewobs$group: C ## ## Shapiro-Wilk normality test ## ## data: dd[x, ] ## W = 0.9595, p-value = 0.7801 Since the test result is insignificant for all groups, we can conclude that the data approximately follow a normal distribution. We could also test the distributional assumptions visually using a Q-Q plot (i.e., quantile-quantile plot). This plot can be used to assess if a set of data plausibly came from some theoretical distribution such as the Normal distribution. Since this is just a visual check, it is somewhat subjective. But it may help us to judge if our assumption is plausible, and if not, which data points contribute to the violation. A Q-Q plot is a scatterplot created by plotting two sets of quantiles against one another. If both sets of quantiles came from the same distribution, we should see the points forming a line that’s roughly straight. In other words, Q-Q plots take your sample data, sort it in ascending order, and then plot them versus quantiles calculated from a theoretical distribution. Quantiles are often referred to as “percentiles” and refer to the points in your data below which a certain proportion of your data fall. Recall, for example, the standard Normal distribution with a mean of 0 and a standard deviation of 1. Since the 50th percentile (or 0.5 quantile) is 0, half the data lie below 0. The 95th percentile (or 0.95 quantile), is about 1.64, which means that 95 percent of the data lie below 1.64. The 97.5th quantile is about 1.96, which means that 97.5% of the data lie below 1.96. In the Q-Q plot, the number of quantiles is selected to match the size of your sample data. To create the Q-Q plot for the normal distribution, you may use the qqnorm() function, which takes the data to be tested as an argument. Using the qqline() function subsequently on the data creates the line on which the data points should fall based on the theoretical quantiles. If the individual data points deviate a lot from this line, it means that the data is not likely to follow a normal distribution. qqnorm(hours_fewobs[hours_fewobs$group==&quot;A&quot;,]$hours) qqline(hours_fewobs[hours_fewobs$group==&quot;A&quot;,]$hours) Figure 5.7: Q-Q plot 1 qqnorm(hours_fewobs[hours_fewobs$group==&quot;B&quot;,]$hours) qqline(hours_fewobs[hours_fewobs$group==&quot;B&quot;,]$hours) Figure 5.8: Q-Q plot 2 qqnorm(hours_fewobs[hours_fewobs$group==&quot;C&quot;,]$hours) qqline(hours_fewobs[hours_fewobs$group==&quot;C&quot;,]$hours) Figure 5.9: Q-Q plot 3 The Q-Q plots suggest an approximately Normal distribution. If the assumption had been violated, you might consider transforming your data or resort to a non-parametric test. Homogeneity of variance Let’s return to our original dataset with 100 observations in each group for the rest of the analysis. You can test the homogeneity of variances in R using Levene’s test: library(car) leveneTest(hours ~ group, data = hours_abc, center = mean) ## Levene&#39;s Test for Homogeneity of Variance (center = mean) ## Df F value Pr(&gt;F) ## group 2 4.9678 0.007548 ** ## 297 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The null hypothesis of the test is that the group variances are equal. Thus, if the test result is significant it means that the variances are not equal. If we cannot reject the null hypothesis (i.e., the group variances are not significantly different), we can proceed with the ANOVA as follows: aov &lt;- aov(hours ~ group, data = hours_abc) summary(aov) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## group 2 21321 10661 332.1 &lt;0.0000000000000002 *** ## Residuals 297 9534 32 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 You can see that the p-value is smaller than 0.05. This means that, if there really was no difference between the population means (i.e., the Null hypothesis was true), the probability of the observed differences (or larger differences) is less than 5%. To compute η2 from the output, we can extract the relevant sum of squares as follows summary(aov)[[1]]$&#39;Sum Sq&#39;[1]/(summary(aov)[[1]]$&#39;Sum Sq&#39;[1] + summary(aov)[[1]]$&#39;Sum Sq&#39;[2]) ## [1] 0.6909988 You can see that the results match the results from our manual computation above (\\(\\eta^2 =\\) 0.69). The aov() function also automatically generates some plots that you can use to judge if the model assumptions are met. We will inspect two of the plots here. We will use the first plot to inspect if the residual variances are equal across the experimental groups: plot(aov,1) Generally, the residual variance (i.e., the range of values on the y-axis) should be the same for different levels of our independent variable. The plot shows, that there are some slight differences. Notably, the range of residuals is higher in group “B” than in group “C”. However, the differences are not that large and since the Levene’s test could not reject the Null of equal variances, we conclude that the variances are similar enough in this case. The second plot can be used to test the assumption that the residuals are approximately normally distributed. We use a Q-Q plot to test this assumption: plot(aov,2) The plot suggests that, the residuals are approximately normally distributed. We could also test this by extracting the residuals from the anova output using the resid() function and using the Shapiro-Wilk test: shapiro.test(resid(aov)) ## ## Shapiro-Wilk normality test ## ## data: resid(aov) ## W = 0.99723, p-value = 0.8925 Confirming the impression from the Q-Q plot, we cannot reject the Null that the residuals are approximately normally distributed. Note that if Levene’s test would have been significant (i.e., variances are not equal), we would have needed to either resort to non-parametric tests (see below), or compute the Welch’s F-ratio instead: oneway.test(hours ~ group, hours_abc) ## ## One-way analysis of means (not assuming equal variances) ## ## data: hours and group ## F = 350.93, num df = 2, denom df = 194, p-value &lt; 0.00000000000000022 You can see that the results are fairly similar, since the variances turned out to be fairly equal across groups. 5.4.3.2 Post-hoc tests Provided that significant differences were detected by the overall ANOVA you can find out which group means are different using post hoc procedures. Post hoc procedures are designed to conduct pairwise comparisons of all different combinations of the treatment groups by correcting the level of significance for each test such that the overall Type I error rate (α) across all comparisons remains at 0.05. In other words, we rejected H0: μ1= μ2= μ3, and now we would like to test: Test1: \\[H_0: \\mu_1 = \\mu_2\\] Test2: \\[H_0: \\mu_1 = \\mu_3\\] Test3: \\[H_0: \\mu_2 = \\mu_3\\] There are several post hoc procedures available to choose from. In this tutorial, we will cover Bonferroni and Tukey’s HSD (“honest significant differences”). Both tests control for family-wise error. Bonferroni tends to have more power when the number of comparisons is small, whereas Tukey’ HSDs is better when testing large numbers of means. 5.4.3.2.1 Bonferroni One of the most popular (and easiest) methods to correct for the family-wise error rate is to conduct the individual t-tests and divide α by the number of comparisons („k“): \\[ p_{CR}= \\frac{\\alpha}{k} \\] In our example with three groups: \\[p_{CR}= \\frac{0.05}{3}=0.017\\] Thus, the “corrected” critical p-value is now 0.017 instead of 0.05 (i.e., the critical t value is higher). You can implement the Bonferroni procedure in R using: bonferroni &lt;- pairwise.t.test(hours_abc$hours, hours_abc$group, data = hours_abc, p.adjust.method = &quot;bonferroni&quot;) bonferroni ## ## Pairwise comparisons using t tests with pooled SD ## ## data: hours_abc$hours and hours_abc$group ## ## A B ## B &lt;0.0000000000000002 - ## C &lt;0.0000000000000002 &lt;0.0000000000000002 ## ## P value adjustment method: bonferroni In the output, you will get the corrected p-values for the individual tests. In our example, we can reject H0 of equal means for all three tests, since p &lt; 0.05 for all combinations of groups. Note the difference between the results from the post-hoc test compared to individual t-tests. For example, when we test the “B” vs. “C” groups, the result from a t-test would be: data_subset &lt;- subset(hours_abc, group != &quot;A&quot;) ttest &lt;- t.test(hours ~ group, data = data_subset, var.equal= TRUE) ttest ## ## Two Sample t-test ## ## data: hours by group ## t = -11.884, df = 198, p-value &lt; 0.00000000000000022 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -11.997471 -8.582529 ## sample estimates: ## mean in group B mean in group C ## 24.70 34.99 Usually the p-value is lower in the t-test, reflecting the fact that the family-wise error is not corrected (i.e., the test is less conservative). In this case the p-value is extremely small in both cases and thus indistinguishable. 5.4.3.2.2 Tukey’s HSD Tukey’s HSD also compares all possible pairs of means (two-by-two combinations; i.e., like a t-test, except that it corrects for family-wise error rate). Test statistic: \\[\\begin{equation} \\begin{split} HSD= q\\sqrt{\\frac{MS_R}{n_c}} \\end{split} \\tag{5.2} \\end{equation}\\] where: q = value from studentized range table (see e.g., here) MSR = Mean Square Error from ANOVA nc = number of observations per group Decision: Reject H0 if \\[|\\bar{Y}_i-\\bar{Y}_j | &gt; HSD\\] The value from the studentized range table can be obtained using the qtukey() function. q &lt;- qtukey(0.95, nm = 3, df = 297) q ## [1] 3.331215 Hence: \\[HSD= 3.33\\sqrt{\\frac{33.99}{100}}=1.94\\] Or, in R: hsd &lt;- q * sqrt(summary(aov)[[1]]$&#39;Mean Sq&#39;[2]/100) hsd ## [1] 1.887434 Since all mean differences between groups are larger than 1.906, we can reject the null hypothesis for all individual tests, confirming the results from the Bonferroni test. To compute Tukey’s HSD, we can use the appropriate function from the multcomp package. library(multcomp) aov$model$group &lt;- as.factor(aov$model$group) tukeys &lt;- glht(aov, linfct = mcp(group = &quot;Tukey&quot;)) summary(tukeys) ## ## Simultaneous Tests for General Linear Hypotheses ## ## Multiple Comparisons of Means: Tukey Contrasts ## ## ## Fit: aov(formula = hours ~ group, data = hours_abc) ## ## Linear Hypotheses: ## Estimate Std. Error t value Pr(&gt;|t|) ## B - A == 0 10.3600 0.8013 12.93 &lt;0.0000000000000002 *** ## C - A == 0 20.6500 0.8013 25.77 &lt;0.0000000000000002 *** ## C - B == 0 10.2900 0.8013 12.84 &lt;0.0000000000000002 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## (Adjusted p values reported -- single-step method) confint(tukeys) ## ## Simultaneous Confidence Intervals ## ## Multiple Comparisons of Means: Tukey Contrasts ## ## ## Fit: aov(formula = hours ~ group, data = hours_abc) ## ## Quantile = 2.3558 ## 95% family-wise confidence level ## ## ## Linear Hypotheses: ## Estimate lwr upr ## B - A == 0 10.3600 8.4723 12.2477 ## C - A == 0 20.6500 18.7623 22.5377 ## C - B == 0 10.2900 8.4023 12.1777 We may also plot the result for the mean differences incl. their confidence intervals: plot(tukeys) Figure 5.10: Tukey’s HSD You can see that the CIs do not cross zero, which means that the true difference between group means is unlikely zero. mean1 &lt;- mean(hours_abc[hours_abc$group==&quot;A&quot;,&quot;hours&quot;]) #mean group &quot;A&quot; mean1 ## [1] 14.34 mean2 &lt;- mean(hours_abc[hours_abc$group==&quot;B&quot;,&quot;hours&quot;]) #mean group &quot;B&quot; mean2 ## [1] 24.7 mean3 &lt;- mean(hours_abc[hours_abc$group==&quot;C&quot;,&quot;hours&quot;]) #mean group &quot;C&quot; mean3 ## [1] 34.99 #CI high vs. medium mean_diff_high_med &lt;- mean2-mean1 mean_diff_high_med ## [1] 10.36 ci_med_high_lower &lt;- mean_diff_high_med-hsd ci_med_high_upper &lt;- mean_diff_high_med+hsd ci_med_high_lower ## [1] 8.472566 ci_med_high_upper ## [1] 12.24743 #CI high vs.low mean_diff_high_low &lt;- mean3-mean1 mean_diff_high_low ## [1] 20.65 ci_low_high_lower &lt;- mean_diff_high_low-hsd ci_low_high_upper &lt;- mean_diff_high_low+hsd ci_low_high_lower ## [1] 18.76257 ci_low_high_upper ## [1] 22.53743 #CI medium vs.low mean_diff_med_low &lt;- mean3-mean2 mean_diff_med_low ## [1] 10.29 ci_low_med_lower &lt;- mean_diff_med_low-hsd ci_low_med_upper &lt;- mean_diff_med_low+hsd ci_low_med_lower ## [1] 8.402566 ci_low_med_upper ## [1] 12.17743 Reporting of post hoc results: The post hoc tests based on Bonferroni and Tukey’s HSD revealed that people listened to music significantly more when: they had access to user created playlists vs. those who did not, they got recommendations vs. those who did not. This is true for both the control group “A” as well as treatment “B”. The following video summarizes how to conduct a one-way ANOVA in R 5.5 Non-parametric tests Non-Parametric tests do not require the sampling distribution to be normally distributed (a.k.a. “assumption free tests”). These tests may be used when the variable of interest is measured on an ordinal scale or when the parametric assumptions do not hold. They often rely on ranking the data instead of analyzing the actual scores. By ranking the data, information on the magnitude of differences is lost. Thus, parametric tests are more powerful if the sampling distribution is normally distributed. When should you use non-parametric tests? When your DV is measured on an ordinal scale When your data is better represented by the median (e.g., there are outliers that you can’t remove) When the assumptions of parametric tests are not met (e.g., normally distributed sampling distribution) You have a very small sample size (i.e., the central limit theorem does not apply) You can download the corresponding R-Code here 5.5.1 Mann-Whitney U Test (a.k.a. Wilcoxon rank-sum test) The Mann-Whitney U test is a non-parametric test of differences between groups, similar to the two sample t-test. In contrast to the two sample t-test it only requires ordinally scaled data and relies on weaker assumptions. Thus it is often useful if the assumptions of the t-test are violated, especially if the data is not on a ratio scale. The following assumptions must be fulfilled for the test to be applicable: The dependent variable is at least ordinally scaled (i.e. a ranking between values can be established) The independent variable has only two levels A between-subjects design is used (i.e., the subjects are not matched across conditions) Intuitively, the test compares the frequency of low and high ranks between groups. Under the null hypothesis, the amount of high and low ranks should be roughly equal in the two groups. This is achieved through comparing the expected sum of ranks to the actual sum of ranks. As an example, we will be using data obtained from a field experiment with random assignment. In a music download store, new releases were randomly assigned to an experimental group and sold at a reduced price (i.e., 7.95€), or a control group and sold at the standard price (9.95€). A representative sample of 102 new releases were sampled and these albums were randomly assigned to the experimental groups (i.e., 51 albums per group). The sales were tracked over one day. Let’s load and investigate the data first: library(psych) library(ggplot2) rm(music_sales) music_sales &lt;- read.table(&quot;https://raw.githubusercontent.com/IMSMWU/Teaching/master/MRDA2017/music_experiment.dat&quot;, sep = &quot;\\t&quot;, header = TRUE) #read in data music_sales$group &lt;- factor(music_sales$group, levels = c(1:2), labels = c(&quot;low_price&quot;, &quot;high_price&quot;)) #convert grouping variable to factor str(music_sales) #inspect data ## &#39;data.frame&#39;: 102 obs. of 3 variables: ## $ product_id: int 1 2 3 4 5 6 7 8 9 10 ... ## $ unit_sales: int 6 27 30 24 21 11 18 15 18 13 ... ## $ group : Factor w/ 2 levels &quot;low_price&quot;,&quot;high_price&quot;: 1 1 1 1 1 1 1 1 1 1 ... head(music_sales) #inspect data Inspect descriptives (overall and by group). psych::describe(music_sales$unit_sales) #overall descriptives ## vars n mean sd median trimmed mad min max range skew kurtosis se ## X1 1 102 7.12 6.26 6 6.1 4.45 0 30 30 1.71 3.02 0.62 describeBy(music_sales$unit_sales, music_sales$group) #descriptives by group ## ## Descriptive statistics by group ## group: low_price ## vars n mean sd median trimmed mad min max range skew kurtosis se ## X1 1 51 8.37 6.44 6 7.17 4.45 2 30 28 1.66 2.22 0.9 ## ------------------------------------------------------------ ## group: high_price ## vars n mean sd median trimmed mad min max range skew kurtosis se ## X1 1 51 5.86 5.87 3 4.9 4.45 0 30 30 1.84 4.1 0.82 Create boxplot and plot of means. ggplot(music_sales, aes(group, unit_sales)) + geom_bar(stat = &quot;summary&quot;, color = &quot;black&quot;, fill = &quot;white&quot;, width = 0.7) + geom_pointrange(stat = &quot;summary&quot;) + labs(x = &quot;Group&quot;, y = &quot;Average number of sales&quot;) + theme_bw() Figure 5.11: Boxplot Let’s assume that one of the parametric assumptions has been violated and we needed to conduct a non-parametric test. Then, the Mann-Whitney U test is implemented in R using the function wilcox.test(). Using the ranking data as an independent variable and the listening time as a dependent variable, the test could be executed as follows: wilcox.test(unit_sales ~ group, data = music_sales) #Mann-Whitney U Test ## ## Wilcoxon rank sum test with continuity correction ## ## data: unit_sales by group ## W = 1710, p-value = 0.005374 ## alternative hypothesis: true location shift is not equal to 0 The p-value is smaller than 0.05, which leads us to reject the null hypothesis, i.e. the test yields evidence that the new service feature leads to higher music listening times. 5.5.2 Wilcoxon signed-rank test The Wilcoxon signed-rank test is a non-parametric test used to analyze the difference between paired observations, analogously to the paired t-test. It can be used when measurements come from the same observational units but the distributional assumptions of the paired t-test do not hold, because it does not require any assumptions about the distribution of the measurements. Since we subtract two values, however, the test requires that the dependent variable is at least interval scaled, meaning that intervals have the same meaning for different points on our measurement scale. Under the null hypothesis \\(H_0\\), the differences of the measurements should follow a symmetric distribution around 0, meaning that, on average, there is no difference between the two matched samples. \\(H_1\\) states that the distributions mean is non-zero. As an example, let’s consider a slightly different experimental setup for the music download store. Imagine that new releases were either sold at a reduced price (i.e., 7.95€), or at the standard price (9.95€). Every time a customer came to the store, the prices were randomly determined for every new release. This means that the same 51 albums were either sold at the standard price or at the reduced price and this price was determined randomly. The sales were then recorded over one day. Note the difference to the previous case, where we randomly split the sample and assigned 50% of products to each condition. Now, we randomly vary prices for all albums between high and low prices. Let’s load and investigate the data first: rm(music_sales_dep) music_sales_dep &lt;- read.table(&quot;https://raw.githubusercontent.com/IMSMWU/Teaching/master/MRDA2017/music_experiment_dependent.dat&quot;, sep = &quot;\\t&quot;, header = TRUE) #read in data str(music_sales_dep) #inspect data ## &#39;data.frame&#39;: 51 obs. of 3 variables: ## $ product_id : int 1 2 3 4 5 6 7 8 9 10 ... ## $ unit_sales_low_price : int 6 27 30 24 21 11 18 15 18 13 ... ## $ unit_sales_high_price: int 9 12 30 18 20 15 2 3 3 9 ... head(music_sales_dep) #inspect data Again, let’s assume that one of the prarametric assumptions has been violated and we needed to conduct a non-parametric test. Then the Wilcoxon signed-rank test can be performed with the same command as the Mann-Whitney U test, provided that the argument paired is set to TRUE. wilcox.test(music_sales_dep$unit_sales_low_price, music_sales_dep$unit_sales_high_price, paired = TRUE) #Wilcoxon signed-rank test ## ## Wilcoxon signed rank test with continuity correction ## ## data: music_sales_dep$unit_sales_low_price and music_sales_dep$unit_sales_high_price ## V = 867.5, p-value = 0.004024 ## alternative hypothesis: true location shift is not equal to 0 Using the 95% confidence level, the result would suggest a significant effect of price on sales (i.e., p &lt; 0.05). 5.5.3 Kruskal-Wallis test When should you use non-parametric tests? When the dependent variable is measured at an ordinal scale and we want to compare more than 2 means When the assumptions of independent ANOVA are not met (e.g., assumptions regarding the sampling distribution in small samples) The Kruskal–Wallis test is the non-parametric counterpart of the one-way independent ANOVA. It is designed to test for significant differences in population medians when you have more than two samples (otherwise you would use the Mann-Whitney U-test). The theory is very similar to that of the Mann–Whitney U-test since it is also based on ranked data. The Kruskal-Wallis test is carried out using the kruskal.test() function. Using the same data as before, we type: kruskal.test(Sales ~ Promotion, data = online_store_promo) ## ## Kruskal-Wallis rank sum test ## ## data: Sales by Promotion ## Kruskal-Wallis chi-squared = 16.529, df = 2, p-value = 0.0002575 The test-statistic follows a chi-square distribution and since the test is significant (p &lt; 0.05), we can conclude that there are significant differences in population medians. Provided that the overall effect is significant, you may perform a post hoc test to find out which groups are different. To get a first impression, we can plot the data using a boxplot: #Boxplot ggplot(online_store_promo, aes(x = Promotion, y = Sales)) + geom_boxplot() + labs(x = &quot;Experimental group (promotion level)&quot;, y = &quot;Number of sales&quot;) + theme_bw() Figure 5.12: Boxplot To test for differences between groups, we can, for example, apply post hoc tests according to Nemenyi for pairwise multiple comparisons of the ranked data using the appropriate function from the PMCMR package. library(PMCMR) posthoc.kruskal.nemenyi.test(x = online_store_promo$Sales, g = online_store_promo$Promotion, dist = &quot;Tukey&quot;) ## ## Pairwise comparisons using Tukey and Kramer (Nemenyi) test ## with Tukey-Dist approximation for independent samples ## ## data: online_store_promo$Sales and online_store_promo$Promotion ## ## high medium ## medium 0.09887 - ## low 0.00016 0.11683 ## ## P value adjustment method: none The results reveal that there is a significant difference between the “low” and “high” promotion groups. Note that the results are different compared to the results from the parametric test above. This difference occurs because non-parametric tests have more power to detect differences between groups since we lose information by ranking the data. Thus, you should rely on parametric tests if the assumptions are met. 5.6 Categorical data In some instances, you will be confronted with differences between proportions, rather than differences between means. For example, you may conduct an A/B-Test and wish to compare the conversion rates between two advertising campaigns. In this case, your data is binary (0 = no conversion, 1 = conversion) and the sampling distribution for such data is binomial. While binomial probabilities are difficult to calculate, we can use a Normal approximation to the binomial when n is large (&gt;100) and the true likelihood of a 1 is not too close to 0 or 1. Let’s use an example: assume a call center where service agents call potential customers to sell a product. We consider two call center agents: Service agent 1 talks to 300 customers and gets 200 of them to buy (conversion rate=2/3) Service agent 2 talks to 300 customers and gets 100 of them to buy (conversion rate=1/3) As always, we load the data first: You can download the corresponding R-Code here call_center &lt;- read.table(&quot;https://raw.githubusercontent.com/IMSMWU/Teaching/master/MRDA2017/call_center.dat&quot;, sep = &quot;\\t&quot;, header = TRUE) #read in data call_center$conversion &lt;- factor(call_center$conversion , levels = c(0:1), labels = c(&quot;no&quot;, &quot;yes&quot;)) #convert to factor call_center$agent &lt;- factor(call_center$agent , levels = c(0:1), labels = c(&quot;agent_1&quot;, &quot;agent_2&quot;)) #convert to factor Next, we create a table to check the relative frequencies: rel_freq_table &lt;- as.data.frame(prop.table(table(call_center), 2)) #conditional relative frequencies rel_freq_table We could also plot the data to visualize the frequencies using ggplot: ggplot(rel_freq_table, aes(x = agent, y = Freq, fill = conversion)) + #plot data geom_col(width = .7) + #position geom_text(aes(label = paste0(round(Freq*100,0),&quot;%&quot;)), position = position_stack(vjust = 0.5), size = 4) + #add percentages ylab(&quot;Proportion of conversions&quot;) + xlab(&quot;Agent&quot;) + # specify axis labels theme_bw() Figure 5.13: proportion of conversions per agent (stacked bar chart) … or using the mosaicplot() function: contigency_table &lt;- table(call_center) mosaicplot(contigency_table, main = &quot;Proportion of conversions by agent&quot;) Figure 5.14: proportion of conversions per agent (mosaic plot) 5.6.1 Confidence intervals for proportions Recall that we can use confidence intervals to determine the range of values that the true population parameter will take with a certain level of confidence based on the sample. Similar to the confidence interval for means, we can compute a confidence interval for proportions. The (1-\\(\\alpha\\))% confidence interval for proportions is approximately \\[ CI = p\\pm z_{1-\\frac{\\alpha}{2}}*\\sqrt{\\frac{p*(1-p)}{N}} \\] where \\(\\sqrt{p(1-p)}\\) is the equivalent to the standard deviation in the formula for the confidence interval for means. Based on the equation, it is easy to compute the confidence intervals for the conversion rates of the call center agents: n1 &lt;- nrow(subset(call_center,agent==&quot;agent_1&quot;)) #number of observations for agent 1 n2 &lt;- nrow(subset(call_center,agent==&quot;agent_2&quot;)) #number of observations for agent 1 n1_conv &lt;- nrow(subset(call_center,agent==&quot;agent_1&quot; &amp; conversion==&quot;yes&quot;)) #number of conversions for agent 1 n2_conv &lt;- nrow(subset(call_center,agent==&quot;agent_2&quot; &amp; conversion==&quot;yes&quot;)) #number of conversions for agent 2 p1 &lt;- n1_conv/n1 #proportion of conversions for agent 1 p2 &lt;- n2_conv/n2 #proportion of conversions for agent 2 error1 &lt;- qnorm(0.975)*sqrt((p1*(1-p1))/n1) ci_lower1 &lt;- p1 - error1 ci_upper1 &lt;- p1 + error1 ci_lower1 ## [1] 0.6133232 ci_upper1 ## [1] 0.7200101 error2 &lt;- qnorm(0.975)*sqrt((p2*(1-p2))/n2) ci_lower2 &lt;- p2 - error2 ci_upper2 &lt;- p2 + error2 ci_lower2 ## [1] 0.2799899 ci_upper2 ## [1] 0.3866768 Similar to testing for differences in means, we could also ask: Is agent 1 twice as likely as agent 2 to convert a customer? Or, to state it formally: \\[H_0: \\pi_1=\\pi_2 \\\\ H_1: \\pi_1\\ne \\pi_2\\] where \\(\\pi\\) denotes the population parameter associated with the proportion in the respective population. One approach to test this is based on confidence intervals to estimate the difference between two populations. We can compute an approximate confidence interval for the difference between the proportion of successes in group 1 and group 2, as: \\[ CI = p_1-p_2\\pm z_{1-\\frac{\\alpha}{2}}*\\sqrt{\\frac{p_1*(1-p_1)}{n_1}+\\frac{p_2*(1-p_2)}{n_2}} \\] If the confidence interval includes zero, then the data does not suggest a difference between the groups. Let’s compute the confidence interval for differences in the proportions by hand first: ci_lower &lt;- p1 - p2 - qnorm(0.975)*sqrt(p1*(1 - p1)/n1 + p2*(1 - p2)/n2) #95% CI lower bound ci_upper &lt;- p1 - p2 + qnorm(0.975)*sqrt(p1*(1 - p1)/n1 + p2*(1 - p2)/n2) #95% CI upper bound ci_lower ## [1] 0.2578943 ci_upper ## [1] 0.4087724 Now we can see that the 95% confidence interval estimate of the difference between the proportion of conversions for agent 1 and the proportion of conversions for agent 2 is between 26% and 41%. This interval tells us the range of plausible values for the difference between the two population proportions. According to this interval, zero is not a plausible value for the difference (i.e., interval does not cross zero), so we reject the null hypothesis that the population proportions are the same. Instead of computing the intervals by hand, we could also use the prop.test() function: prop.test(x = c(n1_conv, n2_conv), n = c(n1, n2), conf.level = 0.95) ## ## 2-sample test for equality of proportions with continuity correction ## ## data: c(n1_conv, n2_conv) out of c(n1, n2) ## X-squared = 65.34, df = 1, p-value = 0.0000000000000006303 ## alternative hypothesis: two.sided ## 95 percent confidence interval: ## 0.2545610 0.4121057 ## sample estimates: ## prop 1 prop 2 ## 0.6666667 0.3333333 Note that the prop.test() function uses a slightly different (more accurate) way to compute the confidence interval (Wilson’s score method is used). It is particularly a better approximation for smaller N. That’s why the confidence interval in the output slightly deviates from the manual computation above, which uses the Wald interval. You can also see that the output from the prop.test() includes the results from a χ2 test for the equality of proportions (which will be discussed below) and the associated p-value. Since the p-value is less than 0.05, we reject the null hypothesis of equal probability. Thus, the reporting would be: The test showed that the conversion rate for agent 1 was higher by 33%. This difference is significant χ (1) = 70, p &lt; .05 (95% CI = [0.25,0.41]). 5.6.2 Chi-square test In the previous section, we saw how we can compute the confidence interval for the difference between proportions to decide on whether or not to reject the null hypothesis. Whenever you would like to investigate the relationship between two categorical variables, the \\(\\chi^2\\) test may be used to test whether the variables are independent of each other. It achieves this by comparing the expected number of observations in a group to the actual values. Let’s continue with the example from the previous section. Under the null hypothesis, the two variables agent and conversion in our contingency table are independent (i.e., there is no relationship). This means that the frequency in each field will be roughly proportional to the probability of an observation being in that category, calculated under the assumption that they are independent. The difference between that expected quantity and the actual quantity can be used to construct the test statistic. The test statistic is computed as follows: \\[ \\chi^2=\\sum_{i=1}^{J}\\frac{(f_o-f_e)^2}{f_e} \\] where \\(J\\) is the number of cells in the contingency table, \\(f_o\\) are the observed cell frequencies and \\(f_e\\) are the expected cell frequencies. The larger the differences, the larger the test statistic and the smaller the p-value. The observed cell frequencies can easily be seen from the contingency table: obs_cell1 &lt;- contigency_table[1,1] obs_cell2 &lt;- contigency_table[1,2] obs_cell3 &lt;- contigency_table[2,1] obs_cell4 &lt;- contigency_table[2,2] The expected cell frequencies can be calculated as follows: \\[ f_e=\\frac{(n_r*n_c)}{n} \\] where \\(n_r\\) are the total observed frequencies per row, \\(n_c\\) are the total observed frequencies per column, and \\(n\\) is the total number of observations. Thus, the expected cell frequencies under the assumption of independence can be calculated as: n &lt;- nrow(call_center) exp_cell1 &lt;- (nrow(call_center[call_center$agent==&quot;agent_1&quot;,])*nrow(call_center[call_center$conversion==&quot;no&quot;,]))/n exp_cell2 &lt;- (nrow(call_center[call_center$agent==&quot;agent_1&quot;,])*nrow(call_center[call_center$conversion==&quot;yes&quot;,]))/n exp_cell3 &lt;- (nrow(call_center[call_center$agent==&quot;agent_2&quot;,])*nrow(call_center[call_center$conversion==&quot;no&quot;,]))/n exp_cell4 &lt;- (nrow(call_center[call_center$agent==&quot;agent_2&quot;,])*nrow(call_center[call_center$conversion==&quot;yes&quot;,]))/n To sum up, these are the expected cell frequencies data.frame(conversion_no = rbind(exp_cell1,exp_cell3),conversion_yes = rbind(exp_cell2,exp_cell4), row.names = c(&quot;agent_1&quot;,&quot;agent_2&quot;)) ## conversion_no conversion_yes ## agent_1 150 150 ## agent_2 150 150 … and these are the observed cell frequencies data.frame(conversion_no = rbind(obs_cell1,obs_cell2),conversion_yes = rbind(obs_cell3,obs_cell4), row.names = c(&quot;agent_1&quot;,&quot;agent_2&quot;)) ## conversion_no conversion_yes ## agent_1 100 200 ## agent_2 200 100 To obtain the test statistic, we simply plug the values into the formula: chisq_cal &lt;- sum(((obs_cell1 - exp_cell1)^2/exp_cell1), ((obs_cell2 - exp_cell2)^2/exp_cell2), ((obs_cell3 - exp_cell3)^2/exp_cell3), ((obs_cell4 - exp_cell4)^2/exp_cell4)) chisq_cal ## [1] 66.66667 The test statistic is \\(\\chi^2\\) distributed. The chi-square distribution is a non-symmetric distribution. Actually, there are many different chi-square distributions, one for each degree of freedom as show in the following figure. Figure 5.15: The chi-square distribution You can see that as the degrees of freedom increase, the chi-square curve approaches a normal distribution. To find the critical value, we need to specify the corresponding degrees of freedom, given by: \\[ df=(r-1)*(c-1) \\] where \\(r\\) is the number of rows and \\(c\\) is the number of columns in the contingency table. Recall that degrees of freedom are generally the number of values that can vary freely when calculating a statistic. In a 2 by 2 table as in our case, we have 2 variables (or two samples) with 2 levels and in each one we have 1 that vary freely. Hence, in our example the degrees of freedom can be calculated as: df &lt;- (nrow(contigency_table) - 1) * (ncol(contigency_table) -1) df ## [1] 1 Now, we can derive the critical value given the degrees of freedom and the level of confidence using the qchisq() function and test if the calculated test statistic is larger than the critical value: chisq_crit &lt;- qchisq(0.95, df) chisq_crit ## [1] 3.841459 chisq_cal &gt; chisq_crit ## [1] TRUE Figure 5.16: Visual depiction of the test result We could also compute the p-value using the pchisq() function, which tells us the probability of the observed cell frequencies if the null hypothesis was true (i.e., there was no association): p_val &lt;- 1-pchisq(chisq_cal,df) p_val ## [1] 0.0000000000000003330669 The test statistic can also be calculated in R directly on the contingency table with the function chisq.test(). chisq.test(contigency_table, correct = FALSE) ## ## Pearson&#39;s Chi-squared test ## ## data: contigency_table ## X-squared = 66.667, df = 1, p-value = 0.0000000000000003215 Since the p-value is smaller than 0.05 (i.e., the calculated test statistic is larger than the critical value), we reject H0 that the two variables are independent. Note that the test statistic is sensitive to the sample size. To see this, let’s assume that we have a sample of 100 observations instead of 1000 observations: chisq.test(contigency_table/10, correct = FALSE) ## ## Pearson&#39;s Chi-squared test ## ## data: contigency_table/10 ## X-squared = 6.6667, df = 1, p-value = 0.009823 You can see that even though the proportions haven’t changed, the test is insignificant now. The following equation lets you compute a measure of the effect size, which is insensitive to sample size: \\[ \\phi=\\sqrt{\\frac{\\chi^2}{n}} \\] The following guidelines are used to determine the magnitude of the effect size (Cohen, 1988): 0.1 (small effect) 0.3 (medium effect) 0.5 (large effect) In our example, we can compute the effect sizes for the large and small samples as follows: test_stat &lt;- chisq.test(contigency_table, correct = FALSE)$statistic phi1 &lt;- sqrt(test_stat/n) test_stat &lt;- chisq.test(contigency_table/10, correct = FALSE)$statistic phi2 &lt;- sqrt(test_stat/(n/10)) phi1 ## X-squared ## 0.3333333 phi2 ## X-squared ## 0.3333333 You can see that the statistic is insensitive to the sample size. Note that the Φ coefficient is appropriate for two dichotomous variables (resulting from a 2 x 2 table as above). If any your nominal variables has more than two categories, Cramér’s V should be used instead: \\[ V=\\sqrt{\\frac{\\chi^2}{n*df_{min}}} \\] where \\(df_{min}\\) refers to the degrees of freedom associated with the variable that has fewer categories (e.g., if we have two nominal variables with 3 and 4 categories, \\(df_{min}\\) would be 3 - 1 = 2). The degrees of freedom need to be taken into account when judging the magnitude of the effect sizes (see e.g., here). Note that the correct = FALSE argument above ensures that the test statistic is computed in the same way as we have done by hand above. By default, chisq.test() applies a correction to prevent overestimation of statistical significance for small data (called the Yates’ correction). The correction is implemented by subtracting the value 0.5 from the computed difference between the observed and expected cell counts in the numerator of the test statistic. This means that the calculated test statistic will be smaller (i.e., more conservative). Although the adjustment may go too far in some instances, you should generally rely on the adjusted results, which can be computed as follows: chisq.test(contigency_table) ## ## Pearson&#39;s Chi-squared test with Yates&#39; continuity correction ## ## data: contigency_table ## X-squared = 65.34, df = 1, p-value = 0.0000000000000006303 As you can see, the results don’t change much in our example, since the differences between the observed and expected cell frequencies are fairly large relative to the correction. Caution is warranted when the cell counts in the contingency table are small. The usual rule of thumb is that all cell counts should be at least 5 (this may be a little too stringent though). When some cell counts are too small, you can use Fisher’s exact test using the fisher.test() function. fisher.test(contigency_table) ## ## Fisher&#39;s Exact Test for Count Data ## ## data: contigency_table ## p-value = 0.0000000000000003391 ## alternative hypothesis: true odds ratio is not equal to 1 ## 95 percent confidence interval: ## 0.1754685 0.3560568 ## sample estimates: ## odds ratio ## 0.2506258 The Fisher test, while more conservative, also shows a significant difference between the proportions (p &lt; 0.05). This is not surprising since the cell counts in our example are fairly large. 5.6.3 Sample size To calculate the required sample size when comparing proportions, the power.prop.test() function can be used. For example, we could ask how large our sample needs to be if we would like to compare two groups with conversion rates of 2% and 2.5%, respectively using the conventional settings for \\(\\alpha\\) and \\(\\beta\\): power.prop.test(p1=0.02,p2=0.025,sig.level=0.05,power=0.8) ## ## Two-sample comparison of proportions power calculation ## ## n = 13808.92 ## p1 = 0.02 ## p2 = 0.025 ## sig.level = 0.05 ## power = 0.8 ## alternative = two.sided ## ## NOTE: n is number in *each* group The output tells us that we need 13809 observations per group to detect a difference of the desired size. 5.7 Summary of hypothesis testing We rarely have information about whole population, so we use hypothesis testing to test our assumptions about certain population parameter. Usually, the population parameter for which we form hypotheses is true population mean (\\(\\mu\\)). A hypothesis about it would tell us how the corresponding sampling distribution looks like. To revise, a sampling distribution is the distribution of a statistic (mean) that is obtained by repeatedly drawing a large number of samples from certain population. Based on the sampling distribution of the hypothesized population mean, we could then determine the probability of a given sample assuming that the hypothesis is true. The null hypothesis refers to the statement that is being tested and is usually a statement of the status quo, one of no difference or no effect. For instance, that true population mean and hypothesized value are equal. The aim is usually to provide an evidence against the null hypothesis. Remember that the null hypothesis can be either rejected or our data can fail to reject it. If the null hypothesis is rejected, this is taken as support for the alternative hypothesis. For instance, that true population mean and the hypothesized value are not equal. The alternative hypothesis assumes that some difference exists. The standardised value that tells us how many standard deviations our sample mean is away from the population mean if the null hypothesis was true is z-score. However, z-score calculation requires that we know the true population standard deviation \\(\\sigma\\) when computing the standard deviation of the sampling distribution \\(\\sigma_{\\bar x}\\). As we already learned, parameters of the true population are rarely known. Therefore, there is a t-statistic (or t-score) which, instad the standard deviation of the sampling distribution \\(\\sigma_{\\bar x}\\)), uses an estimate for the standard deviation of the distribution of the sample mean \\(SE_{\\bar x}\\). The p-value corresponds to the probability that the test statistic (i.e. mean) would take a value as extreme or more extreme than the one that we actually obtained from our sample, assuming that the null hypothesis is true. A small p-value signals that it is unlikely to observe the calculated test statistic under the assumption that the null hypothesis is true. In order to determine significance of our test results, we need to compare our p-value to the given significance level. Note that higher the value of the test statistic, the lower the p-value, i.e. lower the probability to observe as extreme or more extreme test statistic from that test statistic that we observed. Interpretation of the confidence interval: if we take 100 samples and calculated the mean and confidence interval for each of them, then the true population mean would be included in 95% of these intervals. To choose correct statistical test, one neeeds to consider the following questions: On what scale are your variables measured? Categorical or continuous? Do you want to test for relationships or differences? If you test for differences, how many groups would you like to test? For parametric tests, are the assumptions fulfilled? The sequence of steps in the process of hypothesis testing: Formulate null and alternative hypotheses Select an appropriate test Choose the level of significance Descriptive statistics and data visualization Conduct significance test Report results and draw a marketing conclusion A one-sample t-test is a statistical test used to test whether a population mean is significantly different from some hypothesized value (e.g. 100). \\[ H_0: \\mu = 100 \\\\ H_1: \\mu \\neq 100 \\] Between-subject experimental design: When you run experiment and randomly split sample into two groups whose units/participants are independent of each other, one of which receives a treatment (experimental group) while the other doesn’t (control group).In this case we have two independent samples and use independent t-test to compare their means. Within-subject experimental design: If you run an experiment with two experimental conditions and the same participants/units were observed in both experimental conditions, the sample is said to be dependent in the sense that you have the same participants/units in each group. In this case we have two dependent samples and use dependent t-test (Paired test) to compare their means. Hypotheses: H0: The difference between the two (independent or dependent) groups is equal zero. \\[H_0: \\mu_1 - \\mu_2=0=\\delta\\] H1: The difference between the two (independent or dependent) groups is not equal 0. \\[H_1: \\delta \\ne 0\\] Assumptions for both independent t-test and dependent t-test: The sampling distribution is normally distributed. In the dependent t-test this means that the sampling distribution of the differences between scores should be normal, not the scores themselves. Data are measured at least at the interval level. The independent t-test, because it is used to test independent groups of people, also assumes: Scores in different treatment conditions (control group and test group) are independent (i.e. they come from different participants/units/products). Homogeneity of variance – it assumes equal variances. Application in marketing: T-test can help marketing department of retailers to answer questions about customers spendings. Since they have collected extensive data about their customers through loyalty program, they may want to use to conduct a research. They might be interested in comparing average amount of Euros spent per purchase between customers who live closer (within range of 500 m) and customers who live further (more than 500m) from their new outlet. Therefore, they could form the following hypotheses: H0: There is no difference in a spending per purchase between customers who live closer to the outlet and customers who live further. H1: There is a difference in a spending per purchase between customers who live closer to the outlet and customers who live further. When conducting a research, there are factors under direct control of a researcher such as siginificance level (\\(\\alpha\\)) and the size of the sample(\\(n\\)). On the other side, factors such as power (1-\\(β\\)) and effect size (\\(d\\)) are under indirect control of a researcher. Overall, the aim is to maximize the power of the test while maintaining an acceptable significance level and keeping the sample as small as possible. Term “p-hacking” refers to misuse of p-value in the way that, although all samples drawn come from the same population, a researcher draws random samples until the differnce between two sample “proves” to be significant. This should be avoided at all costs. In cases when there are more than 2 groups to compare, we can use ANOVA. Requirements for using ANOVA are the following: A metric dependent variable (i.e., measured using an interval or ratio scale). One or more non-metric (categorical) independent variables (also called factors). Familywise or experiment wise error rate. The reason why we don’t just carry out multiple t-tests to compare 3 or more means between all combinations of groups lies in the fact that, across the groups of test, we would increase Type 1 error (alpha) possibility from acceptable 5% to 14.3%. The basic concept underlying ANOVA is the decomposition of the variance in the data: Total sum of squares (\\(SS_T\\)): coresponds to the total variation that can be attributed to various factors Model sum of squares(\\(SS_M\\)): determines how much of the variance can be explained by the model Residual sum of squares(\\(SS_R\\)): determines how much of the variance can not be explained by the model \\[SS_T= SS_M+SS_R\\] In order to test significance of independent variable on the dependent variable we use F-statistic: \\[F= \\frac{MS_M}{MS_R}=\\frac{\\frac{SS_M}{c-1}}{\\frac{SS_R}{N-c}}\\] …and compare it with the critical value calculated based on degrees of freedom. If the F-statistic exceeds the critical value, zero hypothesis is rejected. More specifically, it means that one or more of the differences between means are statistically significant. One-way ANOVA is used when there is only one categorical variable (factor). One-way ANOVA compares the means of two or more independent groups in order to determine whether there is an evidance that the associated population means are significantly different. In order to conduct it, certain requirements need to be fulfilled: A metric dependent variable must be continuous (interval or ratio). There is only one independent non-metric (categorical) variable or factor. Variances in each group need to be fairly similar (homogeneity of variance). Observations in each group must be independent (i.e. randomly assigned to the groups). In terms of normality, distributions within groups should be normally distributed. However, this applies when the sample size is small (approximately less than 30 observations). Hypotheses: H0: Means of all groups are equal (non-directional hypothesis). \\[H_0: \\mu_1 = \\mu_2 = \\mu_3\\] H1: Means of all groups are not all equal. \\[H_1: \\exists {i,j}: {\\mu_i \\ne \\mu_j}\\] Note that from the alternative hypothesis can not be found out which means are not equal! One-way ANOVA is an omnibus test statistic,i.e. cannot tell you which specific groups were statistically significantly.If you are interested in exploring the data results for any between-groups mean differences that exist, you use post hoc test. Post hoc tests are designed to compare all different combinations of the treatment groups and you can find out which group means are different. They avoid familywise error by correcting the level of significance for each test such that the overall Type I error rate (\\(\\alpha\\)) across all comparisons remains at 0.05. Among the most used post hoc tests are Bonferroni and Tukey’s HSD. While 1-sample t-test, independent(and independent) t-test and One-way ANOVA belong to parametric tests, there is a whole set of their non-parametric such as: Mann-Whitney U Test (a.k.a. Wilcoxon rank-sum test)- non-parametric peer of independent t-test Wilcoxon signed-rank test - non-parametric version of dependent t-test Kruskal-Wallis test - non-parametric counterpart of One-way ANOVA When should you use non-parametric tests? When your dependent variable is measured on an ordinal scale When your data is better represented by the median (e.g., there are outliers that you can’t remove) When the assumptions of parametric tests are not met (e.g., normally distributed sampling distribution) You have a very small sample size (i.e., the central limit theorem does not apply) In some cases, you will want to compare proportions, rather than differences between means (e.g., A/B-Test to compare the conversion rates between two campaigns). Therefore, confidence interval for proportions could be helpful. That interval shows the range of plausible values for the difference between the two population proportions. If the interval does not cross zero, then zero is not a plausible value for the difference between two proportions, and we reject the null hypothesis that the population proportions are the same. Whenever you would like to investigate the relationship between two categorical variables, the \\(\\chi^2\\) test may be used to test whether the variables are independent of each other. It achieves this by comparing the expected number of observations in a group to the actual values. Question to answer with \\(\\chi^2\\) test : Is there an association between categorical variable X (e.g. conversion) and categorical variable Y (e.g. agent)? Hypothesis: H0: There is no association between the two variables. H1: There is an association between the two variables. Interpretation: If we find an association between two variables (p &lt; 0.05), we can conclude that the propotion of conversions to proportion of non-conversions between agent 1 and agent 2 significantly differ from each other. "],
["regression.html", "6 Regression 6.1 Correlation 6.2 Regression 6.3 Potential problems 6.4 Categorical predictors 6.5 Extensions of the linear model 6.6 Summary of regression 6.7 Logistic regression", " 6 Regression This chapter is primarily based on: Field, A., Miles J., &amp; Field, Z. (2012): Discovering Statistics Using R. Sage Publications (chapters 6, 7, 8). James, G., Witten, D., Hastie, T., &amp; Tibshirani, R. (2013): An Introduction to Statistical Learning with Applications in R, Springer (chapter 3) You can download the corresponding R-Code here 6.1 Correlation Before we start with regression analysis, we will review the basic concept of correlation first. Correlation helps us to determine the degree to which the variation in one variable, X, is related to the variation in another variable, Y. 6.1.1 Correlation coefficient The correlation coefficient summarizes the strength of the linear relationship between two metric (interval or ratio scaled) variables. Let’s consider a simple example. Say you collect music data from Spotify with variables describing the acoustic features of a song. In this case we will be using “loudness” and “energy”. The “energy” variable can take values between 0 (unenergetic) and 1 (very energetic), and the “loudness” is measured in decibels (dB) with values typically ranging between -60 and 0 dB. For more detailed Information you can have a look at the Spotify documentation. readRDS(&quot;music_data.rds&quot;) s.genre &lt;- c(&quot;pop&quot;,&quot;hip hop&quot;,&quot;rock&quot;,&quot;rap&quot;,&quot;indie&quot;) music_data &lt;- subset(music_data, top.genre %in% s.genre) music_data$genre_cat &lt;- as.factor(music_data$top.genre) music_data$explicit_cat &lt;- factor(music_data$explicit, levels = c(0:1), labels = c(&quot;not explicit&quot;, &quot;explicit&quot;)) head(music_data) Let’s look at the data. The following graph shows the individual data points for the “energy”\" variable, where the black vertical line represents the mean of the variable (0.65). Figure 6.1: Observations for the energy variable You can see that the data for the “energy” variable looks like a normal distribution with a positive skew (lower values are more likely than higher ones). Figure 6.2: Scores for attitude variable Again, we can see that some respondents have an above average attitude towards the city (more favorable) and some respondents have a below average attitude towards the city. Let’s plot the data in one graph now to see if there is some co-movement: Figure 6.3: Scores for attitude and duration of residency variables We can see that there is indeed some co-movement here. The variables covary because tracks that are very energetic and intense probably also have a higher level of decibles and vice versa. Correlation helps us to quantify this relationship. Before you proceed to compute the correlation coefficient, you should first look at the data. We usually use a scatterplot to visualize the relationship between two metric variables: How can we compute the correlation coefficient? Remember that the variance measures the average deviation from the mean of a variable: \\[\\begin{equation} \\begin{split} s_x^2&amp;=\\frac{\\sum_{i=1}^{N} (X_i-\\overline{X})^2}{N-1} \\\\ &amp;= \\frac{\\sum_{i=1}^{N} (X_i-\\overline{X})*(X_i-\\overline{X})}{N-1} \\end{split} \\tag{6.1} \\end{equation}\\] When we consider two variables, we multiply the deviation for one variable by the respective deviation for the second variable: \\((X_i-\\overline{X})*(Y_i-\\overline{Y})\\) This is called the cross-product deviation. Then we sum the cross-product deviations: \\(\\sum_{i=1}^{N}(X_i-\\overline{X})*(Y_i-\\overline{Y})\\) … and compute the average of the sum of all cross-product deviations to get the covariance: \\[\\begin{equation} Cov(x, y) =\\frac{\\sum_{i=1}^{N}(X_i-\\overline{X})*(Y_i-\\overline{Y})}{N-1} \\tag{6.2} \\end{equation}\\] You can easily compute the covariance manually as follows x &lt;- music_data$energy[!is.na(music_data$energy)] x_bar &lt;- mean(music_data$energy, na.rm = T) y &lt;- music_data$loudness[!is.na(music_data$loudness)] y_bar &lt;- mean(music_data$loudness, na.rm = T) N &lt;- nrow(music_data) cov &lt;- (sum((x - x_bar)*(y - y_bar))) / (N - 1) cov ## [1] 0.348 Or you simply use the built-in cov() function: cov(x, y) # apply the cov function ## [1] 0.348 A positive covariance indicates that as one variable deviates from the mean, the other variable deviates in the same direction. A negative covariance indicates that as one variable deviates from the mean (e.g., increases), the other variable deviates in the opposite direction (e.g., decreases). However, the size of the covariance depends on the scale of measurement. Larger scale units will lead to larger covariance. To overcome the problem of dependence on measurement scale, we need to convert covariance to a standard set of units through standardization by dividing the covariance by the standard deviation (i.e., similar to how we compute z-scores). With two variables, there are two standard deviations. We simply multiply the two standard deviations. We then divide the covariance by the product of the two standard deviations to get the standardized covariance, which is known as a correlation coefficient r: \\[\\begin{equation} r=\\frac{Cov_{xy}}{s_x*s_y} \\tag{6.3} \\end{equation}\\] This is known as the product moment correlation (r) and it is straight-forward to compute: x_sd &lt;- sd(music_data$energy, na.rm = T) y_sd &lt;- sd(music_data$loudness, na.rm = T) r &lt;- cov/(x_sd*y_sd) r ## [1] 0.731 Or you could just use the cor() function: cor(music_data[, c(&quot;energy&quot;, &quot;loudness&quot;)], method = &quot;pearson&quot;, use = &quot;complete&quot;) ## energy loudness ## energy 1.000 0.732 ## loudness 0.732 1.000 Properties of r: ranges from -1 to + 1 +1 indicates perfect linear relationship -1 indicates perfect negative relationship 0 indicates no linear relationship ± .1 represents small effect ± .3 represents medium effect ± .5 represents large effect 6.1.2 Significance testing How can we determine if our two variables are significantly related? To test this, we denote the population moment correlation ρ. Then we test the null of no relationship between variables: \\[H_0:\\rho=0\\] \\[H_1:\\rho\\ne0\\] The test statistic is: \\[\\begin{equation} t=\\frac{r*\\sqrt{N-2}}{\\sqrt{1-r^2}} \\tag{6.4} \\end{equation}\\] It has a t distribution with n - 2 degrees of freedom. Then, we follow the usual procedure of calculating the test statistic and comparing the test statistic to the critical value of the underlying probability distribution. If the calculated test statistic is larger than the critical value, the null hypothesis of no relationship between X and Y is rejected. t_calc &lt;- r*sqrt(N - 2)/sqrt(1 - r^2) #calculated test statistic t_calc ## [1] 220 df &lt;- (N - 2) #degrees of freedom t_crit &lt;- qt(0.975, df) #critical value t_crit ## [1] 1.96 pt(q = t_calc, df = df, lower.tail = F) * 2 #p-value ## [1] 0 Or you can simply use the cor.test() function, which also produces the 95% confidence interval: cor.test(music_data$energy, music_data$loudness, alternative = &quot;two.sided&quot;, method = &quot;pearson&quot;, conf.level = 0.95) ## ## Pearson&#39;s product-moment correlation ## ## data: music_data$energy and music_data$loudness ## t = 220, df = 41890, p-value &lt;0.0000000000000002 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.727 0.736 ## sample estimates: ## cor ## 0.732 To determine the linear relationship between variables, the data only needs to be measured using interval scales. If you want to test the significance of the association, the sampling distribution needs to be normally distributed (we usually assume this when our data are normally distributed or when N is large). If parametric assumptions are violated, you should use non-parametric tests: Spearman’s correlation coefficient: requires ordinal data and ranks the data before applying Pearson’s equation. Kendall’s tau: use when N is small or the number of tied ranks is large. cor.test(music_data$energy, music_data$loudness, alternative = &quot;two.sided&quot;, method = &quot;spearman&quot;, conf.level = 0.95) ## ## Spearman&#39;s rank correlation rho ## ## data: music_data$energy and music_data$loudness ## S = 3504156947897, p-value &lt;0.0000000000000002 ## alternative hypothesis: true rho is not equal to 0 ## sample estimates: ## rho ## 0.714 cor.test(music_data$energy, music_data$loudness, alternative = &quot;two.sided&quot;, method = &quot;kendall&quot;, conf.level = 0.95) ## ## Kendall&#39;s rank correlation tau ## ## data: music_data$energy and music_data$loudness ## z = 161, p-value &lt;0.0000000000000002 ## alternative hypothesis: true tau is not equal to 0 ## sample estimates: ## tau ## 0.526 Report the results: A Pearson product-moment correlation coefficient was computed to assess the relationship between the duration of residence in a city and the attitude toward the city. There was a positive correlation between the two variables, r = 0.936, n = 12, p &lt; 0.05. A scatterplot summarizes the results (Figure XY). A note on the interpretation of correlation coefficients: Correlation coefficients give no indication of the direction of causality. In our example, we can conclude that the attitude toward the city is more positive as the years of residence increases. However, we cannot say that the years of residence cause the attitudes to be more positive. There are two main reasons for caution when interpreting correlations: Third-variable problem: there may be other unobserved factors that affect the results. Direction of causality: Correlations say nothing about which variable causes the other to change (reverse causality: attitudes may just as well cause the years of residence variable). 6.2 Regression Correlations measure relationships between variables (i.e., how much two variables covary). Using regression analysis we can predict the outcome of a dependent variable (Y) from one or more independent variables (X). E.g., how many products will we sell if we increase the advertising expenditures by 1000 Euros? In regression analysis, we fit a model to our data and use it to predict the values of the dependent variable from one predictor variable (bivariate regression) or several predictor variables (multiple regression). The following table shows a comparison of correlation and regression analysis:   Correlation Regression Estimated coefficient Coefficient of correlation (bounded between -1 and +1) Regression coefficient (not bounded a priori) Interpretation Linear association between two variables; Association is bidirectional (Linear) relation between one or more independent variables and dependent variable; Relation is directional Role of theory Theory neither required nor testable Theory required and testable 6.2.1 Simple linear regression In simple linear regression, we assess the relationship between one dependent (regressand) and one independent (regressor) variable. The goal is to fit a line through a scatterplot of observations in order to find the line that best describes the data (scatterplot). Suppose you are a marketing research analyst at a music label and your task is to suggest, on the basis of past data, a marketing plan for the next year that will maximize product sales. The data set that is available to you includes information on the sales of music downloads (thousands of units), advertising expenditures (in Euros), the number of radio plays an artist received per week (airplay), the number of previous releases of an artist (starpower), repertoire origin (country; 0 = local, 1 = international), and genre (1 = rock, 2 = pop, 3 = electronic). Let’s load and inspect the data first: regression &lt;- read.table(&quot;https://raw.githubusercontent.com/IMSMWU/Teaching/master/MRDA2017/music_sales_regression.dat&quot;, sep = &quot;\\t&quot;, header = TRUE) #read in data regression$country &lt;- factor(regression$country, levels = c(0:1), labels = c(&quot;local&quot;, &quot;international&quot;)) #convert grouping variable to factor regression$genre &lt;- factor(regression$genre, levels = c(1:3), labels = c(&quot;rock&quot;, &quot;pop&quot;,&quot;electronic&quot;)) #convert grouping variable to factor head(regression) psych::describe(regression) #descriptive statistics using psych ## vars n mean sd median trimmed mad min max range skew ## sales 1 200 193.20 80.70 200 192.69 88.96 10.0 360 350 0.04 ## adspend 2 200 614.41 485.66 532 560.81 489.09 9.1 2272 2263 0.84 ## airplay 3 200 27.50 12.27 28 27.46 11.86 0.0 63 63 0.06 ## starpower 4 200 6.77 1.40 7 6.88 1.48 1.0 10 9 -1.27 ## genre* 5 200 2.40 0.79 3 2.50 0.00 1.0 3 2 -0.83 ## country* 6 200 1.17 0.38 1 1.09 0.00 1.0 2 1 1.74 ## kurtosis se ## sales -0.72 5.71 ## adspend 0.17 34.34 ## airplay -0.09 0.87 ## starpower 3.56 0.10 ## genre* -0.91 0.06 ## country* 1.05 0.03 As stated above, regression analysis may be used to relate a quantitative response (“dependent variable”) to one or more predictor variables (“independent variables”). In a simple linear regression, we have one dependent and one independent variable. Here are a few important questions that we might seek to address based on the data: Is there a relationship between advertising budget and sales? How strong is the relationship between advertising budget and sales? Which other variables contribute to sales? How accurately can we estimate the effect of each variable on sales? How accurately can we predict future sales? Is the relationship linear? Is there synergy among the advertising activities? We may use linear regression to answer these questions. Let’s start with the first question and investigate the effect of advertising on sales. 6.2.1.1 Estimating the coefficients A simple linear regression model only has one predictor and can be written as: \\[\\begin{equation} Y=\\beta_0+\\beta_1X+\\epsilon \\tag{6.5} \\end{equation}\\] In our specific context, let’s consider only the influence of advertising on sales for now: \\[\\begin{equation} Sales=\\beta_0+\\beta_1*adspend+\\epsilon \\tag{6.6} \\end{equation}\\] The word “adspend” represents data on advertising expenditures that we have observed and β1 (the “slope”“) represents the unknown relationship between advertising expenditures and sales. It tells you by how much sales will increase for an additional Euro spent on advertising. β0 (the”intercept\") is the number of sales we would expect if no money is spent on advertising. Together, β0 and β1 represent the model coefficients or parameters. The error term (ε) captures everything that we miss by using our model, including, (1) misspecifications (the true relationship might not be linear), (2) omitted variables (other variables might drive sales), and (3) measurement error (our measurement of the variables might be imperfect). Once we have used our training data to produce estimates for the model coefficients, we can predict future sales on the basis of a particular value of advertising expenditures by computing: \\[\\begin{equation} \\hat{Sales}=\\hat{\\beta_0}+\\hat{\\beta_1}*adspend \\tag{6.7} \\end{equation}\\] We use the hat symbol, ^, to denote the estimated value for an unknown parameter or coefficient, or to denote the predicted value of the response (sales). In practice, β0 and β1 are unknown and must be estimated from the data to make predictions. In the case of our advertising example, the data set consists of the advertising budget and product sales (n = 200). Our goal is to obtain coefficient estimates such that the linear model fits the available data well. In other words, we fit a line through the scatterplot of observations and try to find the line that best describes the data. The following graph shows the scatterplot for our data, where the black line shows the regression line. The grey vertical lines shows the difference between the predicted values (the regression line) and the observed values. This difference is referred to as the residuals (“e”). Figure 6.4: Ordinary least squares (OLS) Estimation of the regression function is based on the idea of the method of least squares (OLS = ordinary least squares). The first step is to calculate the residuals by subtracting the observed values from the predicted values. \\(e_i = Y_i-(\\beta_0+\\beta_1X_i)\\) This difference is then minimized by minimizing the sum of the squared residuals: \\[\\begin{equation} \\sum_{i=1}^{N} e_i^2= \\sum_{i=1}^{N} [Y_i-(\\beta_0+\\beta_1X_i)]^2\\rightarrow min! \\tag{6.8} \\end{equation}\\] ei: Residuals (i = 1,2,…,N) Yi: Values of the dependent variable (i = 1,2,…,N) β0: Intercept β1: Regression coefficient / slope parameters Xni: Values of the nth independent variables and the ith observation N: Number of observations This is also referred to as the residual sum of squares (RSS). Now we need to choose the values for β0 and β1 that minimize RSS. So how can we derive these values for the regression coefficient? The equation for β1 is given by: \\[\\begin{equation} \\hat{\\beta_1}=\\frac{COV_{XY}}{s_x^2} \\tag{6.9} \\end{equation}\\] The exact mathematical derivation of this formula is beyond the scope of this script, but the intuition is to calculate the first derivative of the squared residuals with respect to β1 and set it to zero, thereby finding the β1 that minimizes the term. Using the above formula, you can easily compute β1 using the following code: cov_y_x &lt;- cov(regression$adspend, regression$sales) cov_y_x ## [1] 22672 var_x &lt;- var(regression$adspend) var_x ## [1] 235861 beta_1 &lt;- cov_y_x/var_x beta_1 ## [1] 0.0961 The interpretation of β1 is as follows: For every extra Euros spent on advertising, sales can be expected to increase by 0.096 units. Or, in other words, if we increase our marketing budget by 1,000 Euros, sales can be expected to increase by 96 units. Using the estimated coefficient for β1 , it is easy to compute β0 (the intercept) as follows: \\[\\begin{equation} \\hat{\\beta_0}=\\overline{Y}-\\hat{\\beta_1}\\overline{X} \\tag{6.10} \\end{equation}\\] The R code for this is: beta_0 &lt;- mean(regression$sales) - beta_1*mean(regression$adspend) beta_0 ## [1] 134 The interpretation of β0 is as follows: If we spend no money on advertising, we would expect to sell 134.14 units. You may also verify this based on a scatterplot of the data. The following plot shows the scatterplot including the regression line, which is estimated using OLS. ggplot(regression, mapping = aes(adspend, sales)) + geom_point(shape = 1) + geom_smooth(method = &quot;lm&quot;, fill = &quot;blue&quot;, alpha = 0.1) + labs(x = &quot;Advertising expenditures (EUR)&quot;, y = &quot;Number of sales&quot;) + theme_bw() Figure 6.5: Scatterplot You can see that the regression line intersects with the y-axis at 134.14, which corresponds to the expected sales level when advertising expenditure (on the x-axis) is zero (i.e., the intercept β0). The slope coefficient (β1) tells you by how much sales (on the y-axis) would increase if advertising expenditures (on the x-axis) are increased by one unit. 6.2.1.2 Significance testing In a next step, we assess if the effect of advertising on sales is statistically significant. This means that we test the null hypothesis H0: “There is no relationship between advertising and sales” versus the alternative hypothesis H1: “The is some relationship between advertising and sales”. Or, to state this mathematically: \\[H_0:\\beta_1=0\\] \\[H_1:\\beta_1\\ne0\\] How can we test if the effect is statistically significant? Recall the generalized equation to derive a test statistic: \\[\\begin{equation} test\\ statistic = \\frac{effect}{error} \\tag{6.11} \\end{equation}\\] The effect is given by the β1 coefficient in this case. To compute the test statistic, we need to come up with a measure of uncertainty around this estimate (the error). This is because we use information from a sample to estimate the least squares line to make inferences regarding the regression line in the entire population. Since we only have access to one sample, the regression line will be slightly different every time we take a different sample from the population. This is sampling variation and it is perfectly normal! It just means that we need to take into account the uncertainty around the estimate, which is achieved by the standard error. Thus, the test statistic for our hypothesis is given by: \\[\\begin{equation} t = \\frac{\\hat{\\beta_1}}{SE(\\hat{\\beta_1})} \\tag{6.12} \\end{equation}\\] After calculating the test statistic, we compare its value to the values that we would expect to find if there was no effect based on the t-distribution. In a regression context, the degrees of freedom are given by N - p - 1 where N is the sample size and p is the number of predictors. In our case, we have 200 observations and one predictor. Thus, the degrees of freedom is 200 - 1 - 1 = 198. In the regression output below, R provides the exact probability of observing a t value of this magnitude (or larger) if the null hypothesis was true. This probability is the p-value. A small p-value indicates that it is unlikely to observe such a substantial association between the predictor and the outcome variable due to chance in the absence of any real association between the predictor and the outcome. To estimate the regression model in R, you can use the lm() function. Within the function, you first specify the dependent variable (“sales”) and independent variable (“adspend”) separated by a ~ (tilde). As mentioned previously, this is known as formula notation in R. The data = regression argument specifies that the variables come from the data frame named “regression”. Strictly speaking, you use the lm() function to create an object called “simple_regression,” which holds the regression output. You can then view the results using the summary() function: simple_regression &lt;- lm(sales ~ adspend, data = regression) #estimate linear model summary(simple_regression) #summary of results ## ## Call: ## lm(formula = sales ~ adspend, data = regression) ## ## Residuals: ## Min 1Q Median 3Q Max ## -152.95 -43.80 -0.39 37.04 211.87 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 134.13994 7.53657 17.80 &lt;0.0000000000000002 *** ## adspend 0.09612 0.00963 9.98 &lt;0.0000000000000002 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 66 on 198 degrees of freedom ## Multiple R-squared: 0.335, Adjusted R-squared: 0.331 ## F-statistic: 99.6 on 1 and 198 DF, p-value: &lt;0.0000000000000002 Note that the estimated coefficients for β0 (134.14) and β1 (0.096) correspond to the results of our manual computation above. The associated t-values and p-values are given in the output. The t-values are larger than the critical t-values for the 95% confidence level, since the associated p-values are smaller than 0.05. In case of the coefficient for β1 this means that the probability of an association between the advertising and sales of the observed magnitude (or larger) is smaller than 0.05, if the value of β1 was, in fact, 0. The coefficients associated with the respective variables represent point estimates. To get a better feeling for the range of values that the coefficients could take, it is helpful to compute confidence intervals. A 95% confidence interval is defined as a range of values such that with a 95% probability, the range will contain the true unknown value of the parameter. For example, for β1, the confidence interval can be computed as. \\[\\begin{equation} CI = \\hat{\\beta_1}\\pm(t_{1-\\frac{\\alpha}{2}}*SE(\\beta_1)) \\tag{6.13} \\end{equation}\\] It is easy to compute confidence intervals in R using the confint() function. You just have to provide the name of you estimated model as an argument: confint(simple_regression) ## 2.5 % 97.5 % ## (Intercept) 119.2777 149.002 ## adspend 0.0771 0.115 For our model, the 95% confidence interval for β0 is [119.28,149], and the 95% confidence interval for β1 is [0.08,0.12]. Thus, we can conclude that when we do not spend any money on advertising, sales will be somewhere between 119 and 149 units on average. In addition, for each increase in advertising expenditures by one Euro, there will be an average increase in sales of between 0.08 and 0.12. 6.2.1.3 Assessing model fit Once we have rejected the null hypothesis in favor of the alternative hypothesis, the next step is to investigate to what extent the model represents (“fits”) the data. How can we assess the model fit? First, we calculate the fit of the most basic model (i.e., the mean) Then, we calculate the fit of the best model (i.e., the regression model) A good model should fit the data significantly better than the basic model R2: Represents the percentage of the variation in the outcome that can be explained by the model The F-ratio measures how much the model has improved the prediction of the outcome compared to the level of inaccuracy in the model Similar to ANOVA, the calculation of model fit statistics relies on estimating the different sum of squares values. SST is the difference between the observed data and the mean value of Y (aka. total variation). In the absence of any other information, the mean value of Y represents the best guess on where an observation at a given level of advertising will fall: \\[\\begin{equation} SS_T= \\sum_{i=1}^{N} (Y_i-\\overline{Y})^2 \\tag{6.14} \\end{equation}\\] The following graph shows the total sum of squares: Figure 6.6: Total sum of squares Based on our linear model, the best guess about the sales level at a given level of advertising is the predicted value. The model sum of squares (SSM) has the mathematical representation: \\[\\begin{equation} SS_M= \\sum_{j=1}^{c} n_j(\\overline{Y}_j-\\overline{Y})^2 \\tag{6.15} \\end{equation}\\] The model sum of squares represents the improvement in prediction resulting from using the regression model rather than the mean of the data. The following graph shows the model sum of squares for our example: Figure 6.7: Ordinary least squares (OLS) The residual sum of squares (SSR) is the difference between the observed data and the predicted values along the regression line (i.e., the variation not explained by the model) \\[\\begin{equation} SS_R= \\sum_{j=1}^{c} \\sum_{i=1}^{n} ({Y}_{ij}-\\overline{Y}_{j})^2 \\tag{6.16} \\end{equation}\\] The following graph shows the residual sum of squares for our example: Figure 6.8: Ordinary least squares (OLS) R-squared The R2 statistic represents the proportion of variance that is explained by the model and is computed as: \\[\\begin{equation} R^2= \\frac{SS_M}{SS_T} \\tag{6.16} \\end{equation}\\] It takes values between 0 (very bad fit) and 1 (very good fit). Note that when the goal of your model is to predict future outcomes, a “too good” model fit can pose severe challenges. The reason is that the model might fit your specific sample so well, that it will only predict well within the sample but not generalize to other samples. This is called overfitting and it shows that there is a trade-off between model fit and out-of-sample predictive ability of the model, if the goal is to predict beyond the sample. You can get a first impression of the fit of the model by inspecting the scatter plot as can be seen in the plot below. If the observations are highly dispersed around the regression line (left plot), the fit will be lower compared to a data set where the values are less dispersed (right plot). Figure 6.9: Good vs. bad model fit The R2 statistic is reported in the regression output (see above). However, you could also extract the relevant sum of squares statistics from the regression object using the anova() function to compute it manually: anova(simple_regression) #anova results ## Analysis of Variance Table ## ## Response: sales ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## adspend 1 433688 433688 99.6 &lt;0.0000000000000002 *** ## Residuals 198 862264 4355 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Now we can compute R2 in the same way that we have computed Eta2 in the last section: r2 &lt;- anova(simple_regression)$&#39;Sum Sq&#39;[1]/(anova(simple_regression)$&#39;Sum Sq&#39;[1] + anova(simple_regression)$&#39;Sum Sq&#39;[2]) #compute R2 Adjusted R-squared Due to the way the R2 statistic is calculated, it will never decrease if a new explanatory variable is introduced into the model. This means that every new independent variable either doesn’t change the R2 or increases it, even if there is no real relationship between the new variable and the dependent variable. Hence, one could be tempted to just add as many variables as possible to increase the R2 and thus obtain a “better” model. However, this actually only leads to more noise and therefore a worse model. To account for this, there exists a test statistic closely related to the R2, the adjusted R2. It can be calculated as follows: \\[\\begin{equation} \\overline{R^2} = 1 - (1 - R^2)\\frac{n-1}{n - k - 1} \\tag{6.17} \\end{equation}\\] where n is the total number of observations and k is the total number of explanatory variables. The adjusted R2 is equal to or less than the regular R2 and can be negative. It will only increase if the added variable adds more explanatory power than one expect by pure chance. Essentially, it contains a “penalty” for including unnecessary variables and therefore favors more parsimonious models. As such, it is a measure of suitability, good for comparing different models and is very useful in the model selection stage of a project. In R, the standard lm() function automatically also reports the adjusted R2. F-test Another significance test is the F-test. It tests the null hypothesis: \\[H_0:R^2=0\\] This is equivalent to the following null hypothesis: \\[H_0:\\beta_1=\\beta_2=\\beta_3=\\beta_k=0\\] The F-test statistic is calculated as follows: \\[\\begin{equation} F=\\frac{\\frac{SS_M}{k}}{\\frac{SS_R}{(n-k-1)}}=\\frac{MS_M}{MS_R} \\tag{6.16} \\end{equation}\\] which has a F distribution with k number of predictors and n degrees of freedom. In other words, you divide the systematic (“explained”) variation due to the predictor variables by the unsystematic (“unexplained”) variation. The result of the F-test is provided in the regression output. However, you might manually compute the F-test using the ANOVA results from the model: anova(simple_regression) #anova results ## Analysis of Variance Table ## ## Response: sales ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## adspend 1 433688 433688 99.6 &lt;0.0000000000000002 *** ## Residuals 198 862264 4355 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 f_calc &lt;- anova(simple_regression)$&#39;Mean Sq&#39;[1]/anova(simple_regression)$&#39;Mean Sq&#39;[2] #compute F f_calc ## [1] 100 f_crit &lt;- qf(.95, df1 = 1, df2 = 100) #critical value f_crit ## [1] 3.9 f_calc &gt; f_crit #test if calculated test statistic is larger than critical value ## [1] TRUE 6.2.1.4 Using the model After fitting the model, we can use the estimated coefficients to predict sales for different values of advertising. Suppose you want to predict sales for a new product, and the company plans to spend 800 Euros on advertising. How much will it sell? You can easily compute this either by hand: \\[\\hat{sales}=134.134 + 0.09612*800=211\\] … or by extracting the estimated coefficients from the model summary: summary(simple_regression)$coefficients[1,1] + # the intercept summary(simple_regression)$coefficients[2,1]*800 # the slope * 800 ## [1] 211 The predicted value of the dependent variable is 211 units, i.e., the product will (on average) sell 211 units. The following video summarizes how to conduct simple linear regression in R 6.2.2 Multiple linear regression Multiple linear regression is a statistical technique that simultaneously tests the relationships between two or more independent variables and an interval-scaled dependent variable. The general form of the equation is given by: \\[\\begin{equation} Y=(\\beta_0+\\beta_1*X_1+\\beta_2*X_2+\\beta_n*X_n)+\\epsilon \\tag{6.5} \\end{equation}\\] Again, we aim to find the linear combination of predictors that correlate maximally with the outcome variable. Note that if you change the composition of predictors, the partial regression coefficient of an independent variable will be different from that of the bivariate regression coefficient. This is because the regressors are usually correlated, and any variation in Y that was shared by X1 and X2 was attributed to X1. The interpretation of the partial regression coefficients is the expected change in Y when X is changed by one unit and all other predictors are held constant. Let’s extend the previous example. Say, in addition to the influence of advertising, you are interested in estimating the influence of airplay on the number of album downloads. The corresponding equation would then be given by: \\[\\begin{equation} Sales=\\beta_0+\\beta_1*adspend+\\beta_2*airplay+\\epsilon \\tag{6.6} \\end{equation}\\] The words “adspend” and “airplay” represent data that we have observed on advertising expenditures and number of radio plays, and β1 and β2 represent the unknown relationship between sales and advertising expenditures and radio airplay, respectively. The coefficients tells you by how much sales will increase for an additional Euro spent on advertising (when radio airplay is held constant) and by how much sales will increase for an additional radio play (when advertising expenditures are held constant). Thus, we can make predictions about album sales based not only on advertising spending, but also on radio airplay. With several predictors, the partitioning of sum of squares is the same as in the bivariate model, except that the model is no longer a 2-D straight line. With two predictors, the regression line becomes a 3-D regression plane. In our example: Figure 6.10: Regression plane Like in the bivariate case, the plane is fitted to the data with the aim to predict the observed data as good as possible. The deviation of the observations from the plane represent the residuals (the error we make in predicting the observed data from the model). Note that this is conceptually the same as in the bivariate case, except that the computation is more complex (we won’t go into details here). The model is fairly easy to plot using a 3-D scatterplot, because we only have two predictors. While multiple regression models that have more than two predictors are not as easy to visualize, you may apply the same principles when interpreting the model outcome: Total sum of squares (SST) is still the difference between the observed data and the mean value of Y (total variation) Residual sum of squares (SSR) is still the difference between the observed data and the values predicted by the model (unexplained variation) Model sum of squares (SSM) is still the difference between the values predicted by the model and the mean value of Y (explained variation) R measures the multiple correlation between the predictors and the outcome R2 is the amount of variation in the outcome variable explained by the model Estimating multiple regression models is straightforward using the lm() function. You just need to separate the individual predictors on the right hand side of the equation using the + symbol. For example, the model: \\[\\begin{equation} Sales=\\beta_0+\\beta_1*adspend+\\beta_2*airplay+\\beta_3*starpower+\\epsilon \\tag{6.6} \\end{equation}\\] could be estimated as follows: multiple_regression &lt;- lm(sales ~ adspend + airplay + starpower, data = regression) #estimate linear model summary(multiple_regression) #summary of results ## ## Call: ## lm(formula = sales ~ adspend + airplay + starpower, data = regression) ## ## Residuals: ## Min 1Q Median 3Q Max ## -121.32 -28.34 -0.45 28.97 144.13 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -26.61296 17.35000 -1.53 0.13 ## adspend 0.08488 0.00692 12.26 &lt; 0.0000000000000002 *** ## airplay 3.36743 0.27777 12.12 &lt; 0.0000000000000002 *** ## starpower 11.08634 2.43785 4.55 0.0000095 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 47 on 196 degrees of freedom ## Multiple R-squared: 0.665, Adjusted R-squared: 0.66 ## F-statistic: 129 on 3 and 196 DF, p-value: &lt;0.0000000000000002 The interpretation of the coefficients is as follows: adspend (β1): when advertising expenditures increase by 1 Euro, sales will increase by 0.085 units airplay (β2): when radio airplay increases by 1 play per week, sales will increase by 3.367 units starpower (β3): when the number of previous albums increases by 1, sales will increase by 11.086 units The associated t-values and p-values are also given in the output. You can see that the p-values are smaller than 0.05 for all three coefficients. Hence, all effects are “significant”. This means that if the null hypothesis was true (i.e., there was no effect between the variables and sales), the probability of observing associations of the estimated magnitudes (or larger) is very small (e.g., smaller than 0.05). Again, to get a better feeling for the range of values that the coefficients could take, it is helpful to compute confidence intervals. confint(multiple_regression) ## 2.5 % 97.5 % ## (Intercept) -60.830 7.604 ## adspend 0.071 0.099 ## airplay 2.820 3.915 ## starpower 6.279 15.894 What does this tell you? Recall that a 95% confidence interval is defined as a range of values such that with a 95% probability, the range will contain the true unknown value of the parameter. For example, for β3, the confidence interval is [6.2785522,15.8941182]. Thus, although we have computed a point estimate of 11.086 for the effect of starpower on sales based on our sample, the effect might actually just as well take any other value within this range, considering the sample size and the variability in our data. The output also tells us that 66.4667687% of the variation can be explained by our model. You may also visually inspect the fit of the model by plotting the predicted values against the observed values. We can extract the predicted values using the predict() function. So let’s create a new variable yhat, which contains those predicted values. regression$yhat &lt;- predict(simple_regression) We can now use this variable to plot the predicted values against the observed values. In the following plot, the model fit would be perfect if all points would fall on the diagonal line. The larger the distance between the points and the line, the worse the model fit. ggplot(regression,aes(yhat,sales)) + geom_point(size=2,shape=1) + #Use hollow circles scale_x_continuous(name=&quot;predicted values&quot;) + scale_y_continuous(name=&quot;observed values&quot;) + geom_abline(intercept = 0, slope = 1) + theme_bw() Figure 6.11: Model fit Partial plots In the context of a simple linear regression (i.e., with a single independent variable), a scatter plot of the dependent variable against the independent variable provides a good indication of the nature of the relationship. If there is more than one independent variable, however, things become more complicated. The reason is that although the scatter plot still show the relationship between the two variables, it does not take into account the effect of the other independent variables in the model. Partial regression plot show the effect of adding another variable to a model that already controls for the remaining variables in the model. In other words, it is a scatterplot of the residuals of the outcome variable and each predictor when both variables are regressed separately on the remaining predictors. As an example, consider the effect of advertising expenditures on sales. In this case, the partial plot would show the effect of adding advertising expenditures as an explanatory variable while controlling for the variation that is explained by airplay and starpower in both variables (sales and advertising). Think of it as the purified relationship between advertising and sales that remains after controlling for other factors. The partial plots can easily be created using the avPlots() function from the car package: library(car) avPlots(multiple_regression) Figure 6.12: Partial plots Using the model After fitting the model, we can use the estimated coefficients to predict sales for different values of advertising, airplay, and starpower. Suppose you would like to predict sales for a new music album with advertising expenditures of 800, airplay of 30 and starpower of 5. How much will it sell? \\[\\hat{sales}=−26.61 + 0.084 * 800 + 3.367*30 + 11.08 ∗ 5= 197.74\\] … or by extracting the estimated coefficients: summary(multiple_regression)$coefficients[1,1] + summary(multiple_regression)$coefficients[2,1]*800 + summary(multiple_regression)$coefficients[3,1]*30 + summary(multiple_regression)$coefficients[4,1]*5 ## [1] 198 The predicted value of the dependent variable is 198 units, i.e., the product will sell 198 units. Comparing effects Using the output from the regression model above, it is difficult to compare the effects of the independent variables because they are all measured on different scales (Euros, radio plays, releases). Standardized regression coefficients can be used to judge the relative importance of the predictor variables. Standardization is achieved by multiplying the unstandardized coefficient by the ratio of the standard deviations of the independent and dependent variables: \\[\\begin{equation} B_{k}=\\beta_{k} * \\frac{s_{x_k}}{s_y} \\tag{6.18} \\end{equation}\\] Hence, the standardized coefficient will tell you by how many standard deviations the outcome will change as a result of a one standard deviation change in the predictor variable. Standardized coefficients can be easily computed using the lm.beta() function from the lm.beta package. library(lm.beta) lm.beta(multiple_regression) ## ## Call: ## lm(formula = sales ~ adspend + airplay + starpower, data = regression) ## ## Standardized Coefficients:: ## (Intercept) adspend airplay starpower ## 0.00 0.51 0.51 0.19 The results show that for adspend and airplay, a change by one standard deviation will result in a 0.51 standard deviation change in sales, whereas for starpower, a one standard deviation change will only lead to a 0.19 standard deviation change in sales. Hence, while the effects of adspend and airplay are comparable in magnitude, the effect of starpower is less strong. The following video summarizes how to conduct multiple regression in R 6.3 Potential problems Once you have built and estimated your model it is important to run diagnostics to ensure that the results are accurate. In the following section we will discuss common problems. 6.3.1 Outliers The following video summarizes how to handle outliers in R Outliers are data points that differ vastly from the trend. They can introduce bias into a model due to the fact that they alter the parameter estimates. Consider the example below. A linear regression was performed twice on the same data set, except during the second estimation the two green points were changed to be outliers by being moved to the positions indicated in red. The solid red line is the regression line based on the unaltered data set, while the dotted line was estimated using the altered data set. As you can see the second regression would lead to different conclusions than the first. Therefore it is important to identify outliers and further deal with them. Figure 6.13: Effects of outliers One quick way to visually detect outliers is by creating a scatterplot (as above) to see whether anything seems off. Another approach is to inspect the studentized residuals. If there are no outliers in your data, about 95% will be between -2 and 2, as per the assumptions of the normal distribution. Values well outside of this range are unlikely to happen by chance and warrant further inspection. As a rule of thumb, observations whose studentized residuals are greater than 3 in absolute values are potential outliers. The studentized residuals can be obtained in R with the function rstudent(). We can use this function to create a new variable that contains the studentized residuals e music sales regression from before yields the following residuals: regression$stud_resid &lt;- rstudent(multiple_regression) head(regression) A good way to visually inspect the studentized residuals is to plot them in a scatterplot and roughly check if most of the observations are within the -3, 3 bounds. plot(1:nrow(regression),regression$stud_resid, ylim=c(-3.3,3.3)) #create scatterplot abline(h=c(-3,3),col=&quot;red&quot;,lty=2) #add reference lines Figure 6.14: Plot of the studentized residuals To identify potentially influential observations in our data set, we can apply a filter to our data: outliers &lt;- subset(regression,abs(stud_resid)&gt;3) outliers After a detailed inspection of the potential outliers, you might decide to delete the affected observations from the data set or not. If an outlier has resulted from an error in data collection, then you might simply remove the observation. However, even though data may have extreme values, they might not be influential to determine a regression line. That means, the results wouldn’t be much different if we either include or exclude them from analysis. This means that the decision of whether to exclude an outlier or not is closely related to the question whether this observation is an influential observation, as will be discussed next. 6.3.2 Influential observations Related to the issue of outliers is that of influential observations, meaning observations that exert undue influence on the parameters. It is possible to determine whether or not the results are driven by an influential observation by calculating how far the predicted values for your data would move if the model was fitted without this particular observation. This calculated total distance is called Cook’s distance. To identify influential observations, we can inspect the respective plots created from the model output. A rule of thumb to determine whether an observation should be classified as influential or not is to look for observation with a Cook’s distance &gt; 1 (although opinions vary on this). The following plot can be used to see the Cook’s distance associated with each data point: plot(multiple_regression,4) Figure 6.15: Cook’s distance It is easy to see that none of the Cook’s distance values is close to the critical value of 1. Another useful plot to identify influential observations is plot number 5 from the output: plot(multiple_regression,5) Figure 6.16: Residuals vs. Leverage In this plot, we look for cases outside of a dashed line, which represents Cook’s distance. Lines for Cook’s distance thresholds of 0.5 and 1 are included by default. In our example, this line is not even visible, since the Cook’s distance values are far away from the critical values. Generally, you would watch out for outlying values at the upper right corner or at the lower right corner of the plot. Those spots are the places where cases can be influential against a regression line. In our example, there are no influential cases. To see how influential observations can impact your regression, have a look at this example. 6.3.3 Non-linearity An important underlying assumption for OLS is that of linearity, meaning that the relationship between the dependent and the independent variable can be reasonably approximated in linear terms. One quick way to assess whether a linear relationship can be assumed is to inspect the added variable plots that we already came across earlier: library(car) avPlots(multiple_regression) Figure 6.17: Partial plots In our example, it appears that linear relationships can be reasonably assumed. Please note, however, that the assumption of linearity implies two things: Constant marginal returns (e.g., an increase in ad-spend from 10€ to 11€ yields the same increase in sales as an increase from 100,000€ to 100,001€) Elasticities increase with X (e.g., advertising becomes relatively more effective; i.e., a relatively smaller change in advertising expenditure will yield the same return) These assumptions may not be justifiable in certain contexts and you might have to transform your data (e.g., using log-transformations) in these cases, as we will see below. 6.3.4 Non-constant error variance The following video summarizes how to identify non-constant error variance in R Another important assumption of the linear model is that the error terms have a constant variance (i.e., homoscedasticity). The following plot from the model output shows the residuals (the vertical distance from an observed value to the predicted values) versus the fitted values (the predicted value from the regression model). If all the points fell exactly on the dashed grey line, it would mean that we have a perfect prediction. The residual variance (i.e., the spread of the values on the y-axis) should be similar across the scale of the fitted values on the x-axis. plot(multiple_regression, 1) Figure 6.18: Residuals vs. fitted values In our case, this appears to be the case. You can identify non-constant variances in the errors (i.e., heteroscedasticity) from the presence of a funnel shape in the above plot. When the assumption of constant error variances is not met, this might be due to a misspecification of your model (e.g., the relationship might not be linear). In these cases, it often helps to transform your data (e.g., using log-transformations). The red line also helps you to identify potential misspecification of your model. It is a smoothed curve that passes through the residuals and if it lies close to the gray dashed line (as in our case) it suggest a correct specification. If the line would deviate from the dashed grey line a lot (e.g., a U-shape or inverse U-shape), it would suggest that the linear model specification is not reasonable and you should try different specifications. If OLS is performed despite heteroscedasticity, the estimates of the coefficient will still be correct on average. However, the estimator is inefficient, meaning that the standard error is wrong, which will impact the significance tests (i.e., the p-values will be wrong). However, there are also robust regression methods, which you can use to estimate your model despite the presence of heteroscedasticity. 6.3.5 Non-normally distributed errors Another assumption of OLS is that the error term is normally distributed. This can be a reasonable assumption for many scenarios, but we still need a way to check if it is actually the case. As we can not directly observe the actual error term, we have to work with the next best thing - the residuals. A quick way to assess whether a given sample is approximately normally distributed is by using Q-Q plots. These plot the theoretical position of the observations (under the assumption that they are normally distributed) against the actual position. The plot below is created by the model output and shows the residuals in a Q-Q plot. As you can see, most of the points roughly follow the theoretical distribution, as given by the straight line. If most of the points are close to the line, the data is approximately normally distributed. plot(multiple_regression,2) Figure 6.19: Q-Q plot Another way to check for normal distribution of the data is to employ statistical tests that test the null hypothesis that the data is normally distributed, such as the Shapiro–Wilk test. We can extract the residuals from our model using the resid() function and apply the shapiro.test() function to it: shapiro.test(resid(multiple_regression)) ## ## Shapiro-Wilk normality test ## ## data: resid(multiple_regression) ## W = 1, p-value = 0.7 As you can see, we can not reject the H0 of the normally distributed residuals, which means that we can assume the residuals to be approximately normally distributed. When the assumption of normally distributed errors is not met, this might again be due to a misspecification of your model, in which case it might help to transform your data (e.g., using log-transformations). 6.3.6 Correlation of errors The assumption of independent errors implies that for any two observations the residual terms should be uncorrelated. This is also known as a lack of autocorrelation. In theory, this could be tested with the Durbin-Watson test, which checks whether adjacent residuals are correlated. However, be aware that the test is sensitive to the order of your data. Hence, it only makes sense if there is a natural order in the data (e.g., time-series data) when the presence of dependent errors indicates autocorrelation. Since there is no natural order in our data, we don’t need to apply this test.. If you are confronted with data that has a natural order, you can performed the test using the command durbinWatsonTest(), which takes the object that the lm() function generates as an argument. The test statistic varies between 0 and 4, with values close to 2 being desirable. As a rule of thumb values below 1 and above 3 are causes for concern. 6.3.7 Collinearity Linear dependence of regressors, also known as multicollinearity, is when there is a strong linear relationship between the independent variables. Some correlation will always be present, but severe correlation can make proper estimation impossible. When present, it affects the model in several ways: Limits the size of R2: when two variables are highly correlated, the amount of unique explained variance is low; therefore the incremental change in R2 by including an additional predictor is larger if the predictor is uncorrelated with the other predictors. Increases the standard errors of the coefficients, making them less trustworthy. Uncertainty about the importance of predictors: if two predictors explain similar variance in the outcome, we cannot know which of these variables is important. A quick way to find obvious multicollinearity is to examine the correlation matrix of the data. Any value &gt; 0.8 - 0.9 should be cause for concern. You can, for example, create a correlation matrix using the rcorr() function from the Hmisc package. library(&quot;Hmisc&quot;) rcorr(as.matrix(regression[,c(&quot;adspend&quot;,&quot;airplay&quot;,&quot;starpower&quot;)])) ## adspend airplay starpower ## adspend 1.00 0.10 0.08 ## airplay 0.10 1.00 0.18 ## starpower 0.08 0.18 1.00 ## ## n= 200 ## ## ## P ## adspend airplay starpower ## adspend 0.1511 0.2557 ## airplay 0.1511 0.0099 ## starpower 0.2557 0.0099 The bivariate correlations can also be show in a plot: plot(regression[,c(&quot;adspend&quot;,&quot;airplay&quot;,&quot;starpower&quot;)]) Figure 6.20: Bivariate correlation plots However, this only spots bivariate multicollinearity. Variance inflation factors can be used to spot more subtle multicollinearity arising from multivariate relationships. It is calculated by regressing Xi on all other X and using the resulting R2 to calculate \\[\\begin{equation} \\begin{split} \\frac{1}{1 - R_i^2} \\end{split} \\tag{6.19} \\end{equation}\\] VIF values of over 4 are certainly cause for concern and values over 2 should be further investigated. If the average VIF is over 1 the regression may be biased. The VIF for all variables can easily be calculated in R with the vif() function. library(car) vif(multiple_regression) ## adspend airplay starpower ## 1 1 1 As you can see the values are well below the cutoff, indicating that we do not have to worry about multicollinearity in our example. 6.3.8 Omitted Variables If a variable that influences the outcome is left out of the model (“omitted”), a bias in other variables’ coefficients might be introduced. Specifically, the other coefficients will be biased if the corresponding variables are correlated with the omitted variable. Intuitively, the variables left in the model “pick up” the effect of the omitted variable to the degree that they are related. Let’s illustrate this with an example. Consider the following data on the number of people visiting concerts of smaller bands. head(concerts) The data set contains three variables: avg_rating: The average rating a band has, resulting from a ten-point scale. followers: The number of followers the band has at the time of the concert. concert_visitors: The number of tickets sold for the concert. If we estimate a model to explain the number of tickets sold as a function of the average rating and the number of followers, the results would look as follows: concert_mod &lt;- lm(concert_visitors ~ avg_rating + followers, data = concerts) summary(concert_mod) ## ## Call: ## lm(formula = concert_visitors ~ avg_rating + followers, data = concerts) ## ## Residuals: ## Min 1Q Median 3Q Max ## -37.99 -13.64 -2.17 11.50 67.33 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -2.810495 5.778273 -0.49 0.63 ## avg_rating 20.512461 0.708420 28.96 &lt;0.0000000000000002 *** ## followers 0.199940 0.000755 265.00 &lt;0.0000000000000002 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 20 on 97 degrees of freedom ## Multiple R-squared: 0.999, Adjusted R-squared: 0.999 ## F-statistic: 3.95e+04 on 2 and 97 DF, p-value: &lt;0.0000000000000002 Now assume we don’t have data on the number of followers a band has, but we still have information on the average rating and want to explain the number of tickets sold. Fitting a linear model with just the avg_rating variable included yields the following results: concert_mod2 &lt;- lm(concert_visitors ~ avg_rating, data = concerts) summary(concert_mod2) ## ## Call: ## lm(formula = concert_visitors ~ avg_rating, data = concerts) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1040.1 -403.6 -16.7 360.7 952.9 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1114.8 105.8 10.5 &lt; 0.0000000000000002 *** ## avg_rating 64.5 18.4 3.5 0.00071 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 523 on 98 degrees of freedom ## Multiple R-squared: 0.111, Adjusted R-squared: 0.102 ## F-statistic: 12.2 on 1 and 98 DF, p-value: 0.00071 What happens to the coefficient of avg_rating? Because avg_rating and followersare not independent (e.g. one could argue that bands with a higher average rating probably have more followers) the coefficient will be biased. In our case we massively overestimate the effect that the average rating of a band has on ticket sales. In the original model, the effect was about 20.5. In the new, smaller model, the effect is approximately 3.1 times higher. We can also work out intuitively what the bias will be. The marginal effect of followers on concert_visitors is captured by avg_rating to the degree that avg_rating is related to followers. There are two coefficients of interest: What is the marginal effect of followers on concert_visitors? How much of that effect is captured by avg_rating? The former is just the coefficient of followers in the original regression. followers_concert_mod &lt;- as.numeric(coef(concert_mod)[&quot;followers&quot;]) # Marginal effect of followers on sales followers_concert_mod ## [1] 0.2 The latter is the coefficient of avg_rating obtained from a regression on followers, since the coefficient shows how avg_rating and followers relate to each other. mod_follow_rating &lt;- lm(followers ~ avg_rating, data = concerts) follow_rating &lt;- as.numeric(coef(mod_follow_rating)[&quot;avg_rating&quot;]) # Relation between followers and avg_rating follow_rating ## [1] 220 Now we can calculate the bias induced by omitting followers bias &lt;- followers_concert_mod * follow_rating cat(&quot;Bias:&quot;, bias) ## Bias: 44 To calculate the biased coefficient, simply add the bias to the coefficient from the original model. b1_bias &lt;- coef(concert_mod)[&quot;avg_rating&quot;] + bias cat(&quot;Biased Beta:&quot;, b1_bias) ## Biased Beta: 65 6.4 Categorical predictors 6.4.1 Two categories Suppose, you wish to investigate the effect of the variable “country” on sales, which is a categorical variable that can only take two levels (i.e., 0 = local artist, 1 = international artist). Categorical variables with two levels are also called binary predictors. It is straightforward to include these variables in your model as “dummy” variables. Dummy variables are factor variables that can only take two values. For our “country” variable, we can create a new predictor variable that takes the form: \\[\\begin{equation} x_4 = \\begin{cases} 1 &amp; \\quad \\text{if } i \\text{th artist is international}\\\\ 0 &amp; \\quad \\text{if } i \\text{th artist is local} \\end{cases} \\tag{6.20} \\end{equation}\\] This new variable is then added to our regression equation from before, so that the equation becomes \\[\\begin{align} Sales =\\beta_0 &amp;+\\beta_1*adspend\\\\ &amp;+\\beta_2*airplay\\\\ &amp;+\\beta_3*starpower\\\\ &amp;+\\beta_4*international+\\epsilon \\end{align}\\] where “international” represents the new dummy variable and \\(\\beta_4\\) is the coefficient associated with this variable. Estimating the model is straightforward - you just need to include the variable as an additional predictor variable. Note that the variable needs to be specified as a factor variable before including it in your model. If you haven’t converted it to a factor variable before, you could also use the wrapper function as.factor() within the equation. multiple_regression_bin &lt;- lm(sales ~ adspend + airplay + starpower + country, data = regression) #estimate linear model summary(multiple_regression_bin) #summary of results ## ## Call: ## lm(formula = sales ~ adspend + airplay + starpower + country, ## data = regression) ## ## Residuals: ## Min 1Q Median 3Q Max ## -109.20 -24.30 -1.82 29.19 156.31 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -16.40060 16.39540 -1.00 0.32 ## adspend 0.08146 0.00653 12.48 &lt; 0.0000000000000002 *** ## airplay 3.03766 0.26809 11.33 &lt; 0.0000000000000002 *** ## starpower 10.08100 2.29546 4.39 0.00001843 *** ## countryinternational 45.67274 8.69117 5.26 0.00000039 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 44 on 195 degrees of freedom ## Multiple R-squared: 0.706, Adjusted R-squared: 0.7 ## F-statistic: 117 on 4 and 195 DF, p-value: &lt;0.0000000000000002 You can see that we now have an additional coefficient in the regression output, which tells us the effect of the binary predictor. The dummy variable can generally be interpreted as the average difference in the dependent variable between the two groups (similar to a t-test). In this case, the coefficient tells you the difference in sales between international and local artists, and whether this difference is significant. Specifically, it means that international artists on average sell 45.67 units more than local artists, and this difference is significant (i.e., p &lt; 0.05). 6.4.2 More than two categories Predictors with more than two categories, like our “genre”\" variable, can also be included in your model. However, in this case one dummy variable cannot represent all possible values, since there are three genres (i.e., 1 = Rock, 2 = Pop, 3 = Electronic). Thus, we need to create additional dummy variables. For example, for our “genre” variable, we create two dummy variables as follows: \\[\\begin{equation} x_5 = \\begin{cases} 1 &amp; \\quad \\text{if } i \\text{th product is from Pop genre}\\\\ 0 &amp; \\quad \\text{if } i \\text{th product is from Rock genre} \\end{cases} \\tag{6.21} \\end{equation}\\] \\[\\begin{equation} x_6 = \\begin{cases} 1 &amp; \\quad \\text{if } i \\text{th product is from Electronic genre}\\\\ 0 &amp; \\quad \\text{if } i \\text{th product is from Rock genre} \\end{cases} \\tag{6.22} \\end{equation}\\] We would then add these variables as additional predictors in the regression equation and obtain the following model \\[\\begin{align} Sales =\\beta_0 &amp;+\\beta_1*adspend\\\\ &amp;+\\beta_2*airplay\\\\ &amp;+\\beta_3*starpower\\\\ &amp;+\\beta_4*international\\\\ &amp;+\\beta_5*Pop\\\\ &amp;+\\beta_6*Electronic+\\epsilon \\end{align}\\] where “Pop” and “Rock” represent our new dummy variables, and \\(\\beta_5\\) and \\(\\beta_6\\) represent the associated regression coefficients. The interpretation of the coefficients is as follows: \\(\\beta_5\\) is the difference in average sales between the genres “Rock” and “Pop”, while \\(\\beta_6\\) is the difference in average sales between the genres “Rock” and “Electro”. Note that the level for which no dummy variable is created is also referred to as the baseline. In our case, “Rock” would be the baseline genre. This means that there will always be one fewer dummy variable than the number of levels. You don’t have to create the dummy variables manually as R will do this automatically when you add the variable to your equation: multiple_regression &lt;- lm(sales ~ adspend + airplay + starpower+ country + genre, data = regression) #estimate linear model summary(multiple_regression) #summary of results ## ## Call: ## lm(formula = sales ~ adspend + airplay + starpower + country + ## genre, data = regression) ## ## Residuals: ## Min 1Q Median 3Q Max ## -116.18 -26.54 0.05 27.98 154.56 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -30.67901 16.59989 -1.85 0.06611 . ## adspend 0.07233 0.00657 11.00 &lt; 0.0000000000000002 *** ## airplay 2.71418 0.26824 10.12 &lt; 0.0000000000000002 *** ## starpower 10.49628 2.19380 4.78 0.0000034 *** ## countryinternational 40.87988 8.40868 4.86 0.0000024 *** ## genrepop 47.69640 10.48717 4.55 0.0000095 *** ## genreelectronic 27.62034 8.17223 3.38 0.00088 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 42 on 193 degrees of freedom ## Multiple R-squared: 0.735, Adjusted R-squared: 0.727 ## F-statistic: 89.2 on 6 and 193 DF, p-value: &lt;0.0000000000000002 How can we interpret the coefficients? It is estimated based on our model that products from the “Pop” genre will on average sell 47.69 units more than products from the “Rock” genre, and that products from the “Electronic” genre will sell on average 27.62 units more than the products from the “Rock” genre. The p-value of both variables is smaller than 0.05, suggesting that there is statistical evidence for a real difference in sales between the genres. The level of the baseline category is arbitrary. As you have seen, R simply selects the first level as the baseline. If you would like to use a different baseline category, you can use the relevel() function and set the reference category using the ref argument. The following would estimate the same model using the second category as the baseline: multiple_regression &lt;- lm(sales ~ adspend + airplay + starpower+ country + relevel(genre,ref=2), data = regression) #estimate linear model summary(multiple_regression) #summary of results ## ## Call: ## lm(formula = sales ~ adspend + airplay + starpower + country + ## relevel(genre, ref = 2), data = regression) ## ## Residuals: ## Min 1Q Median 3Q Max ## -116.18 -26.54 0.05 27.98 154.56 ## ## Coefficients: ## Estimate Std. Error t value ## (Intercept) 17.01739 18.19704 0.94 ## adspend 0.07233 0.00657 11.00 ## airplay 2.71418 0.26824 10.12 ## starpower 10.49628 2.19380 4.78 ## countryinternational 40.87988 8.40868 4.86 ## relevel(genre, ref = 2)rock -47.69640 10.48717 -4.55 ## relevel(genre, ref = 2)electronic -20.07606 7.98747 -2.51 ## Pr(&gt;|t|) ## (Intercept) 0.351 ## adspend &lt; 0.0000000000000002 *** ## airplay &lt; 0.0000000000000002 *** ## starpower 0.0000034 *** ## countryinternational 0.0000024 *** ## relevel(genre, ref = 2)rock 0.0000095 *** ## relevel(genre, ref = 2)electronic 0.013 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 42 on 193 degrees of freedom ## Multiple R-squared: 0.735, Adjusted R-squared: 0.727 ## F-statistic: 89.2 on 6 and 193 DF, p-value: &lt;0.0000000000000002 Note that while your choice of the baseline category impacts the coefficients and the significance level, the prediction for each group will be the same regardless of this choice. 6.5 Extensions of the linear model The standard linear regression model provides results that are easy to interpret and is useful to address many real-world problems. However, it makes rather restrictive assumptions that might be violated in many cases. Notably, it assumes that the relationships between the response and predictor variable is additive and linear. The additive assumption states that the effect of an independent variable on the dependent variable is independent of the values of the other independent variables included in the model. The linear assumption means that the effect of a one-unit change in the independent variable on the dependent variable is the same, regardless of the values of the value of the independent variable. This is also referred to as constant marginal returns. For example, an increase in ad-spend from 10€ to 11€ yields the same increase in sales as an increase from 100,000€ to 100,001€. This section presents alternative model specifications if the assumptions do not hold. 6.5.1 Interaction effects Regarding the additive assumption, it might be argued that the effect of some variables are not fully independent of the values of other variables. In our example, one could argue that the effect of advertising depends on the type of artist. For example, for local artist advertising might be more effective. We can investigate if this is the case using a grouped scatterplot: ggplot(regression, aes(adspend, sales, colour = as.factor(country))) + geom_point() + geom_smooth(method=&quot;lm&quot;, alpha=0.1) + labs(x = &quot;Advertising expenditures (EUR)&quot;, y = &quot;Number of sales&quot;, colour=&quot;country&quot;) + theme_bw() Figure 6.21: Effect of advertising by group The scatterplot indeed suggests that there is a difference in advertising effectiveness between local and international artists. You can see this from the two different regression lines. We can incorporate this interaction effect by including an interaction term in the regression equation as follows: \\[\\begin{align} Sales =\\beta_0 &amp;+\\beta_1*adspend\\\\ &amp;+\\beta_2*airplay\\\\ &amp;+\\beta_3*starpower\\\\ &amp;+\\beta_4*international\\\\ &amp;+\\beta_5*(adspend*international)\\\\ &amp;+\\epsilon \\end{align}\\] You can see that the effect of advertising now depends on the type of artist. Hence, the additive assumption is removed. Note that if you decide to include an interaction effect, you should always include the main effects of the variables that are included in the interaction (even if the associated p-values do not suggest significant effects). It is easy to include an interaction effect in you model by adding an additional variable that has the format ```var1:var2````. In our example, this could be achieved using the following specification: multiple_regression &lt;- lm(sales ~ adspend + airplay + starpower + country + adspend:country, data = regression) #estimate linear model summary(multiple_regression) #summary of results ## ## Call: ## lm(formula = sales ~ adspend + airplay + starpower + country + ## adspend:country, data = regression) ## ## Residuals: ## Min 1Q Median 3Q Max ## -105.43 -26.26 -2.72 29.35 161.53 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -14.1169 16.2937 -0.87 0.387 ## adspend 0.0885 0.0073 12.13 &lt; 0.0000000000000002 ## airplay 2.9575 0.2686 11.01 &lt; 0.0000000000000002 ## starpower 9.4373 2.2969 4.11 0.0000587 ## countryinternational 71.5753 15.1288 4.73 0.0000043 ## adspend:countryinternational -0.0347 0.0167 -2.08 0.039 ## ## (Intercept) ## adspend *** ## airplay *** ## starpower *** ## countryinternational *** ## adspend:countryinternational * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 44 on 194 degrees of freedom ## Multiple R-squared: 0.713, Adjusted R-squared: 0.705 ## F-statistic: 96.2 on 5 and 194 DF, p-value: &lt;0.0000000000000002 How can we interpret the coefficient? The adspend main effect tells you the effect of advertising for the reference group that has the factor level zero. In our example, it is the advertising effect for local artist. This means that for local artists, spending an additional 1,000 Euros on advertising will result in approximately 89 additional unit sales. The interaction effect tells you by how much the effect differs for the other group (i.e., international artists) and whether this difference is significant. In our example, it means that the effect for international artists can be computed as: 0.0885 - 0.0347 = 0.0538. This means that for international artists, spending an additional 1,000 Euros on advertising will result in approximately 54 additional unit sales. Since the interaction effect is significant (p &lt; 0.05) we can conclude that advertising is less effective for international artists. The above example showed the interaction between a categorical variable (i.e., “country”) and a continuous variable (i.e., “adspend”). However, interaction effects can be defined for different combinations of variable types. For example, you might just as well specify an interaction between two continuous variables. In our example, you might suspect that there are synergy effects between advertising expenditures and radio airplay. It could be that advertising is more effective when an artist receives a large number of radio plays. In this case, we would specify our model as: \\[\\begin{align} Sales =\\beta_0 &amp;+\\beta_1*adspend\\\\ &amp;+\\beta_2*airplay\\\\ &amp;+\\beta_3*starpower\\\\ &amp;+\\beta_4*(adspend*airplay)\\\\ &amp;+\\epsilon \\end{align}\\] In this case, we can interpret \\(\\beta_4\\) as the increase in the effectiveness of advertising for a one unit increase in radio airplay (or vice versa). This can be translated to R using: multiple_regression &lt;- lm(sales ~ adspend + airplay + starpower + adspend:airplay, data = regression) #estimate linear model summary(multiple_regression) #summary of results ## ## Call: ## lm(formula = sales ~ adspend + airplay + starpower + adspend:airplay, ## data = regression) ## ## Residuals: ## Min 1Q Median 3Q Max ## -128.30 -28.59 -2.15 28.04 141.59 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -34.520477 19.311540 -1.79 0.075 . ## adspend 0.104387 0.022004 4.74 0.0000040434105916 *** ## airplay 3.680426 0.435398 8.45 0.0000000000000066 *** ## starpower 10.890242 2.447672 4.45 0.0000144701215058 *** ## adspend:airplay -0.000649 0.000695 -0.93 0.352 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 47 on 195 degrees of freedom ## Multiple R-squared: 0.666, Adjusted R-squared: 0.659 ## F-statistic: 97.3 on 4 and 195 DF, p-value: &lt;0.0000000000000002 However, since the p-value of the interaction is larger than 0.05, there is little statistical evidence for an interaction between the two variables. 6.5.2 Non-linear relationships In our example above, it appeared that linear relationships could be reasonably assumed. In many practical applications, however, this might not be the case. Let’s review the implications of a linear specification again: Constant marginal returns (e.g., an increase in ad-spend from 10€ to 11€ yields the same increase in sales as an increase from 100,000€ to 100,001€) Elasticities increase with X (e.g., advertising becomes relatively more effective; i.e., a relatively smaller change in advertising expenditure will yield the same return) In many marketing contexts, these might not be reasonable assumptions. Consider the case of advertising. It is unlikely that the return on advertising will not depend on the level of advertising expenditures. It is rather likely that saturation occurs at some level, meaning that the return from an additional Euro spend on advertising is decreasing with the level of advertising expenditures (i.e., decreasing marginal returns). In other words, at some point the advertising campaign has achieved a certain level of penetration and an additional Euro spend on advertising won’t yield the same return as in the beginning. Let’s use an example data set, containing the advertising expenditures of a company and the sales (in thousand units). non_linear_reg &lt;- read.table(&quot;https://raw.githubusercontent.com/IMSMWU/Teaching/master/MRDA2017/non_linear.dat&quot;, sep = &quot;\\t&quot;, header = TRUE) #read in data head(non_linear_reg) Now we inspect if a linear specification is appropriate by looking at the scatterplot: ggplot(data = non_linear_reg, aes(x = advertising, y = sales)) + geom_point(shape=1) + geom_smooth(method = &quot;lm&quot;, fill = &quot;blue&quot;, alpha=0.1) + theme_bw() Figure 6.22: Non-linear relationship It appears that a linear model might not represent the data well. It rather appears that the effect of an additional Euro spend on advertising is decreasing with increasing levels of advertising expenditures. Thus, we have decreasing marginal returns. We could put this to a test and estimate a linear model: linear_reg &lt;- lm(sales ~ advertising, data = non_linear_reg) summary(linear_reg) ## ## Call: ## lm(formula = sales ~ advertising, data = non_linear_reg) ## ## Residuals: ## Min 1Q Median 3Q Max ## -10.477 -2.389 -0.356 2.188 16.745 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 9.9575216 0.2251151 44.2 &lt;0.0000000000000002 *** ## advertising 0.0005024 0.0000156 32.2 &lt;0.0000000000000002 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.6 on 998 degrees of freedom ## Multiple R-squared: 0.509, Adjusted R-squared: 0.509 ## F-statistic: 1.04e+03 on 1 and 998 DF, p-value: &lt;0.0000000000000002 Advertising appears to be positively related to sales with an additional Euro that is spent on advertising resulting in 0.0005 additional sales. The R2 statistic suggests that approximately 51% of the total variation can be explained by the model To test if the linear specification is appropriate, let’s inspect some of the plots that are generated by R. We start by inspecting the residuals plot. plot(linear_reg,1) Figure 6.23: Residuals vs. Fitted The plot suggests that the assumption of homoscedasticity is violated (i.e., the spread of values on the y-axis is different for different levels of the fitted values). In addition, the red line deviates from the dashed grey line, suggesting that the relationship might not be linear. Finally, the Q-Q plot of the residuals suggests that the residuals are not normally distributed. plot(linear_reg,2) Figure 6.24: Q-Q plot To sum up, a linear specification might not be the best model for this data set. In this case, a multiplicative model might be a better representation of the data. The multiplicative model has the following formal representation: \\[\\begin{equation} Y =\\beta_0 *X_1^{\\beta_1}*X_2^{\\beta_2}*...*X_J^{\\beta_J}*\\epsilon \\tag{6.23} \\end{equation}\\] This functional form can be linearized by taking the logarithm of both sides of the equation: \\[\\begin{equation} log(Y) =log(\\beta_0) + \\beta_1*log(X_1) + \\beta_2*log(X_2) + ...+ \\beta_J*log(X_J) + log(\\epsilon) \\tag{6.24} \\end{equation}\\] This means that taking logarithms of both sides of the equation makes linear estimation possible. Let’s test how the scatterplot would look like if we use the logarithm of our variables using the log() function instead of the original values. ggplot(data = non_linear_reg, aes(x = log(advertising), y = log(sales))) + geom_point(shape=1) + geom_smooth(method = &quot;lm&quot;, fill = &quot;blue&quot;, alpha=0.1) + theme_bw() Figure 6.25: Linearized effect It appears that now, with the log-transformed variables, a linear specification is a much better representation of the data. Hence, we can log-transform our variables and estimate the following equation: \\[\\begin{equation} log(sales) = log(\\beta_0) + \\beta_1*log(advertising) + log(\\epsilon) \\tag{6.25} \\end{equation}\\] This can be easily implemented in R by transforming the variables using the log() function: log_reg &lt;- lm(log(sales) ~ log(advertising), data = non_linear_reg) summary(log_reg) ## ## Call: ## lm(formula = log(sales) ~ log(advertising), data = non_linear_reg) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.666 -0.127 0.003 0.134 0.640 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.01493 0.05971 -0.25 0.8 ## log(advertising) 0.30077 0.00651 46.20 &lt;0.0000000000000002 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2 on 998 degrees of freedom ## Multiple R-squared: 0.681, Adjusted R-squared: 0.681 ## F-statistic: 2.13e+03 on 1 and 998 DF, p-value: &lt;0.0000000000000002 Note that this specification implies decreasing marginal returns (i.e., the returns of advertising are decreasing with the level of advertising), which appear to be more consistent with the data. The specification is also consistent with proportional changes in advertising being associated with proportional changes in sales (i.e., advertising does not become more effective with increasing levels). This has important implications on the interpretation of the coefficients. In our example, you would interpret the coefficient as follows: A 1% increase in advertising leads to a 0.3% increase in sales. Hence, the interpretation is in proportional terms and no longer in units. This means that the coefficients in a log-log model can be directly interpreted as elasticities, which also makes communication easier. We can generally also inspect the R2 statistic to see that the model fit has increased compared to the linear specification (i.e., R2 has increased to 0.681 from 0.509). However, please note that the variables are now measured on a different scale, which means that the model fit in theory is not directly comparable. Also, we could use the residuals plot to confirm that the revised specification is more appropriate: plot(log_reg,1) Figure 6.26: Residuals plot plot(log_reg,2) Figure 6.27: Q-Q plot Finally, we can plot the predicted values against the observed values to see that the results from the log-log model (red) provide a better prediction than the results from the linear model (blue). non_linear_reg$pred_lin_reg &lt;- predict(linear_reg) non_linear_reg$pred_log_reg &lt;- predict(log_reg) ggplot(data = non_linear_reg) + geom_point(aes(x = advertising, y = sales),shape=1) + geom_line(data = non_linear_reg,aes(x=advertising,y=pred_lin_reg),color=&quot;blue&quot;, size=1.05) + geom_line(data = non_linear_reg,aes(x=advertising,y=exp(pred_log_reg)),color=&quot;red&quot;, size=1.05) + theme_bw() Figure 6.28: Comparison if model fit Another way of modelling non-linearities is including a squared term if there are decreasing or increasing effects. In fact, we can model non-constant slopes as long as the form is a linear combination of exponentials (i.e. squared, cubed, …) of the explanatory variables. Usually we do not expect many inflection points so squared or third power terms suffice. Note that the degree of the polynomial has to be equal to the number of inflection points. When using squared terms we can model diminishing and eventually negative returns. Think about advertisement spending. If a brand is not well known, spending on ads will increase brand awareness and have a large effect on sales. In a regression model this translates to a steep slope for spending at the origin (i.e. for lower spending). However, as more and more people will already know the brand we expect that an additional Euro spent on advertisement will have less and less of an effect the more the company spends. We say that the returns are diminishing. Eventually, if they keep putting more and more ads out, people get annoyed and some will stop buying from the company. In that case the return might even get negative. To model such a situation we need a linear as well as a squared term in the regression. lm(...) can take squared (or any power) terms as input by adding I(X^2) as explanatory variable. In the example below we see a clear quadratic relationship with an inflection point at around 70. If we try to model this using the level of the covariates without the quadratic term we do not get a very good fit. set.seed(1234) X &lt;- as.integer(runif(1000, 0, 12000)) Y &lt;- 80000 + 140 * X - 0.01 * (X^2) + rnorm(1000, 0, 35000) modLinear &lt;- lm(Y/100000 ~ X) sales_quad &lt;- data.frame(sales = Y/100000, advertising = X*0.01, Prediction = fitted(modLinear)) ggplot(sales_quad) + geom_point(aes(x = advertising, y = sales, color = &quot;Data&quot;)) + geom_line(aes(x = advertising, y = Prediction, color = &quot;Prediction&quot;)) + theme_bw() + ggtitle(&quot;Linear Predictor&quot;) + theme(legend.title = element_blank()) The graph above clearly shows that advertising spending of between 0 and 50 increases sales. However, the marginal increase (i.e. the slope of the data curve) is decreasing. Around 70 there is an inflection point. After that point additional ad-spending actually decreases sales (e.g. people get annoyed). Notice that the prediction line is straight, that is, the marginal increase of sales due to additional spending on advertising is the same for any amount of spending. This shows the danger of basing business decisions on wrongly specified models. But even in the area in which the sign of the prediction is correct, we are quite far off. Lets take a look at the top 5 sales values and the corresponding predictions: top5 &lt;- which(sales_quad$sales %in% head(sort(sales_quad$sales, decreasing = TRUE), 5)) dplyr::arrange(sales_quad[top5, ], desc(sales_quad[top5, 1])) By including a quadratic term we can fit the data very well. This is still a linear model since the outcome variable is still explained by a linear combination of regressors even though one of the regressors is now just a non-linear function of the same variable (i.e. the squared value). quad_mod &lt;- lm(sales ~ advertising + I(advertising^2), data = sales_quad) summary(quad_mod) ## ## Call: ## lm(formula = sales ~ advertising + I(advertising^2), data = sales_quad) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.0216 -0.2201 0.0036 0.2236 0.9562 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.81627190 0.03170112 25.8 &lt;0.0000000000000002 *** ## advertising 0.13964258 0.00119945 116.4 &lt;0.0000000000000002 *** ## I(advertising^2) -0.00099972 0.00000955 -104.7 &lt;0.0000000000000002 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.33 on 997 degrees of freedom ## Multiple R-squared: 0.936, Adjusted R-squared: 0.936 ## F-statistic: 7.29e+03 on 2 and 997 DF, p-value: &lt;0.0000000000000002 confint(quad_mod) ## 2.5 % 97.5 % ## (Intercept) 0.754 0.87848 ## advertising 0.137 0.14200 ## I(advertising^2) -0.001 -0.00098 sales_quad$Prediction &lt;- predict(quad_mod) ggplot(data = sales_quad, aes(x = Prediction, y = sales)) + geom_point(shape=1) + geom_smooth(method = &quot;lm&quot;, fill = &quot;blue&quot;, alpha=0.1) + theme_bw() plot(quad_mod,1) plot(quad_mod,2) shapiro.test(resid(quad_mod)) ## ## Shapiro-Wilk normality test ## ## data: resid(quad_mod) ## W = 1, p-value = 0.2 sales_quad$pred_lin_reg &lt;- predict(modLinear) ggplot(data = sales_quad) + geom_point(aes(x = advertising, y = sales),shape=1) + geom_line(data = sales_quad,aes(x=advertising,y=pred_lin_reg),color=&quot;blue&quot;, size=1.05) + geom_line(data = sales_quad,aes(x=advertising,y=Prediction),color=&quot;red&quot;, size=1.05) + theme_bw() + xlab(&quot;Advertising (thsd. Euro)&quot;) + ylab(&quot;Sales (million units)&quot;) Now the prediction of the model is very close to the actual data and we could base our production decisions on that model. top5 &lt;- which(sales_quad$sales %in% head(sort(sales_quad$sales, decreasing = TRUE), 5)) dplyr::arrange(sales_quad[top5, ], desc(sales_quad[top5, 1])) When interpreting the coefficients of the predictor in this model we have to be careful. Since we included the squared term, the slope is now different at each level of production (this can be seen in the graph above). That is, we do not have a single coefficient to interpret as the slope anymore. This can easily be shown by calculating the derivative of the model with respect to production. \\[ \\text{Sales} = \\alpha + \\beta_1 \\text{ Advertising} + \\beta_2 \\text{ Advertising}^2 + \\varepsilon\\\\ {\\delta \\text{ Sales} \\over \\delta \\text{ Advertising}} = \\beta_1 + 2 \\beta_2 \\text{ Advertising} \\equiv \\text{Slope} \\] Intuitively, this means that the change of sales due to an additional Euro spent on advertising depends on the current level of advertising. \\(\\alpha\\), the intercept can still be interpreted as the expected value of sales given that we do not advertise at all (set advertising to 0 in the model). The sign of the squared term (\\(\\beta_2\\)) can be used to determine the curvature of the function. If the sign is positive, the function is convex (curvature is upwards), if it is negative it is concave curvature is downwards). We can interpret \\(\\beta_1\\) and \\(\\beta_2\\) separately in terms of their influence on the slope. By setting advertising to \\(0\\) we observe that \\(\\beta_1\\) is the slope at the origin. By taking the derivative of the slope with respect to advertising we see that the change of the slope due to additional spending on advertising is two times \\(\\beta_2\\). \\[ {\\delta Slope \\over \\delta Advertising} = 2\\beta_2 \\] At the maximum predicted value the slope is close to \\(0\\) (theoretically it is equal to \\(0\\) but this would require decimals and we can only sell whole pieces). Above we only calculated the prediction for the observed data, so let’s first predict the profit for all possible values between \\(1\\) and \\(200\\) to get the optimal production level according to our model. predictionAll&lt;- predict(quad_mod, newdata = data.frame(advertising = 1:200)) (optimalAdvertising &lt;- as.integer(which.max(predictionAll))) ## [1] 70 #Slope at optimum: coef(quad_mod)[[&quot;advertising&quot;]] + 2 * coef(quad_mod)[[&quot;I(advertising^2)&quot;]] * optimalAdvertising ## [1] -0.00032 For all other levels of advertising we insert the pieces produced into the formula to obtain the slope at that point. In the following example you can choose the level of advertising. 6.6 Summary of regression Correlation: it helps us to determine the degree to which the variation in one variable, X, is related to the variation in another variable, Y. Note: correlation coefficients give no indication of the direction of causality. Moreover, consideration of the third-variable problem is necessary when interpreting correlation as there might be other unobserved variables that influence the result. Correlation (Pearson’s) can be observed between 2 or more continuous variables (interval or ratio). If we apply Pearson’s equation on ordinal data, which we ranked beforehand, we will obtain Spearman’s correlation coefficient. A raw measure of the relationship between variables is the covariance. The standardized covariance (divided by the product of the two standard deviations) results in Pearson’s correlation coefficient, \\(r\\). The correlation coefficient can be between −1 and +1. +1 indicates a perfectly positive relationship; −1 indicates a perfectly negative relationship; 0 indicates no linear relationship at all. The correlation coefficient is a commonly used measure of the size of an effect: values of +/-0.1 are considered a small effect, +/-0.3 a medium effect and +/-0.5 a large effect. However, an interpretation of the size of correlation should be made based on the context of the research rather than on following these benchmarks. Regression analysis is a way of predicting an outcome (dependent) variable from one predictor (independent) variable (simple regression) or several predictor variables (multiple regression). In a simple linear regression, we predict a dependent variable this by fitting a statistical model to the data in the form of a straight line. A regression line is a line that goes as close as possible to as many of the data points as possible. R-squared indicates how much variance is explained by the model compared to how much variance there is to explain at all. It can be defined as the proportion of variance in the outcome variable that is shared by the predictor variable. F-statistic denotes the extent to which the model can explain variability relative to the extent to which the model cannot explain it. Beta coefficient in a single linear regression determines the slope of the regression line and the strength of the relationship between a predictor and the outcome. The coefficient is significant if Pr(&gt;|t|) &lt; .05. It shows the extent to which the outcome variable changes when comes to one-unit change in the predictor variable. Confidence interval of beta coefficient tell us, if we’d collected 100 samples, and calculated the confidence intervals for a beta coefficient, that 95% of these confidence intervals (out of 100) would contain the true value of beta coefficient. Intercept shows the value that dependent variable holds when beta coefficients are 0. Assessing the regression model: Check for the outliers and influential cases as they can cause a bias, because they influence the values of the estimated regression coefficients. Check for the linearity, meaning that the relationship between the dependent and the independent variable can be reasonably approximated in linear terms. Check that the error terms have a constant variance (homoscedasticity). Check that the error term is normally distributed. Check for the “third-variables” that are left out the model, but influence the outcome, and therefore introduce a bias in other variables’ coefficients. In a regression model we can use categrical predictors with 2 or more categories. These predictors are included in the model as “dummy” variables. Dummy variables are factor variables that can only take two values. \\[\\begin{equation} x = \\begin{cases} 1 &amp; \\quad \\text{if } i \\text{th artist is international}\\\\ 0 &amp; \\quad \\text{if } i \\text{th artist is local} \\end{cases} \\end{equation}\\] In case of multiple categorical predictors, one dummy variable cannot represent all possible values, since there are three genres (i.e., 1 = Rock, 2 = Pop, 3 = Electronic). Thus, we need to create additional dummy variables. \\[\\begin{equation} x_5 = \\begin{cases} 1 &amp; \\quad \\text{if } i \\text{th product is from Pop genre}\\\\ 0 &amp; \\quad \\text{if } i \\text{th product is from Rock genre} \\end{cases} \\end{equation}\\] \\[\\begin{equation} x_6 = \\begin{cases} 1 &amp; \\quad \\text{if } i \\text{th product is from Electronic genre}\\\\ 0 &amp; \\quad \\text{if } i \\text{th product is from Rock genre} \\end{cases} \\end{equation}\\] The standard linear regression model assumes that the relationships between the response and predictor variable is additive and linear. The additive assumption states that the effect of an independent variable on the dependent variable is independent of the values of the other independent variables included in the model. The additive assumption states that the effect of an independent variable on the dependent variable is independent of the values of the other independent variables included in the model. Regarding the additive assumption, it might be argued in reality that the effect of some variables are not fully independent of the values of other variables. In our example, one could argue that the effect of advertising depends on the type of artist. We can incorporate this interaction effect by including an interaction term in the regression equation. Interaction effect can be possible for different combinations of variable types. There are cases when a linear specification might not be the best model for this data set. In this chapter we saw that a multiplicative model might be a better representation of the data we had. Multiplicative model means taking logarithms of both sides of the equation. A linear specification with log-transformed variables have important implications on interpreteation of coefficinets: A 1% increase in independent varibale leads to a XY% increase in dependent variable. If our data shows initially diminishing and eventually negative returns (inverted U-shape), we can use squared terms to obtain a model that fits good. This results still in a linear model since the outcome variable is still explained by a linear combination of regressors even though one of the regressors is now just a non-linear function of the same variable (i.e. the squared value). Via linear regression models we can predict continuous outcomes such as sales or price. However, if we would like to predict a binary outcome, i.e. when the variable we want to model can only take two values (such as 0 or 1; yes or no). For such cases when we estimate how our predictor variables change the probability of a value being 0 or 1 we use logsitic regression. While linear probability model produces probabilities that are above 1 and below 0 the logistic model stays between 0 and 1. Interpretation of coefficients in the logistic regression is a bit specific. It gives the change in the log odds of the dependent variable due to a unit change in the independent variable. This makes the exact interpretation of the coefficients difficult, but we can still interpret the signs and the p-values which will tell us if a variable has a significant positive or negative impact on the probability of the dependent variable being 1. Logistic regression model can consist of 1 or more independent variables. In order to determine which variables should be included, we take a look at AIC (Akaike Information Criterium) and seek for the model with the lowest AIC value. Alternatively, pseudo \\(R^2\\) can be used as an indicator for the model fit too. 6.7 Logistic regression The following video summarizes how to visualize log-transformed regressions in R 6.7.1 Motivation and intuition In the last section we saw how to predict continuous outcomes (sales, height, etc.) via linear regression models. Another interesting case is that of binary outcomes, i.e. when the variable we want to model can only take two values (yes or no, group 1 or group 2, dead or alive, etc.). To this end we would like to estimate how our predictor variables change the probability of a value being 0 or 1. In this case we can technically still use a linear model (e.g. OLS). However, its predictions will most likely not be particularly useful. A more useful method is the logistic regression. In particular we are going to have a look at the logit model. In the following dataset we are trying to predict whether a song will be a top-10 hit on a popular music streaming platform. In a first step we are going to use only the danceability index as a predictor. Later we are going to add more independent variables. library(ggplot2) library(gridExtra) chart_data &lt;- read.delim2(&quot;https://raw.githubusercontent.com/IMSMWU/MRDA2018/master/data/chart_data_logistic.dat&quot;,header=T, sep = &quot;\\t&quot;,stringsAsFactors = F, dec = &quot;.&quot;) #Create a new dummy variable &quot;top10&quot;, which is 1 if a song made it to the top10 and 0 else: chart_data$top10 &lt;- ifelse(chart_data$rank&lt;11,1,0) # Inspect data head(chart_data) str(chart_data) ## &#39;data.frame&#39;: 422 obs. of 27 variables: ## $ artistName : chr &quot;dj mustard&quot; &quot;bing crosby&quot; &quot;post malone&quot; &quot;chris brown&quot; ... ## $ trackID : chr &quot;01gNiOqg8u7vT90uVgOVmz&quot; &quot;01h424WG38dgY34vkI3Yd0&quot; &quot;02opp1cycqiFNDpLd2o1J3&quot; &quot;02yRHV9Cgk8CUS2fx9lKVC&quot; ... ## $ trackName : chr &quot;Whole Lotta Lovin&#39;&quot; &quot;White Christmas&quot; &quot;Big Lie&quot; &quot;Anyway&quot; ... ## $ rank : int 120 70 129 130 182 163 12 86 67 77 ... ## $ streams : int 917710 1865526 1480436 894216 642784 809256 3490456 1737890 1914768 1056689 ... ## $ frequency : int 3 9 1 1 1 2 2 12 17 11 ... ## $ danceability : num 0.438 0.225 0.325 0.469 0.286 0.447 0.337 0.595 0.472 0.32 ... ## $ energy : num 0.399 0.248 0.689 0.664 0.907 0.795 0.615 0.662 0.746 0.752 ... ## $ key : int 4 9 6 7 8 8 9 11 6 6 ... ## $ loudness : num -8.75 -15.87 -4.95 -7.16 -4.74 ... ## $ speechiness : num 0.0623 0.0337 0.243 0.121 0.113 0.0443 0.0937 0.0362 0.119 0.056 ... ## $ acousticness : num 0.154 0.912 0.197 0.0566 0.0144 0.211 0.0426 0.0178 0.072 0.289 ... ## $ instrumentalness: num 0.00000845 0.000143 0 0.00000158 0 0.00169 0.0000167 0 0 0.000101 ... ## $ liveness : num 0.0646 0.404 0.0722 0.482 0.268 0.0725 0.193 0.0804 0.116 0.102 ... ## $ valence : num 0.382 0.185 0.225 0.267 0.271 0.504 0.0729 0.415 0.442 0.398 ... ## $ tempo : num 160.2 96 77.9 124.7 75.6 ... ## $ duration_ms : int 299160 183613 207680 211413 266640 397093 199973 218447 196040 263893 ... ## $ time_signature : int 5 4 4 4 4 4 4 4 4 4 ... ## $ isrc : chr &quot;QMJMT1500808&quot; &quot;USMC14750470&quot; &quot;USUM71614468&quot; &quot;USRC11502943&quot; ... ## $ spotifyArtistID : chr &quot;0YinUQ50QDB7ZxSCLyQ40k&quot; &quot;6ZjFtWeHP9XN7FeKSUe80S&quot; &quot;246dkjvS1zLTtiykXe5h60&quot; &quot;7bXgB6jMjp9ATFy66eO08Z&quot; ... ## $ releaseDate : chr &quot;08.01.2016&quot; &quot;27.08.2007&quot; &quot;09.12.2016&quot; &quot;11.12.2015&quot; ... ## $ daysSinceRelease: int 450 1000 114 478 527 429 506 132 291 556 ... ## $ spotifyFollowers: int 139718 123135 629600 4077185 2221348 9687258 8713999 39723 4422933 3462797 ... ## $ mbid : chr &quot;0612bcce-e351-40be-b3d7-2bb5e1c23479&quot; &quot;2437980f-513a-44fc-80f1-b90d9d7fcf8f&quot; &quot;b1e26560-60e5-4236-bbdb-9aa5a8d5ee19&quot; &quot;c234fa42-e6a6-443e-937e-2f4b073538a3&quot; ... ## $ artistCountry : chr &quot;US&quot; &quot;US&quot; &quot;0&quot; &quot;US&quot; ... ## $ indicator : int 1 1 1 1 1 1 1 1 1 1 ... ## $ top10 : num 0 0 0 0 0 0 0 0 0 0 ... Below are two attempts to model the data. The left assumes a linear probability model (calculated with the same methods that we used in the last chapter), while the right model is a logistic regression model. As you can see, the linear probability model produces probabilities that are above 1 and below 0, which are not valid probabilities, while the logistic model stays between 0 and 1. Notice that songs with a higher danceability index (on the right of the x-axis) seem to cluster more at \\(1\\) and those with a lower more at \\(0\\) so we expect a positive influence of danceability on the probability of a song to become a top-10 hit. Figure 6.29: The same binary data explained by two models; A linear probability model (on the left) and a logistic regression model (on the right) A key insight at this point is that the connection between \\(\\mathbf{X}\\) and \\(Y\\) is non-linear in the logistic regression model. As we can see in the plot, the probability of success is most strongly affected by danceability around values of \\(0.5\\), while higher and lower values have a smaller marginal effect. This obviously also has consequences for the interpretation of the coefficients later on. 6.7.2 Technical details of the model As the name suggests, the logistic function is an important component of the logistic regression model. It has the following form: \\[ f(\\mathbf{X}) = \\frac{1}{1 + e^{-\\mathbf{X}}} \\] This function transforms all real numbers into the range between 0 and 1. We need this to model probabilities, as probabilities can only be between 0 and 1. The logistic function on its own is not very useful yet, as we want to be able to determine how predictors influence the probability of a value to be equal to 1. To this end we replace the \\(\\mathbf{X}\\) in the function above with our familiar linear specification, i.e. \\[ \\mathbf{X} = \\beta_0 + \\beta_1 * x_{1,i} + \\beta_2 * x_{2,i} + ... +\\beta_m * x_{m,i}\\\\ f(\\mathbf{X}) = P(y_i = 1) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 * x_{1,i} + \\beta_2 * x_{2,i} + ... +\\beta_m * x_{m,i})}} \\] In our case we only have \\(\\beta_0\\) and \\(\\beta_1\\), the coefficient associated with danceability. In general we now have a mathematical relationship between our predictor variables \\((x_1, ..., x_m)\\) and the probability of \\(y_i\\) being equal to one. The last step is to estimate the parameters of this model \\((\\beta_0, \\beta_1, ..., \\beta_m)\\) to determine the magnitude of the effects. 6.7.3 Estimation in R We are now going to show how to perform logistic regression in R. Instead of lm() we now use glm(Y~X, family=binomial(link = 'logit')) to use the logit model. We can still use the summary() command to inspect the output of the model. #Run the glm logit_model &lt;- glm(top10 ~ danceability,family=binomial(link=&#39;logit&#39;),data=chart_data) #Inspect model summary summary(logit_model ) ## ## Call: ## glm(formula = top10 ~ danceability, family = binomial(link = &quot;logit&quot;), ## data = chart_data) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.8852 -0.5011 -0.2385 0.2932 2.8196 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -10.0414 0.8963 -11.20 &lt;0.0000000000000002 *** ## danceability 17.0939 1.6016 10.67 &lt;0.0000000000000002 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 539.05 on 421 degrees of freedom ## Residual deviance: 258.49 on 420 degrees of freedom ## AIC: 262.49 ## ## Number of Fisher Scoring iterations: 6 Noticeably this output does not include an \\(R^2\\) value to asses model fit. Multiple “Pseudo \\(R^2\\)s”, similar to the one used in OLS, have been developed. There are packages that return the \\(R^2\\) given a logit model (see rcompanion or pscl). The calculation by hand is also fairly simple. We define the function logisticPseudoR2s() that takes a logit model as an input and returns three popular pseudo \\(R^2\\) values. logisticPseudoR2s &lt;- function(LogModel) { dev &lt;- LogModel$deviance nullDev &lt;- LogModel$null.deviance modelN &lt;- length(LogModel$fitted.values) R.l &lt;- 1 - dev / nullDev R.cs &lt;- 1- exp ( -(nullDev - dev) / modelN) R.n &lt;- R.cs / ( 1 - ( exp (-(nullDev / modelN)))) cat(&quot;Pseudo R^2 for logistic regression\\n&quot;) cat(&quot;Hosmer and Lemeshow R^2 &quot;, round(R.l, 3), &quot;\\n&quot;) cat(&quot;Cox and Snell R^2 &quot;, round(R.cs, 3), &quot;\\n&quot;) cat(&quot;Nagelkerke R^2 &quot;, round(R.n, 3), &quot;\\n&quot;) } #Inspect Pseudo R2s logisticPseudoR2s(logit_model ) ## Pseudo R^2 for logistic regression ## Hosmer and Lemeshow R^2 0.52 ## Cox and Snell R^2 0.486 ## Nagelkerke R^2 0.673 The coefficients of the model give the change in the log odds of the dependent variable due to a unit change in the regressor. This makes the exact interpretation of the coefficients difficult, but we can still interpret the signs and the p-values which will tell us if a variable has a significant positive or negative impact on the probability of the dependent variable being \\(1\\). In order to get the odds ratios we can simply take the exponent of the coefficients. exp(coef(logit_model )) ## (Intercept) danceability ## 0.00004355897 26532731.71142345294 Notice that the coefficient is extremely large. That is (partly) due to the fact that the danceability variable is constrained to values between \\(0\\) and \\(1\\) and the coefficients are for a unit change. We can make the “unit-change” interpretation more meaningful by multiplying the danceability index by \\(100\\). This linear transformation does not affect the model fit or the p-values. #Re-scale independet variable chart_data$danceability_100 &lt;- chart_data$danceability*100 #Run the regression model logit_model &lt;- glm(top10 ~ danceability_100,family=binomial(link=&#39;logit&#39;),data=chart_data) #Inspect model summary summary(logit_model ) ## ## Call: ## glm(formula = top10 ~ danceability_100, family = binomial(link = &quot;logit&quot;), ## data = chart_data) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.8852 -0.5011 -0.2385 0.2932 2.8196 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -10.04139 0.89629 -11.20 &lt;0.0000000000000002 *** ## danceability_100 0.17094 0.01602 10.67 &lt;0.0000000000000002 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 539.05 on 421 degrees of freedom ## Residual deviance: 258.49 on 420 degrees of freedom ## AIC: 262.49 ## ## Number of Fisher Scoring iterations: 6 #Inspect Pseudo R2s logisticPseudoR2s(logit_model ) ## Pseudo R^2 for logistic regression ## Hosmer and Lemeshow R^2 0.52 ## Cox and Snell R^2 0.486 ## Nagelkerke R^2 0.673 #Convert coefficients to odds ratios exp(coef(logit_model )) ## (Intercept) danceability_100 ## 0.00004355897 1.18641825295 We observe that danceability positively affects the likelihood of becoming at top-10 hit. To get the confidence intervals for the coefficients we can use the same function as with OLS confint(logit_model) ## 2.5 % 97.5 % ## (Intercept) -11.9208213 -8.3954496 ## danceability_100 0.1415602 0.2045529 In order to get a rough idea about the magnitude of the effects we can calculate the partial effects at the mean of the data (that is the effect for the average observation). Alternatively, we can calculate the mean of the effects (that is the average of the individual effects). Both can be done with the logitmfx(...) function from the mfx package. If we set logitmfx(logit_model, data = my_data, atmean = FALSE) we calculate the latter. Setting atmean = TRUE will calculate the former. However, in general we are most interested in the sign and significance of the coefficient. library(mfx) # Average partial effect logitmfx(logit_model, data = chart_data, atmean = FALSE) ## Call: ## logitmfx(formula = logit_model, data = chart_data, atmean = FALSE) ## ## Marginal Effects: ## dF/dx Std. Err. z P&gt;|z| ## danceability_100 0.0157310 0.0029761 5.2857 0.0000001252 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 This now gives the average partial effects in percentage points. An additional point on the danceability scale (from \\(1\\) to \\(100\\)), on average, makes it \\(1.57%\\) more likely for a song to become at top-10 hit. To get the effect of an additional point at a specific value, we can calculate the odds ratio by predicting the probability at a value and at the value \\(+1\\). For example if we are interested in how much more likely a song with 51 compared to 50 danceability is to become a hit we can simply calculate the following #Probability of a top 10 hit with a danceability of 50 prob_50 &lt;- exp(-(-summary(logit_model)$coefficients[1,1]-summary(logit_model)$coefficients[2,1]*50 )) prob_50 ## [1] 0.224372 #Probability of a top 10 hit with a danceability of 51 prob_51 &lt;- exp(-(-summary(logit_model)$coefficients[1,1]-summary(logit_model)$coefficients[2,1]*51 )) prob_51 ## [1] 0.266199 #Odds ratio prob_51/prob_50 ## [1] 1.186418 So the odds are 20% higher at 51 than at 50. 6.7.3.1 Logistic model with multiple predictors Of course we can also use multiple predictors in logistic regression as shown in the formula above. We might want to add spotify followers (in million) and weeks since the release of the song. chart_data$spotify_followers_m &lt;- chart_data$spotifyFollowers/1000000 chart_data$weeks_since_release &lt;- chart_data$daysSinceRelease/7 Again, the familiar formula interface can be used with the glm() function. All the model summaries shown above still work with multiple predictors. multiple_logit_model &lt;- glm(top10 ~ danceability_100 + spotify_followers_m + weeks_since_release,family=binomial(link=&#39;logit&#39;),data=chart_data) summary(multiple_logit_model) ## ## Call: ## glm(formula = top10 ~ danceability_100 + spotify_followers_m + ## weeks_since_release, family = binomial(link = &quot;logit&quot;), data = chart_data) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.8861 -0.4390 -0.2083 0.2311 2.8015 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -9.603762 0.990481 -9.696 &lt; 0.0000000000000002 *** ## danceability_100 0.166236 0.016358 10.162 &lt; 0.0000000000000002 *** ## spotify_followers_m 0.197717 0.060030 3.294 0.000989 *** ## weeks_since_release -0.012976 0.004956 -2.619 0.008832 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 534.91 on 416 degrees of freedom ## Residual deviance: 239.15 on 413 degrees of freedom ## (5 observations deleted due to missingness) ## AIC: 247.15 ## ## Number of Fisher Scoring iterations: 6 logisticPseudoR2s(multiple_logit_model) ## Pseudo R^2 for logistic regression ## Hosmer and Lemeshow R^2 0.553 ## Cox and Snell R^2 0.508 ## Nagelkerke R^2 0.703 exp(coef(multiple_logit_model)) ## (Intercept) danceability_100 spotify_followers_m weeks_since_release ## 0.0000674744 1.1808513243 1.2186174345 0.9871076460 confint(multiple_logit_model) ## 2.5 % 97.5 % ## (Intercept) -11.67983072 -7.782122558 ## danceability_100 0.13625795 0.200625438 ## spotify_followers_m 0.08079476 0.317115293 ## weeks_since_release -0.02307859 -0.003566462 6.7.3.2 Model selection The question remains, whether a variable should be added to the model. We will present two methods for model selection for logistic regression. The first is based on the Akaike Information Criterium (AIC). It is reported with the summary output for logit models. The value of the AIC is relative, meaning that it has no interpretation by itself. However, it can be used to compare and select models. The model with the lowest AIC value is the one that should be chosen. Note that the AIC does not indicate how well the model fits the data, but is merely used to compare models. For example, consider the following model, where we exclude the followers covariate. Seeing as it was able to contribute significantly to the explanatory power of the model, the AIC increases, indicating that the model including followers is better suited to explain the data. We always want the lowest possible AIC. multiple_logit_model2 &lt;- glm(top10 ~ danceability_100 + weeks_since_release,family=binomial(link=&#39;logit&#39;),data=chart_data) summary(multiple_logit_model2) ## ## Call: ## glm(formula = top10 ~ danceability_100 + weeks_since_release, ## family = binomial(link = &quot;logit&quot;), data = chart_data) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.9578 -0.4721 -0.2189 0.2562 2.8759 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -8.980225 0.930654 -9.649 &lt;0.0000000000000002 *** ## danceability_100 0.166498 0.016107 10.337 &lt;0.0000000000000002 *** ## weeks_since_release -0.012805 0.004836 -2.648 0.0081 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 534.91 on 416 degrees of freedom ## Residual deviance: 250.12 on 414 degrees of freedom ## (5 observations deleted due to missingness) ## AIC: 256.12 ## ## Number of Fisher Scoring iterations: 6 As a second measure for variable selection, you can use the pseudo \\(R^2\\)s as shown above. The fit is distinctly worse according to all three values presented here, when excluding the spotify followers. logisticPseudoR2s(multiple_logit_model2) ## Pseudo R^2 for logistic regression ## Hosmer and Lemeshow R^2 0.532 ## Cox and Snell R^2 0.495 ## Nagelkerke R^2 0.685 6.7.3.3 Predictions We can predict the probability given an observation using the predict(my_logit, newdata = ..., type = \"response\") function. Replace ... with the observed values for which you would like to predict the outcome variable. # Prediction for one observation predict(multiple_logit_model, newdata = data.frame(danceability_100=50, spotify_followers_m=10, weeks_since_release=1), type = &quot;response&quot;) ## 1 ## 0.6619986 The prediction indicates that a song with danceability of \\(50\\) from an artist with \\(10M\\) spotify followers has a \\(66%\\) chance of being in the top-10, 1 week after its release. 6.7.3.4 Perfect Prediction Logit Perfect prediction occurs whenever a linear function of \\(X\\) can perfectly separate the \\(1\\)s from the \\(0\\)s in the dependent variable. This is problematic when estimating a logit model as it will result in biased estimators (also check to p-values in the example!). R will return the following message if this occurs: glm.fit: fitted probabilities numerically 0 or 1 occurred Given this error, one should not use the output of the glm(...) function for the analysis. There are various ways to deal with this problem, one of which is to use Firth’s bias-reduced penalized-likelihood logistic regression with the logistf(Y~X) function in the logistf package. 6.7.3.4.1 Example In this example data \\(Y = 0\\) if \\(x_1 &lt;0\\) and \\(Y=1\\) if \\(x_1&gt;0\\) and we thus have perfect prediction. As we can see the output of the regular logit model is not interpretable. The standard errors are huge compared to the coefficients and thus the p-values are \\(1\\) despite \\(x_1\\) being a predictor of \\(Y\\). Thus, we turn to the penalized-likelihood version. This model correctly indicates that \\(x_1\\) is in fact a predictor for \\(Y\\) as the coefficient is significant. Y &lt;- c(0,0,0,0,1,1,1,1) X &lt;- cbind(c(-1,-2,-3,-3,5,6,10,11),c(3,2,-1,-1,2,4,1,0)) # Perfect prediction with regular logit summary(glm(Y~X, family=binomial(link=&quot;logit&quot;))) ## ## Call: ## glm(formula = Y ~ X, family = binomial(link = &quot;logit&quot;)) ## ## Deviance Residuals: ## 1 2 3 4 5 ## -0.0000102197 -0.0000012300 -0.0000033675 -0.0000033675 0.0000105893 ## 6 7 8 ## 0.0000060786 0.0000000211 0.0000000211 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -6.943 113859.814 0 1 ## X1 7.359 15925.251 0 1 ## X2 -3.125 43853.489 0 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 11.09035488895912 on 7 degrees of freedom ## Residual deviance: 0.00000000027772 on 5 degrees of freedom ## AIC: 6 ## ## Number of Fisher Scoring iterations: 24 library(logistf) # Perfect prediction with penalized-likelihood logit summary(logistf(Y~X)) ## logistf(formula = Y ~ X) ## ## Model fitted by Penalized ML ## Confidence intervals and p-values by Profile Likelihood Profile Likelihood Profile Likelihood ## ## coef se(coef) lower 0.95 upper 0.95 Chisq p ## (Intercept) -0.98871431 1.4283595 -10.2169313 1.884501 0.59231445 0.44152553 ## X1 0.33195157 0.2142516 0.0417035 1.463409 5.31583569 0.02113246 ## X2 0.08250307 0.6085055 -2.1788866 3.379327 0.01980379 0.88808646 ## ## Likelihood ratio test=5.800986 on 2 df, p=0.05499609, n=8 ## Wald test = 2.706445 on 2 df, p = 0.2584062 ## ## Covariance-Matrix: ## [,1] [,2] [,3] ## [1,] 2.04021092 -0.04555678 -0.56761805 ## [2,] -0.04555678 0.04590374 -0.03356574 ## [3,] -0.56761805 -0.03356574 0.37027898 "],
["exploratory-factor-analysis.html", "7 Exploratory factor analysis 7.1 Introduction 7.2 Steps in factor analysis 7.3 Reliability analysis 7.4 Summary of factor analysis", " 7 Exploratory factor analysis This chapter is primarily based on Field, A., Miles J., &amp; Field, Z. (2012): Discovering Statistics Using R. Sage Publications, chapter 17 Malhotra, N. K.(2010). Marketing Research: An Applied Orientation (6th. ed.). Prentice Hall. chapter 19 You can download the corresponding R-Code here 7.1 Introduction In this chapter, we will focus on exploratory factor analysis. Generally, factor analysis is a class of procedures used for data reduction or summarization. It is an interdependence technique, meaning that there is no distinction between dependent and independent variables and all variables are considered simultaneously. In exploratory factor analysis, specific hypotheses about how many factors will emerge, and what items these factors will comprise are not requires (as opposed to confirmatory factor analysis). Principal Components Analysis (PCA) is one of the most frequently used techniques. The goals are … To identify underlying dimensions, or factors, that explain the correlations among a set of variables To identify a new, smaller set of uncorrelated variables to replace the original set of correlated variables in subsequent multivariate analysis (e.g., regression analysis, t-test, etc.) To see what this means, let’s use a simple example. Say, you wanted to explain the motives underlying the purchasing of toothpaste. You come up with six items that represent different motives of purchasing toothpaste: Item 1: It is important to buy toothpaste that prevents cavities. Item 2: I like a toothpaste that gives shiny teeth. Item 3: A toothpaste should strengthen your gums. Item 4: I prefer a toothpaste that freshens breath. Item 5: Prevention of tooth decay should not be an important benefit offered by a toothpaste. Item 6: The most important consideration in buying a toothpaste is attractive teeth. Let’s assume you collect data from 30 respondents and you use 7-point itemized rating scales to measure the extent of agreement to each of these statements. This is the data that you have collected: factor_analysis &lt;- read.table(&quot;https://raw.githubusercontent.com/IMSMWU/Teaching/master/MRDA2017/toothpaste.dat&quot;, sep = &quot;\\t&quot;, header = TRUE) #read in data str(factor_analysis) #inspect data ## &#39;data.frame&#39;: 30 obs. of 6 variables: ## $ prevent_cavities: int 7 1 6 4 1 6 5 6 3 2 ... ## $ shiny_teeth : int 3 3 2 5 2 3 3 4 4 6 ... ## $ strengthen_gum : int 6 2 7 4 2 6 6 7 2 2 ... ## $ fresh_breath : int 4 4 4 6 3 4 3 4 3 6 ... ## $ prevent_decay : int 6 3 7 6 2 6 4 7 2 1 ... ## $ attract_teeth : int 4 4 3 5 2 4 3 4 3 6 ... head(factor_analysis) #inspect data A construct is a specific type of concept that exists at a higher level of abstraction than everyday concepts. In this example, the perceived benefits of toothpaste represent the construct that we would like to measure. The construct is unobservable (‘latent’) but it can be inferred from other measurable variables (items) that together comprise a scale (latent construct). A multi-item scale consists of multiple items, where an item is a single question or statement to be evaluated. In the above example, we use six items to measure the perceived benefits of toothpaste. If several items correlate highly, they might measure aspects of a common underlying dimension (a.k.a. factors). That is, specific patterns in the correlation matrix signal the existence of one or more factors underlying the data. Let’s inspect the correlation matrix using the rcorr() function from the Hmisc package. library(&quot;Hmisc&quot;) rcorr(as.matrix(factor_analysis)) ## prevent_cavities shiny_teeth strengthen_gum fresh_breath ## prevent_cavities 1.00 -0.05 0.87 -0.09 ## shiny_teeth -0.05 1.00 -0.16 0.57 ## strengthen_gum 0.87 -0.16 1.00 -0.25 ## fresh_breath -0.09 0.57 -0.25 1.00 ## prevent_decay 0.86 -0.02 0.78 0.01 ## attract_teeth 0.00 0.64 -0.02 0.64 ## prevent_decay attract_teeth ## prevent_cavities 0.86 0.00 ## shiny_teeth -0.02 0.64 ## strengthen_gum 0.78 -0.02 ## fresh_breath 0.01 0.64 ## prevent_decay 1.00 0.14 ## attract_teeth 0.14 1.00 ## ## n= 30 ## ## ## P ## prevent_cavities shiny_teeth strengthen_gum fresh_breath ## prevent_cavities 0.7800 0.0000 0.6508 ## shiny_teeth 0.7800 0.4134 0.0010 ## strengthen_gum 0.0000 0.4134 0.1868 ## fresh_breath 0.6508 0.0010 0.1868 ## prevent_decay 0.0000 0.9175 0.0000 0.9725 ## attract_teeth 0.9826 0.0001 0.9245 0.0001 ## prevent_decay attract_teeth ## prevent_cavities 0.0000 0.9826 ## shiny_teeth 0.9175 0.0001 ## strengthen_gum 0.0000 0.9245 ## fresh_breath 0.9725 0.0001 ## prevent_decay 0.4723 ## attract_teeth 0.4723 You can see that some of the items correlate highly, while others don’t. Specifically, there appear to be two groups of items that correlate highly and that might represent underlying dimensions of the construct: Factor 1: Items 1, 3, 5 Factor 2: Items 2, 4, 6 Going back to the specific wording of the items you can see that the first group of items (i.e., items 1,3,5) refer to the health benefits, while the second item group (i.e., items 2,4,6) refer to the social benefits. Imagine now, for example, you would like to include the above variables as explanatory variables in a regression model. Due to the high degree of correlation among the items, you are likely to run into problems of multicollinearity. Instead of omitting some of the items, you might try to combine highly correlated items into one variable. Another application could be when you are developing a new measurement scale for a construct and you wish to explore the underlying dimensions of this construct. In these applications, you need to make sure that the questions that you are asking actually relate to the construct that you intend to measure. The goal of factor analysis is to explain the maximum amount of total variance in a correlation matrix by transforming the original variables into linear components. This means that the correlation matrix is broken down into a smaller set of dimensions. The generalized formal representation of the linear relationship between a latent factor Y and the set of variables can be written as: \\[\\begin{equation} Y_i=b_1X_{1i} + b_2X_{2i} + b_nX_{ni}+\\epsilon_i \\tag{7.1} \\end{equation}\\] where Xn represents the data that we have collected for the different variables. To make it more explicit, the equation could also be written as \\[\\begin{equation} Factor_i=b_1Variable_{1i} + b_2Variable_{2i} + b_nVariable_{ni}+\\epsilon_i \\tag{7.2} \\end{equation}\\] where the dependent variable “Factor” refers to the factor score of person i on the underlying dimensions. In our case, the initial inspection suggested two underlying factors (i.e., health benefits and social benefits), so that we can construct two equations that describe both factors in terms of the variables that we have measured: \\[\\begin{equation} Health_i=b_1X_{1i} + b_2X_{2i} + b_3X_{3i}+ b_4X_{4i}+ b_5X_{5i}+ b_6X_{6i}+\\epsilon_i \\tag{7.3} \\end{equation}\\] \\[\\begin{equation} Social_i=b_1X_{1i} + b_2X_{2i} + b_3X_{3i}+ b_4X_{4i}+ b_5X_{5i}+ b_6X_{6i}+\\epsilon_i \\tag{7.4} \\end{equation}\\] where the b’s in each equation represent the factor loadings. You can see that both equations include the same set of predictors. However, their values in each equation will be different, depending on the importance of each variable to the respective factor. Once the factor loadings have been computed (we will see how this is done below), we can summarize them in a component matrix, which is usually denoted as A: \\[\\mathbf{A} = \\left[\\begin{array} {rrr} 0.93 &amp; 0.25 \\\\ -0.30 &amp; 0.80 \\\\ 0.94 &amp; 0.13 \\\\ -0.34 &amp; 0.79 \\\\ 0.87 &amp; 0.35 \\\\ -0.18 &amp; 0.87 \\end{array}\\right]\\] The linear relation between the factors and the factor loadings can also be shown in a graph, where each axis represents a factor and the variables are placed on the coordinates according to the strength of the relationship between the variable and each factor. The greater the loading of variables on a factor, the more that factor explains relationships between those variables. You can also think of the factor loadings as the correlations between a factor and a variable. Figure 7.1: Factor loadings The factor loadings may then be used to compute the two new variables (i.e., factor scores) representing the two underlying dimensions. Using a rather simplistic approach, the factor scores for person i could be computed by \\[\\begin{equation} \\begin{split} Health_i=&amp; 0.93*preventCavities_{i} -0.3*shinyTeeth_{i} + 0.94*strengthenGum_{i}\\\\ &amp;-0.34*freshBreath_{i} + 0.87*preventDecay_{i} - 0.18*attractTeeth_{i} \\end{split} \\tag{7.5} \\end{equation}\\] \\[\\begin{equation} \\begin{split} Social_i=&amp; 0.25*preventCavities_{i} + 0.80*shinyTeeth_{i} + 0.13*strengthenGum_{i}\\\\ &amp;+ 0.79*freshBreath_{i}+ 0.35*preventDecay_{i}+ 0.87*attractTeeth_{i} \\end{split} \\tag{7.6} \\end{equation}\\] where the variable names are replaced by the values that were observed for respondent i to compute the factor scores for respondent i. This means that we have reduced the number of variables from six to two. Note that this is a rather simple approach that is intended to explain the underlying logic. However, that the resulting factor scores will depend on the measurement scales of the variables. If different measurement scales would be used, the resulting factor scores for different factors could not be compared. Thus, R will compute the factor scores using more sophisticated methods as we will see below. 7.2 Steps in factor analysis readRDS(&quot;music_data.rds&quot;) Now that we have a broad understanding of how factor analysis works, let’s use another example to go through the process of deriving factors step by step. In this section, we will use Spotify data. We have collected data for 48.919 songs and stored the results in the data set “music_data”. 7.2.1 Are the assumptions satisfied? Since PCA is based on the correlation between variables, the first step is to inspect the correlation matrix, which can be created using the cor() function. spotify_matrix &lt;- cor(music_data) round(spotify_matrix,3) ## danceability energy key loudness mode acousticness liveness ## danceability 1.000 0.198 0.010 0.187 -0.123 -0.247 -0.085 ## energy 0.198 1.000 0.022 0.737 -0.110 -0.602 0.166 ## key 0.010 0.022 1.000 0.004 -0.161 -0.006 -0.010 ## loudness 0.187 0.737 0.004 1.000 -0.065 -0.460 0.088 ## mode -0.123 -0.110 -0.161 -0.065 1.000 0.086 0.019 ## acousticness -0.247 -0.602 -0.006 -0.460 0.086 1.000 -0.070 ## liveness -0.085 0.166 -0.010 0.088 0.019 -0.070 1.000 ## valence 0.369 0.402 0.031 0.271 -0.061 -0.168 0.060 ## tempo -0.084 0.094 -0.004 0.077 0.011 -0.083 0.018 ## valence tempo ## danceability 0.369 -0.084 ## energy 0.402 0.094 ## key 0.031 -0.004 ## loudness 0.271 0.077 ## mode -0.061 0.011 ## acousticness -0.168 -0.083 ## liveness 0.060 0.018 ## valence 1.000 0.037 ## tempo 0.037 1.000 If the variables measure the same construct, we would expect to see a certain degree of correlation between the variables. Even if the variables turn out to measure different dimensions of the same underlying construct, we would still expect to see some degree of correlation. So the first problem that could occur is that the correlations are not high enough. A first approach would be to scan the correlation matrix for correlations lower than about 0.3 and identify variables that have many correlations below this threshold. If your data set contains many variables, this task can be quite tedious. To make the task a little easier, you could proceed as follows. Create a dataframe from the correlation matrix and set the diagonal elements to missing since these are always 1: correlations &lt;- as.data.frame(spotify_matrix) diag(correlations) &lt;- NA Now we can use the apply() function to count the number of correlations for each variable that are below a certain threshold (say, 0.3). The apply() function is very useful as it lets you apply function by the rows or columns in your dataframe. In the following example abs(correlations) &lt; 0.3 returns a logical value for the correlation matrix that returns TRUE if the statement is true. The second argument 1 means that the function should be applied to the rows (2 would apply it to the columns). The third argument states the function that should be applied. In our case, we would like to count the number of absolute correlations below 0.3 so we apply the sum function, which sums the number of TRUE occurrences by row. The final argument na.rm = TRUE simply tells R to neglect the missing values that we have created for the diagonals of the matrix. apply(abs(correlations) &lt; 0.3, 1, sum, na.rm = TRUE) ## danceability energy key loudness mode acousticness ## 7 5 8 6 8 6 ## liveness valence tempo ## 8 6 8 The output shows you the number of correlations below the threshold for each variable. In a similar way, it would also be possible to compute the mean correlation for each variable. apply(abs(correlations),1,mean,na.rm=TRUE) ## danceability energy key loudness mode acousticness ## 0.16283732 0.29136769 0.03108516 0.23636671 0.07951326 0.21539451 ## liveness valence tempo ## 0.06474855 0.17491214 0.05117508 Another way to make the correlations more salient is to plot the correlation matrix using different colors that indicate the strength of the correlations. This can be done using the corPlot() function from the psych package. corPlot(correlations,numbers=TRUE,upper=FALSE,diag=FALSE,xlas=2,main=&quot;Correlations between variables&quot;) Figure 7.2: Correlation matrix You will, however, notice that this is a rather subjective approach. The Bartlett’s test is a statistical test that can be used to test whether all the off-diagonal elements in the population correlation matrix are zero (i.e., whether the population correlation matrix resembles an identify matrix). Thus, it tests whether the correlations are overall too small. If the matrix is an identify matrix, it means that all variables are independent. Thus, a significant test statistic (i.e., p &lt; 0.05) indicates that there is some relationship between variables. The test can be implemented using the cortest.bartlett() function from the psych package: library(psych) cortest.bartlett(spotify_matrix, n = nrow(music_data)) ## $chisq ## [1] 85448.1 ## ## $p.value ## [1] 0 ## ## $df ## [1] 36 In our example, the p-value is less than 0.05, which is good news since it confirms that overall the correlation between variables is different from zero. The other problem that might occur is that the correlations are too high. Actually, a certain degree of multicollinearity is not a problem in PCA. However, it is important to avoid extreme multicollinearity (i.e., variables are highly correlated) and singularity (i.e., variables are perfectly correlated). Multicollinearity causes problems, because it becomes difficult to determine the unique contribution of a variable (as was the case in linear regression analysis). Again, inspecting the entire correlation matrix when there are many variables will be a tedious task. . apply(abs(correlations) &gt; 0.8, 1, sum, na.rm = TRUE) ## danceability energy key loudness mode acousticness ## 0 0 0 0 0 0 ## liveness valence tempo ## 0 0 0 The results do not suggest any extreme or perfect correlations. Again, there is a more objective measure that could be applied. The determinant tells us whether the correlation matrix is singular (determinant = 0), or if all variables are completely unrelated (determinant = 1), or somewhere in between. As a rule of thumb, the determinant should be greater than 0.00001. The det() function can be used to compute the determinant: det(spotify_matrix) ## [1] 0.1743137 det(spotify_matrix) &gt; 0.00001 ## [1] TRUE As you can see, the determinant is larger than the threshold, indicating that the overall correlation between variables is not too strong. Finally, you should test if the correlation pattern in the matrix is appropriate for factor analysis using the Kaiser-Meyer-Olkin (KMO) measure of sampling adequacy. This statistic is a measure of the proportion of variance among variables that might be common variance. \\[\\begin{equation} MSA_j=\\frac{\\sum_{k\\ne j}^{}{r^2_{jk}}}{\\sum_{k\\ne j}^{}{r^2_{jk}}+\\sum_{k\\ne j}^{}{p^2_{jk}}} \\tag{7.7} \\end{equation}\\] where \\(r_{jk}\\) is the correlation between two variables of interest and \\(p_{jk}\\) is their partial correlation. The partial correlation measures the degree of association between the two variables, when the effect of the remaining variables is controlled for. It can take values between 0 (bad) and 1 (good), where a value of 0 indicates that the sum of partial correlations is large relative to the sum of correlations. If the remaining correlation between two variables remains high if you control for all the other variables, this provides an indication that the correlation is fairly concentrated and on a subset of the variables and factor analysis is likely to be inappropriate. In contrast, a value close to 1 means that the sum of the partial correlations is fairly low, indicating a more compact pattern of correlations between a larger set of variables. Values should at least exceed 0.50, with the thresholds: &lt;.50 = unacceptable &gt;.50 = miserable &gt;.60 = mediocre &gt;.70 = middling &gt;.80 = meritorious &gt;.90 = marvelous The KMO measure of sampling adequacy can be computed using the KMO() function from the psych package: KMO(music_data) ## Kaiser-Meyer-Olkin factor adequacy ## Call: KMO(r = music_data) ## Overall MSA = 0.64 ## MSA for each item = ## danceability energy key loudness mode acousticness ## 0.58 0.61 0.51 0.68 0.62 0.71 ## liveness valence tempo ## 0.59 0.60 0.59 You can see that the statistic is calculated for the entire matrix and for each variable individually. In our example, the values for all variables as well as the overall matrix is above 0.5, suggesting that factor analysis is appropriate. 7.2.2 Deriving factors After testing that the data can be used for PCA, we can move on to conducting PCA. Conducting PCA is fairly simple in R using the principal() function from the psych package. However, before we apply the function to our data, it is useful to reflect on the goals of PCA again. The goal is to explain the maximum amount of total variance in a correlation matrix by transforming the original variables into a smaller set of linear components (factors). So the first decision we have to make is how many factors we should extract. There are different methods that can be used to decide on the appropriate number of factors, including: A priori determination: Requires prior knowledge Determination based on percentage of variance: When cumulative percentage of variance extracted by the factors reaches a satisfactory level (e.g., factors extracted should account for at least 60% of the variance) Eigenvalues: Eigenvalues refer to the total variance that is explained by each factor. Factors with eigenvalues greater than 1.0 are retained (factors with a variance less than 1.0 are no better than a single variable) Scree plot: Is a plot of the eigenvalues against the number of factors in order of extraction. Assumption: the point at which the scree begins denotes the true number of factors. Often, the decision is made based on a combination of different criteria. By extracting as many factors as there are variables we can inspect their eigenvalues and make decisions about which factors to extract. Since we have 9 variables, we set the argument nfactors to 9, which is equivalent to the number of columns ncol in our dataset. pc1 &lt;- principal(music_data, nfactors = ncol(music_data), rotate = &quot;none&quot;) pc1 ## Principal Components Analysis ## Call: principal(r = music_data, nfactors = ncol(music_data), rotate = &quot;none&quot;) ## Standardized loadings (pattern matrix) based upon correlation matrix ## PC1 PC2 PC3 PC4 PC5 PC6 PC7 PC8 PC9 h2 ## danceability 0.45 -0.56 -0.39 -0.03 0.20 -0.07 -0.45 0.28 0.05 1 ## energy 0.89 0.18 0.06 0.03 -0.12 0.06 0.16 0.00 0.35 1 ## key 0.05 -0.40 0.69 0.08 0.12 0.58 -0.10 0.02 0.00 1 ## loudness 0.80 0.19 0.03 -0.02 -0.22 0.10 0.23 0.39 -0.23 1 ## mode -0.19 0.50 -0.51 -0.01 0.12 0.64 -0.14 0.02 0.02 1 ## acousticness -0.73 -0.12 -0.04 0.09 0.32 -0.01 0.40 0.40 0.13 1 ## liveness 0.17 0.45 0.22 0.70 0.38 -0.19 -0.22 0.07 -0.03 1 ## valence 0.57 -0.27 -0.25 0.05 0.57 0.07 0.35 -0.28 -0.09 1 ## tempo 0.12 0.39 0.30 -0.69 0.47 -0.14 -0.13 0.07 0.00 1 ## u2 com ## danceability -0.00000000000000133 4.7 ## energy 0.00000000000000044 1.5 ## key 0.00000000000000278 2.8 ## loudness 0.00000000000000067 2.2 ## mode 0.00000000000000155 3.3 ## acousticness 0.00000000000000022 2.9 ## liveness 0.00000000000000011 3.3 ## valence 0.00000000000000000 4.2 ## tempo -0.00000000000000067 3.2 ## ## PC1 PC2 PC3 PC4 PC5 PC6 PC7 PC8 PC9 ## SS loadings 2.58 1.24 1.10 0.98 0.91 0.83 0.66 0.48 0.21 ## Proportion Var 0.29 0.14 0.12 0.11 0.10 0.09 0.07 0.05 0.02 ## Cumulative Var 0.29 0.42 0.55 0.66 0.76 0.85 0.92 0.98 1.00 ## Proportion Explained 0.29 0.14 0.12 0.11 0.10 0.09 0.07 0.05 0.02 ## Cumulative Proportion 0.29 0.42 0.55 0.66 0.76 0.85 0.92 0.98 1.00 ## ## Mean item complexity = 3.1 ## Test of the hypothesis that 9 components are sufficient. ## ## The root mean square of the residuals (RMSR) is 0 ## with the empirical chi square 0 with prob &lt; NA ## ## Fit based upon off diagonal values = 1 The output is quite complex, but we will focus only on the SS loadings for now, which are the Eigenvalues (a.k.a. sum of squared loadings). One common rule is to retain factors with eigenvalues greater than 1.0. So based on this rule, we would extract 3 factors (i.e., the SS loadings for the 4th factor is &lt; 1). You can also plot the eigenvalues against the number of factors in order of extraction using a so-called Scree plot: plot(pc1$values, type=&quot;b&quot;) axis(side=1,c(1:ncol(music_data))) abline(h=1, lty=2) abline(v=nfac, lty=2) Figure 7.3: Scree plot The dashed line is simply a visualization of the rule that we will retain factors with Eigenvalues &gt; 1 (suggesting 3 factors). Another criterion based on this plot would be to find the point where the curve flattens (point of inflection). If the largest few eigenvalues in the covariance matrix dominate in magnitude, then the scree plot will exhibit an “elbow”. From that point onwards, the incremental gain in explained variance is rather low. Also according to this criterion, we would extract 3 factors. Taken together, the results suggest that we should extract 3 factors. Now that we know how many components we want to extract, we can rerun the analysis, specifying that number: pc2 &lt;- principal(music_data, nfactors = nfac, rotate = &quot;none&quot;) pc2 ## Principal Components Analysis ## Call: principal(r = music_data, nfactors = nfac, rotate = &quot;none&quot;) ## Standardized loadings (pattern matrix) based upon correlation matrix ## PC1 PC2 PC3 h2 u2 com ## danceability 0.45 -0.56 -0.39 0.67 0.33 2.7 ## energy 0.89 0.18 0.06 0.83 0.17 1.1 ## key 0.05 -0.40 0.69 0.63 0.37 1.6 ## loudness 0.80 0.19 0.03 0.68 0.32 1.1 ## mode -0.19 0.50 -0.51 0.55 0.45 2.3 ## acousticness -0.73 -0.12 -0.04 0.55 0.45 1.1 ## liveness 0.17 0.45 0.22 0.28 0.72 1.8 ## valence 0.57 -0.27 -0.25 0.46 0.54 1.9 ## tempo 0.12 0.39 0.30 0.26 0.74 2.1 ## ## PC1 PC2 PC3 ## SS loadings 2.58 1.24 1.10 ## Proportion Var 0.29 0.14 0.12 ## Cumulative Var 0.29 0.42 0.55 ## Proportion Explained 0.52 0.25 0.22 ## Cumulative Proportion 0.52 0.78 1.00 ## ## Mean item complexity = 1.7 ## Test of the hypothesis that 3 components are sufficient. ## ## The root mean square of the residuals (RMSR) is 0.12 ## with the empirical chi square 52467.02 with prob &lt; 0 ## ## Fit based upon off diagonal values = 0.7 Now that the output is less complex, we can inspect the remaining statistics. The first part of the output is the factor matrix, which contains the factor loadings of all the variables on the 3 extracted factors (i.e., PC1, PC2, PC3). As we have said before, the factor loadings are the correlations between the variables and the factors. The factor matrix also contains the columns “h2” and “u2”. “h2” refers to the communality, which is the proportion of variance a variable shares with all the other variables being considered. This is also the proportion of variance explained by the common factors. In contrast, “u2” refers to the unique variance, which is the proportion of the variance that is unique to a particular variable. In PCA, we are primarily interested in the common variance. When the communality is very low (say &lt;.30), a variable is “quite unique” and should be removed, as it is definitely measuring “something else”. In our example, all communalities (i.e., h2) are above 0.3 so that we retain all variables. #pc2$communality &lt; 0.3 sum(pc2$communality &lt; 0.3) ## [1] 2 In our example this is not true because the communalities of “tempo” and “liveness” are smaller 0.3. For illustrative purposes we will still continue our analysis. Note that there is a difference between PCA and common factor analysis. PCA focuses on the variance and aims to reproduce the total variable variance. This means that the components reflect both common and unique variance of the variables. Factor analysis, in contrast, focuses on the correlation and aims to reproduce the correlations among variables. Here, the factors only represent the common variance that variable share and do not include the unique variance. In other words, while factor analysis focuses on explaining the off-diagonal terms in the correlation matrix (i.e., shared co-variance), PCA focuses on explaining the diagonal terms (i.e., the variances). However, although PCA aims to reproduce the on-diagonal terms in the correlation matrix, it also tends to fit the off-diagonal correlations quite well. Hence, the results are often comparable. See also here. You should also take a closer look at the residuals in order to check whether you have extracted the correct number of factors. The difference between the reproduced and the actual correlation matrices are the residuals. We can extract the residuals from our model using the factor.residuals() function from the psych package. It takes the original correlation matrix and the factor loadings as arguments: residuals &lt;- factor.residuals(spotify_matrix, pc2$loadings) round(residuals,3) ## danceability energy key loudness mode acousticness liveness ## danceability 0.326 -0.081 0.031 -0.057 0.046 -0.002 0.179 ## energy -0.081 0.168 0.005 -0.015 0.007 0.073 -0.077 ## key 0.031 0.005 0.369 0.018 0.401 0.009 0.007 ## loudness -0.057 -0.015 0.018 0.318 0.012 0.150 -0.139 ## mode 0.046 0.007 0.401 0.012 0.447 -0.015 -0.058 ## acousticness -0.002 0.073 0.009 0.150 -0.015 0.451 0.116 ## liveness 0.179 -0.077 0.007 -0.139 -0.058 0.116 0.721 ## valence -0.138 -0.038 0.070 -0.124 0.053 0.201 0.143 ## tempo 0.202 -0.103 -0.062 -0.104 -0.007 0.066 -0.247 ## valence tempo ## danceability -0.138 0.202 ## energy -0.038 -0.103 ## key 0.070 -0.062 ## loudness -0.124 -0.104 ## mode 0.053 -0.007 ## acousticness 0.201 0.066 ## liveness 0.143 -0.247 ## valence 0.542 0.153 ## tempo 0.153 0.737 Note that the diagonal elements in the residual matrix correspond to the unique variance in each variable that cannot be explained by the factors (i.e., “u2” in the output above). For example, the proportion of unique variance for danceability is 0.33, which is reflected in the first cell in the residual matrix. The off-diagonal elements represent the difference between the actual correlations and the correlation based on the reproduced correlation matrix for all variable pairs. To see this, the reproduced correlation matrix could be generated by using the factor.model() function: reproduced_matrix &lt;- factor.model(pc2$loadings) round(reproduced_matrix,3) ## danceability energy key loudness mode acousticness liveness ## danceability 0.674 0.278 -0.020 0.244 -0.169 -0.245 -0.264 ## energy 0.278 0.832 0.016 0.752 -0.116 -0.675 0.244 ## key -0.020 0.016 0.631 -0.014 -0.562 -0.015 -0.018 ## loudness 0.244 0.752 -0.014 0.682 -0.077 -0.610 0.227 ## mode -0.169 -0.116 -0.562 -0.077 0.553 0.101 0.078 ## acousticness -0.245 -0.675 -0.015 -0.610 0.101 0.549 -0.187 ## liveness -0.264 0.244 -0.018 0.227 0.078 -0.187 0.279 ## valence 0.507 0.440 -0.039 0.395 -0.114 -0.369 -0.083 ## tempo -0.286 0.197 0.058 0.181 0.017 -0.149 0.265 ## valence tempo ## danceability 0.507 -0.286 ## energy 0.440 0.197 ## key -0.039 0.058 ## loudness 0.395 0.181 ## mode -0.114 0.017 ## acousticness -0.369 -0.149 ## liveness -0.083 0.265 ## valence 0.458 -0.116 ## tempo -0.116 0.263 You can see that the reproduced correlation between the first and second variable is 0.278. From the correlation table from the beginning we know, however, that the observed correlation was 0.198. Hence, the difference between the observed and reproduced correlation is: 0.198 - 0.278 = -0.081, which corresponds to the residual of this variable pair in the residual matrix. Note that the diagonal elements in the reproduced matrix correspond to the communalities in the model summary above (i.e., “h2”). A measure of fit can now be computed based on the size of the residuals. In the worst case, the residuals would be as large as the correlations in the original matrix, which would be the case if we extracted no factors at all. A measure of fit could therefore be the sum of the squared residuals divided by the sum of the squared correlations. We square the residuals to account for positive and negative deviations. Values &gt;0.90 are considered indicators of good fit. From the output above, you can see that: “Fit based upon off diagonal values = 0.7”. Thus, we conclude that the model has a middling fit. You could also manually compute this statistic by summing over the squared residuals and correlations, take their ratio and subtract it from one (note that we use the upper.tri() function to use the upper triangle of the matrix only; this has the effect of discarding the diagonal elements and the elements below the diagonal). ssr &lt;- (sum(residuals[upper.tri((residuals))]^2)) #sum of squared residuals ssc &lt;- (sum(spotify_matrix[upper.tri((spotify_matrix))]^2)) #sum of squared correlations 1-(ssr/ssc) #model fit ## [1] 0.7028767 In a next step, we check the size of the residuals. If fewer residuals than 50% have absolute values greater than 0.05 the model is a good fit. This can be tested using the following code. We first convert the residuals to a matrix and select the upper triangular again to avoid duplicates. Finally, we count all occurrences where the absolute value is larger than 0.05 and divide it by the number of total observations to get the proportion. residuals &lt;- as.matrix(residuals[upper.tri((residuals))]) large_res &lt;- abs(residuals) &gt; 0.05 sum(large_res) ## [1] 23 sum(large_res)/nrow(residuals) ## [1] 0.6388889 In our example, we cannot confirm that the proportion of residuals &gt; 0.05 is less than 50%. Another way to evaluate the residuals is by looking at their mean value (rather, we square the residuals first to account for positive and negative values, compute the mean and then take the square root). sqrt(mean(residuals^2)) ## [1] 0.1220501 This means that our mean residual is 0.1221 and this value should be as low as possible. Finally, we need to validate if the residuals are approximately normally distributed, which we do by using a histogram, a Q-Q plot and the Shapiro test. hist(residuals) Figure 7.4: Histogram of residuals qqnorm(residuals) qqline(residuals) Figure 7.5: Q-Q plot shapiro.test(residuals) ## ## Shapiro-Wilk normality test ## ## data: residuals ## W = 0.96007, p-value = 0.2161 All of the tests suggest that the distribution of the residuals is approximately normal. 7.2.3 Factor interpretation To aid interpretation, it is possible to maximize the loading of a variable on one factor while minimizing its loading on all other factors. This is known as factor rotation. There are two types of factor rotation: orthogonal (assumes that factors are uncorrelated) oblique (assumes that factors are intercorrelated) To carry out a orthogonal rotation, we change the rotate option in the principal() function from “none” to “varimax” (we could also exclude it altogether because varimax is the default if the option is not specified): pc3 &lt;- principal(music_data, nfactors = nfac, rotate = &quot;varimax&quot;) pc3 ## Principal Components Analysis ## Call: principal(r = music_data, nfactors = nfac, rotate = &quot;varimax&quot;) ## Standardized loadings (pattern matrix) based upon correlation matrix ## RC1 RC2 RC3 h2 u2 com ## danceability 0.42 -0.70 0.06 0.67 0.33 1.7 ## energy 0.90 0.15 0.05 0.83 0.17 1.1 ## key -0.04 0.07 0.79 0.63 0.37 1.0 ## loudness 0.81 0.14 0.01 0.68 0.32 1.1 ## mode -0.11 0.12 -0.73 0.55 0.45 1.1 ## acousticness -0.73 -0.09 -0.05 0.55 0.45 1.0 ## liveness 0.19 0.49 -0.06 0.28 0.72 1.3 ## valence 0.55 -0.39 0.01 0.46 0.54 1.8 ## tempo 0.14 0.49 0.03 0.26 0.74 1.2 ## ## RC1 RC2 RC3 ## SS loadings 2.56 1.20 1.17 ## Proportion Var 0.28 0.13 0.13 ## Cumulative Var 0.28 0.42 0.55 ## Proportion Explained 0.52 0.24 0.24 ## Cumulative Proportion 0.52 0.76 1.00 ## ## Mean item complexity = 1.2 ## Test of the hypothesis that 3 components are sufficient. ## ## The root mean square of the residuals (RMSR) is 0.12 ## with the empirical chi square 52467.02 with prob &lt; 0 ## ## Fit based upon off diagonal values = 0.7 Interpreting the factor loading matrix is a little complex, so we can make it easier by using the print.psych() function, which we can use to exclude loading below a cutoff from the display and order the variables by their loading within each factor. In the following, we will only display loadings that exceed the value 0.3. print.psych(pc3, cut = 0.3, sort = TRUE) ## Principal Components Analysis ## Call: principal(r = music_data, nfactors = nfac, rotate = &quot;varimax&quot;) ## Standardized loadings (pattern matrix) based upon correlation matrix ## item RC1 RC2 RC3 h2 u2 com ## energy 2 0.90 0.83 0.17 1.1 ## loudness 4 0.81 0.68 0.32 1.1 ## acousticness 6 -0.73 0.55 0.45 1.0 ## valence 8 0.55 -0.39 0.46 0.54 1.8 ## danceability 1 0.42 -0.70 0.67 0.33 1.7 ## tempo 9 0.49 0.26 0.74 1.2 ## liveness 7 0.49 0.28 0.72 1.3 ## key 3 0.79 0.63 0.37 1.0 ## mode 5 -0.73 0.55 0.45 1.1 ## ## RC1 RC2 RC3 ## SS loadings 2.56 1.20 1.17 ## Proportion Var 0.28 0.13 0.13 ## Cumulative Var 0.28 0.42 0.55 ## Proportion Explained 0.52 0.24 0.24 ## Cumulative Proportion 0.52 0.76 1.00 ## ## Mean item complexity = 1.2 ## Test of the hypothesis that 3 components are sufficient. ## ## The root mean square of the residuals (RMSR) is 0.12 ## with the empirical chi square 52467.02 with prob &lt; 0 ## ## Fit based upon off diagonal values = 0.7 After obtaining the rotated matrix, variables with high loading are used for interpreting (=naming) the factor. Note that factor loading can be positive or negative (depends on scaling of the variable), thus take care when interpreting! Look for simple structure: each variable (hopefully) loads high on 1 factor and low on other factors. As an example, we could name our factors as follows: Factor 1: energetic Factor 2: fast Factor 3: pitch The previous type of rotation (i.e., “varimax”) assumed that the factors are independent. Oblique rotation is another type of rotation that can handle correlation between the factors. The command for an oblique rotation is very similar to that for an orthogonal rotation – we just change the rotate option from “varimax” to “oblimin”. pc4 &lt;- principal(music_data, nfactors = nfac, rotate = &quot;oblimin&quot;, scores = TRUE) print.psych(pc4, cut = 0.3, sort = TRUE) ## Principal Components Analysis ## Call: principal(r = music_data, nfactors = nfac, rotate = &quot;oblimin&quot;, ## scores = TRUE) ## Standardized loadings (pattern matrix) based upon correlation matrix ## item TC1 TC2 TC3 h2 u2 com ## energy 2 0.91 0.83 0.17 1.0 ## loudness 4 0.82 0.68 0.32 1.0 ## acousticness 6 -0.73 0.55 0.45 1.0 ## danceability 1 0.79 0.67 0.33 1.0 ## valence 8 0.36 0.50 0.46 0.54 1.8 ## tempo 9 0.33 -0.47 0.26 0.74 1.8 ## liveness 7 0.39 -0.46 0.28 0.72 2.0 ## key 3 0.80 0.63 0.37 1.0 ## mode 5 -0.72 0.55 0.45 1.1 ## ## TC1 TC2 TC3 ## SS loadings 2.43 1.33 1.16 ## Proportion Var 0.27 0.15 0.13 ## Cumulative Var 0.27 0.42 0.55 ## Proportion Explained 0.49 0.27 0.24 ## Cumulative Proportion 0.49 0.76 1.00 ## ## With component correlations of ## TC1 TC2 TC3 ## TC1 1.00 0.23 0.06 ## TC2 0.23 1.00 0.07 ## TC3 0.06 0.07 1.00 ## ## Mean item complexity = 1.3 ## Test of the hypothesis that 3 components are sufficient. ## ## The root mean square of the residuals (RMSR) is 0.12 ## with the empirical chi square 52467.02 with prob &lt; 0 ## ## Fit based upon off diagonal values = 0.7 The component correlations indicate that the factors are not correlated, therefore orthogonal rotation is appropriate. 7.2.4 Creating new variables Once we have decided on the final model, we can calculate the new variables as the weighted sum of the variables that form a factor. This means, we estimate a person’s score on a factor based on their scores on the items that constitute the measurement scales. These scores are also referred to as the factor scores. Because we have used the scores = TRUE argument in the previous command, the factor scores have already been created for us. By default, R uses the regression method to compute the factor scores, which controls for differences in the units of measurement. You can access the residuals as follows: head(pc4$scores) ## TC1 TC2 TC3 ## [1,] 0.2790735 -0.6352026 -0.09713790 ## [2,] -0.3402132 0.9864093 -0.08629953 ## [3,] 0.5673092 -2.2430912 0.16181608 ## [4,] 0.5118725 0.9366503 1.34593205 ## [5,] 0.4376880 0.2085055 -1.40321622 ## [6,] -0.2050096 1.0838175 1.45112640 We can also use the cbind() function to add the computed factor scores to the existing data set: music_data &lt;- cbind(music_data, pc4$scores) This way, it is easier to use the new variables in subsequent analysis (e.g., t-tests, regression, ANOVA, cluster analysis). 7.3 Reliability analysis When you are using multi-item scales to measure a latent construct (e.g., the output from the PCA above), it is useful to check the reliability of your scale. Reliability means that our items should consistently reflect the construct that they are intended to measure. In other words, individual items should produce results consistent with the overall scale. This means that for a scale to be reliable, the score of a person on one half of the items should be similar to the score derived based on the other half of the items (split-half reliability). The problem is that there are several ways in which data can be split. A measure that reflects this underlying intuition is Cronbach’s alpha, which is approximately equal to the average of all possible split-half reliabilities. It is computed as follows: \\[\\begin{equation} \\alpha=\\frac{N^2\\overline{Cov}}{\\sum{s^2_{item}}+\\sum{Cov_{item}}} \\tag{7.8} \\end{equation}\\] The share of the items common variance (inter-correlation) in the total variance is supposed to be as high as possible across all items. The thresholds are as follows: &gt;0.7 reasonable for practical application/explorative research &gt;0.8 necessary for fundamental research &gt;0.9 desirable for applied research To see if the subscales that were derived from the previous PCA exhibit a sufficient degree of reliability, we first create subsets of our data set that contain the respective items for each of the factors (we use the results from the oblimin rotation here): energetic &lt;- music_data[,c(rc1)] pitch &lt;- music_data[,c(rc3)] dancefloor &lt;- music_data[,c(rc2)] Now we can use the alpha() function from the psych package to test the reliability. Note that the keys argument may be used to indicate reverse coded items. psych::alpha(energetic, check.keys=TRUE) ## ## Reliability analysis ## Call: psych::alpha(x = energetic, check.keys = TRUE) ## ## raw_alpha std.alpha G6(smc) average_r S/N ase mean sd median_r ## 0.26 0.74 0.76 0.36 2.9 0.0013 -13 0.64 0.32 ## ## lower alpha upper 95% confidence boundaries ## 0.26 0.26 0.26 ## ## Reliability if an item is dropped: ## raw_alpha std.alpha G6(smc) average_r S/N alpha se var.r med.r ## danceability 0.25 0.76 0.76 0.44 3.1 0.0012 0.044 0.43 ## energy 0.18 0.61 0.57 0.28 1.6 0.0013 0.013 0.26 ## loudness 0.65 0.66 0.66 0.33 2.0 0.0025 0.026 0.31 ## acousticness- 0.19 0.69 0.70 0.36 2.3 0.0012 0.042 0.32 ## valence 0.23 0.73 0.73 0.41 2.7 0.0011 0.054 0.35 ## ## Item statistics ## n raw.r std.r r.cor r.drop mean sd ## danceability 48919 0.27 0.57 0.39 0.23 0.65 0.15 ## energy 48919 0.79 0.84 0.85 0.77 0.64 0.18 ## loudness 48919 0.99 0.76 0.72 0.59 -6.85 2.82 ## acousticness- 48919 0.55 0.71 0.61 0.48 -58.64 0.26 ## valence 48919 0.36 0.63 0.48 0.30 0.50 0.23 psych::alpha(pitch, check.keys=TRUE) ## ## Reliability analysis ## Call: psych::alpha(x = pitch, check.keys = TRUE) ## ## raw_alpha std.alpha G6(smc) average_r S/N ase mean sd median_r ## 0.082 0.28 0.16 0.16 0.38 0.0022 3.1 1.9 0.16 ## ## lower alpha upper 95% confidence boundaries ## 0.08 0.08 0.09 ## ## Reliability if an item is dropped: ## raw_alpha std.alpha G6(smc) average_r S/N alpha se var.r med.r ## key- 0.161 0.16 0.026 0.16 NA NA 0.161 0.16 ## mode 0.026 0.16 NA NA NA NA 0.026 0.16 ## ## Item statistics ## n raw.r std.r r.cor r.drop mean sd ## key- 48919 0.99 0.76 0.31 0.16 5.66 3.62 ## mode 48919 0.29 0.76 0.31 0.16 0.58 0.49 psych::alpha(dancefloor, check.keys=TRUE) ## ## Reliability analysis ## Call: psych::alpha(x = dancefloor, check.keys = TRUE) ## ## raw_alpha std.alpha G6(smc) average_r S/N ase mean sd median_r ## 0.00067 0.25 0.25 0.076 0.33 0.00015 86 7.1 0.051 ## ## lower alpha upper 95% confidence boundaries ## 0 0 0 ## ## Reliability if an item is dropped: ## raw_alpha std.alpha G6(smc) average_r S/N alpha se var.r ## danceability -0.00062 -0.083 -0.052 -0.026 -0.077 0.00013 0.0016 ## liveness- 0.00048 0.326 0.295 0.139 0.483 0.00015 0.0435 ## valence 0.00161 0.166 0.119 0.062 0.199 0.00010 0.0015 ## tempo- 0.31124 0.312 0.289 0.131 0.453 0.00503 0.0477 ## med.r ## danceability -0.037 ## liveness- 0.084 ## valence 0.084 ## tempo- 0.085 ## ## Item statistics ## n raw.r std.r r.cor r.drop mean sd ## danceability 48919 0.093 0.69 0.621 0.087 0.65 0.15 ## liveness- 48919 0.023 0.47 0.053 0.018 232.51 0.14 ## valence 48919 -0.027 0.57 0.376 -0.035 0.50 0.23 ## tempo- 48919 1.000 0.48 0.069 0.019 111.38 28.21 The above output would lead us to conclude that the fear of computers, fear of statistics and fear of maths subscales of the RAQ all had sufficiently high reliabilities (i.e., Cronbach’s alpha &gt; 0.70). However, the fear of negative peer evaluation subscale had relatively low reliability (Cronbach’s alpha = 0.57). As the output under “Reliability if an item is dropped” suggests, the alpha score would also not increase if an item was dropped from the scale. As another example, consider the multi-item scale from the statistical ability questionnaire. test_data &lt;- read.table(&quot;https://raw.githubusercontent.com/IMSMWU/Teaching/master/MRDA2017/survey2017.dat&quot;, sep = &quot;\\t&quot;, header = TRUE) head(test_data) The four variables “multi_1” - “multi_4” represent the multi-item scales. We can test the reliability of the scale using the alpha() function (item 4 was reverse coded, hence the “-1” in the keys vector): psych::alpha(test_data[,c(&quot;multi_1&quot;,&quot;multi_2&quot;,&quot;multi_3&quot;,&quot;multi_4&quot;)], keys=c(1,1,1,-1)) ## ## Reliability analysis ## Call: psych::alpha(x = test_data[, c(&quot;multi_1&quot;, &quot;multi_2&quot;, &quot;multi_3&quot;, ## &quot;multi_4&quot;)], keys = c(1, 1, 1, -1)) ## ## raw_alpha std.alpha G6(smc) average_r S/N ase mean sd median_r ## 0.9 0.9 0.88 0.68 8.7 0.024 2.7 0.92 0.65 ## ## lower alpha upper 95% confidence boundaries ## 0.85 0.9 0.94 ## ## Reliability if an item is dropped: ## raw_alpha std.alpha G6(smc) average_r S/N alpha se var.r med.r ## multi_1 0.85 0.86 0.81 0.66 5.9 0.035 0.0101 0.61 ## multi_2 0.83 0.83 0.77 0.63 5.0 0.039 0.0028 0.61 ## multi_3 0.86 0.86 0.83 0.68 6.3 0.034 0.0195 0.61 ## multi_4- 0.91 0.91 0.88 0.77 10.0 0.022 0.0058 0.78 ## ## Item statistics ## n raw.r std.r r.cor r.drop mean sd ## multi_1 55 0.89 0.89 0.86 0.80 2.7 1.0 ## multi_2 55 0.92 0.92 0.92 0.85 2.6 1.0 ## multi_3 55 0.88 0.88 0.83 0.78 2.5 1.1 ## multi_4- 55 0.81 0.80 0.68 0.65 2.8 1.1 ## ## Non missing response frequency for each item ## 1 2 3 4 5 miss ## multi_1 0.11 0.36 0.25 0.25 0.02 0 ## multi_2 0.11 0.42 0.25 0.18 0.04 0 ## multi_3 0.18 0.38 0.22 0.20 0.02 0 ## multi_4 0.04 0.31 0.16 0.40 0.09 0 Since the scale exhibits a sufficient degree of reliability, we can compute the new variable as the average score on these items. However, before doing this, we need to recode the reverse coded variable in the appropriate way. It is easy to recode the reverse coded item to be in line with the remaining items on the dimension using the recode() function from the car package. library(car) test_data$multi_4_rec = recode(test_data$multi_4, &quot;1=5; 2=4; 3=3; 4=2; 5=1&quot;) Now we can compute the new variable as the average score of the four items: library(car) test_data$new_variable = (test_data$multi_1 + test_data$multi_2 + test_data$multi_3 + test_data$multi_4_rec) / 4 7.4 Summary of factor analysis Factor analysis belongs to a class of procedures used for data reduction. The basic assumption behind factor analysis is that for the set of observed variables there is a set of underlying variables (“factors”) that has smaller number of variables and can explain the relationships among variables. Factor analysis can be divided in exploratory and confirmatory. An exploratory factor analysis explores the relationships among the variables and does not require an a-priori fixed number of underlying factors to be given. One can have a good guess about what are the underlying factors behind the large set of variables, but still aims to form a specific hypothesis about it. Principal Components Analysis (PCA) is one of the most frequently used techniques of this sort. On the other hand, a confirmatory factor analysis requires a firm idea about the number of underlying variables. Main goals of PCA: Identification of underlying dimensions, or factors, that explain the correlations among a set of variables. Identification a new, smaller set of uncorrelated variables to replace the original set of correlated variables in subsequent multivariate analysis. Sequence of steps in PCA. First, we inspect the correlation matrix because, if the variables measure the same construct, we would expect to see a certain degree of correlation between the variables. However, we use Barttlet’s test to be certain weather the correlations are overall too small. At this point we can face two issues: too high correlation and too low correlation between variables. Too high correlation indicates possible extreme multicollinearity, while too low correlation suggests that variables do not indicate the same underlying construct. As an addtional tool for checking any extreme or perfect correlations, we can use determinant. It tells us whether the correlation matrix is singular (determinant = 0), or if all variables are completely unrelated (determinant = 1), or somewhere in between. Finally, we need to test if the correlation pattern in the matrix is appropriate for factor analysis using the Kaiser-Meyer-Olkin (KMO) measure of sampling adequacy. This statistic is a measure of the proportion of variance among variables that might be common variance. Second step in PCA is deriving factors. There are different methods used for making a decision on the appropriate number of factors: A priori determination Determination based on percentage of variance Method based on Eigenvalues Method based on a scree plot After we decide on the appropriate number of factors, we inspect a factor loadings in a factor matrix. More specifically, we take a closer look at communality, which is the proportion of variance a variable shares with all the other variables being considered. When the communality is very low (e.g. &lt;.30), a variable is “quite unique” and should be removed, as it is certainly measuring “something else”. Finally, in order to double-check whether we extracted the correct number of factors, we check residuals. The difference between the reproduced and the actual correlation matrices are the residuals. A measure of fit can be computed based on the size of the residuals. More specifically, a measure of fit could be the sum of the squared residuals (to account for possitve and negative deviations) divided by the sum of the squared correlations. Values &gt;0.90 are considered indicators of good fit. In a next step, we check the size of the residuals. If fewer residuals than 50% have absolute values greater than 0.05 the model is a good fit. Finally, we need to validate if the residuals are approximately normally distributed, which we do by using a histogram, a Q-Q plot and the Shapiro test. What comes next is a factor interpretation. To make an interpretation more convenient, we do a factor rotation. It means maximizing the loading of a variable on one factor while minimizing its loading on all other factors. We differ two types of factor rotation: orthogonal (assumes that factors are uncorrelated) oblique (assumes that factors are intercorrelated) After obtaining the rotated matrix, variables with high loading are used for naming the factor. It is important to note that factor loadings can be positive or negative, so we need to be careful when interpreting! In the best case scenario, each variable loads high on 1 factor and low on other factors. When you are using multi-item scales to measure a latent construct, it is necessary to check reliability of scale. Reliability means that items should consistently reflect the construct that they are intended to measure. For that we can use Cronbach’s alpha, which calculates the share itmes common variance in the total variance. The share of the items common variance (inter-correlation) in the total variance is supposed to be as high as possible across all items. "],
["appendix.html", "8 Appendix 8.1 Random Variables &amp; Probability Distributions 8.2 Regression", " 8 Appendix This chapter is primarily based on: Casella, G., &amp; Berger, R. L. (2002). Statistical inference (Vol. 2). Pacific Grove, CA: Duxbury (chapters 1 &amp; 3). 8.1 Random Variables &amp; Probability Distributions 8.1.1 Random variables 8.1.1.1 Why Random Variables? Random variables are used to extract and simplify information that we obtain from an experiment. For example, if we toss five coins it would be tedious to say “I have observed heads, heads, tails, heads, tails”. With five coins this might still be a feasible way to convey the outcome of the experiment, but what about 500 coins? Instead, we naturally condense the information into a random variable (even though we might not call it that) and say “I have observed three heads” or we define another random variable and say “I have observed two tails”. There are many ways to summarize the outcome of an experiment and hence we can define multiple random variables from the same experiment. We could also define a more “complicated” random variable that adds four points for each heads and one point for each tails. In general there are no restrictions on our function as long as it maps from the sample space to the real numbers. We distinguish two types of random variables, discrete and continuous, which are discussed in the following sections. 8.1.1.2 Tossing coins As a first example of a random variable, assume you toss a chosen number of coins (between 1 and 20). The tossing is the experiment in this case. The sample space consists of all the possible combinations of heads (“h”) and tails (“t”) given the number of coins. For a single coin: \\[ S = \\{h, t\\} \\] For two coins: \\[ S = \\{hh, ht, th, tt\\} \\] For three coins: \\[ S = \\{hhh, hht, hth, thh, tth, tht, htt, ttt\\} \\] and so on. One of the outcomes in the sample space will be realized whenever one tosses the coin(s). The definition of random variables allows for many possibilities. An operation that takes any realization of the experiment (e.g. hht) as an input and gives us a real number as an outcome is a random variable. For this example we have chosen “number of heads” as the function but it could also be number of tails or (number of heads)+(4*number of tails). Lets call our function \\(g\\). Then \\[ g(hhh) = 3 &gt; g(hht) = g(hth) = g(thh) = 2 &gt; g(tth) = g(tht) = g(htt) = 1 &gt; g(ttt) = 0 \\] for three coins. So far we have only considered the possible outcomes but not how likely they are. We might be interested in how likely it is to observe 2 or less heads when tossing three coins. Let’s first consider a fair coin. With a fair coin it is just as likely to get heads as it is tails. Formally \\[ p(h) = p(t) = 0.5 \\] By definition the probabilities have to add up to one. If you think of probabilities in percentages, this just expresses that with 100% certainty something happens. If we toss a coin we are certain that we get either heads or tails and thus for a fair coin the chance is 50/50. Below you will find an app where you can experiment with different numbers of coins and different probabilities. If you set the slider to 3 and the probability of observing h (\\(p(h)\\)) to \\(0.5\\) the cumulative distribution function will update accordingly. The dots mean that at that point we are already at the higher probability and not on the line below. Let’s analyze the result. The probability of observing less than or equal to 0 heads lies between 0 and 0.2. Of course we cannot observe a negative number of heads and so this is just the probability of observing no heads. There is only one realization of our experiment that fulfills that property: \\(g(ttt) = 0\\). So how likely is that to happen? Each coin has the probability 0.5 to show tails and we need all of them to land on tails. \\[ p(ttt) = 0.5 * 0.5 * 0.5 = 0.125 \\] Another way of calculating the probability is to look at the sample space. There are 8 equally likely outcomes (for fair coins!) one of which fulfills the property that we observe 0 heads. \\[ p(ttt) = \\frac{1}{8} = 0.125 = F_x(0) \\] The next “level” shows the probability of observing less than or equal to 1 head. That is the probability of observing 0 heads (\\(p(ttt) = 0.125\\)) plus the probability of observing one head (\\(p(htt) + p(tht) + p(tth)\\)). The probability of observing one head is given by the sum of the probabilities of the possibilities from the sample space. Let’s take a second to think about how probabilities are combined. If we want to know the probability of one event and another we have to multiply their respective probabilities such as in the case of \\(p(ttt)\\). There we wanted to know how likely it is that the first and the second and the third coin are all tails. Now we want to know the probability of either \\(p(ttt)\\) or \\(p(htt)\\) or \\(p(tht)\\) or \\(p(tth)\\). In the case that either event fulfills the condition we add the probabilities. This is possible because the probabilities are independent. That is, having observed heads (or tails) on the first coin does not influence the probability of observing heads on the others. \\[ p(ttt) = p(htt) = \\underbrace{0.5}_{p(h)} * \\underbrace{0.5}_{p(t)} *\\underbrace{0.5}_{p(t)} = p(tht) = p(tth)= 0.125 \\\\ \\Rightarrow F_X(1) = \\underbrace{0.125}_{p(ttt) = F_X(0)} + \\underbrace{0.125}_{p(htt)} + \\underbrace{0.125}_{p(tht)} + \\underbrace{0.125}_{p(tth)} = 0.5 \\] Now 4 out of the 8 possibilities (50%) in the sample space fulfill the property. For \\(F_X(2)\\) we add the probabilities of the observing 2 heads (\\(p(hht) + p(hth) + p(thh)\\)). \\[ F_X(2) = \\underbrace{0.5}_{F_X(1)} + \\underbrace{0.125}_{p(hht)} + \\underbrace{0.125}_{p(hth)} + \\underbrace{0.125}_{p(thh)} = 0.875 = \\frac{7}{8} \\] Since we are interested in less than or equal to we can always just add the probabilities of the possible outcomes at a given point to the cumulative distribution of the previous value (this gives us an idea about the link between cumulative distribution and probability mass functions). Now 7 out of 8 outcomes fulfill the property. Obviously the probability of observing 3 or less heads when tossing 3 coins is 1 (the certain outcome). This analysis changes if we consider a weighted coin that shows a higher probability on one side than the other. As the probability of observing heads increases the lines in the cumulative distribution shift downward. That means each of the levels are now less likely. In order to see why, let’s look at the probability of observing 2 or less heads when the probability of observing head is 0.75 (the probability of tails is thus 0.25) for each of the 3 coins. \\[ F_X(2) = \\overbrace{\\underbrace{0.25*0.25*0.25}_{p(ttt) = F_X(0) = 0.016} + \\underbrace{0.75 * 0.25 * 0.25}_{p(htt)=0.047} + \\underbrace{0.25 * 0.75 * 0.25}_{p(tht)=0.047} + \\underbrace{0.25 * 0.25 * 0.75}_{p(tth)=0.047}}^{F_X(1) = 0.156}\\dots\\\\ + \\underbrace{0.75 * 0.75 * 0.25}_{p(hht) = 0.141} + \\underbrace{0.75 * 0.25 * 0.75}_{p(hth) = 0.141} + \\underbrace{0.25 * 0.75 * 0.75}_{p(thh) = 0.141} = 0.578 \\] What happens if you decrease the probability of observing heads? The probability mass function defines the probability of observing an exact amount of heads (for all amounts) given the number of coins and the probability of observing heads. Continuing our example with 3 fair coins this means that \\(f_X(0) = p(ttt) = 0.125\\), \\(f_X(1) = p(htt) + p(tht) + p(tth) = 0.375\\), \\(f_X(2) = p(hht) + p(hth) + p(thh) = 0.375\\) and \\(f_X(3) = p(hhh) = 0.125\\). So instead of summing up the probabilities up to a given point we look at each point individually. This is also the link between the probability mass function and the cumulative distribution function: The cumulative distribution function at a given point (\\(x\\)) is just the sum of the probability mass function up to that point. That is \\[ F_X(0) = f_X(0),\\ F_X(1) = f_X(0) + f_X(1),\\ F_X(2) = f_X(0) + f_X(1) + f_X(2),\\ \\dots\\\\ F_X(x) = f_X(0) + f_X(1) + \\dots + f_X(x) \\] A more general way to write this is: \\[ F_X(x) = \\sum_{i=0}^x f_X(i) \\] 8.1.1.3 Sum of two dice Another example for a discrete random variable is the sum of two dice throws. Assume first that you have a six sided die. The six values it can take are all equally probable (if we assume that it is fair). Now if we throw two six sided dice and sum up the displayed dots, the possible values are no longer all equally probable. This is because some values can be produced by more combinations of throws. Consider the value 2. A 2 can only be produced by both dice displaying one dot. As the probability for a specific value on one die is \\(\\frac{1}{6}\\), the probability of both throws resulting in a 1 is \\(\\frac{1}{6} * \\frac{1}{6} = \\frac{1}{36}\\). Now consider the value 3. 3 can be produced by the first dice roll being a 1 and the second being a 2 and by the first roll being a 2 and the second a 1. While these may seem like the same thing, they are actually two distinct events. To calculate the probability of a 3 you sum the probabilities of these two possibilities together, i.e. \\(P\\{1,2\\} + P\\{2,1\\} = \\frac{1}{6} * \\frac{1}{6} + \\frac{1}{6} * \\frac{1}{6} = \\frac{2}{36}\\). This implies that a 3 is twice as probable as a 2. When done for all possible values of the sum of two dice you arrive at the following probabilities: \\[ P(x) = \\begin{cases} \\frac{1}{36} &amp; \\text{if }x = 2 \\text{ or } 12 \\\\ \\frac{2}{36} = \\frac{1}{18} &amp; \\text{if } x = 3 \\text{ or } 11\\\\ \\frac{3}{36} = \\frac{1}{12} &amp; \\text{if } x = 4 \\text{ or } 10\\\\ \\frac{4}{36} = \\frac{1}{9} &amp; \\text{if } x = 5 \\text{ or } 11\\\\ \\frac{5}{36} &amp; \\text{if } x = 6 \\text{ or } 8\\\\ \\frac{6}{36} = \\frac{1}{6} &amp; \\text{if } x = 7\\\\ \\end{cases} \\] To see what this looks like in practice you can simulate dice throws below. The program randomly throws two (or more) dice and displays their sum in a histogram with all previous throws. The longer you let the simulation run, the more the sample probabilities will converge to the theoretically calculated values above. 8.1.1.4 Discrete Random Variables Now that we have seen examples for discrete random variables, we can define them more formally. A random variable is discrete if its cumulative distribution function is a step function as in the plot below. That is, the CDF shifts or jumps from one probability to the next at some point(s). Notice that the black dots indicate that at that specific point the probability is already at the higher step. More formally: the CDF is “right-continuous”. That is the case for all CDFs. To illustrate this concept we explore the plot below. We have a discrete random variable as the CDF jumps rather than being one line. We can observe integer values between 0 and 10 whereas the probability of observing less than or equal to 0 is almost 0 and the probability of observing less than or equal to 10 is 1. The function is right continuous: Let’s look at the values 4 and 5 for example. The probability of observing 4 or less than 4 is just under 0.4. The probability of observing 5 or less is just over 0.6. For further examples see tossing coins and sum of two dice y &lt;- rbinom(1e5, 10, 0.5) plot(NULL, xlim=c(0, 10), ylim = c(0,1), ylab = &quot;Cumulative Probability&quot;, xlab = &quot;observed #&quot;, xaxt = &quot;n&quot;, main=&quot;Cumulative distribution function of a discrete random variable&quot;) axis(1, at = 0:10) grid(NULL, NULL, lwd = 1, lty = &#39;solid&#39;, col = &quot;gray93&quot;, equilogs = FALSE) lines(ecdf(y)) For discrete random variables the function that defines the probability of each event is called the probability mass function. Notice that the “jumps” in the CDF are equivalent to the mass at every point. It follows that the sum of the mass up to a point in the PMF (below) is equal to the level at that point in the CDF (above). library(ggplot2) y &lt;- data.frame(x=0:10, y=dbinom(0:10, 10, 0.5)) ggplot(y, aes(x = x, y = y))+ geom_point()+ theme_bw()+ scale_x_continuous(breaks = 0:10, labels=0:10)+ labs(x = &quot;observed #&quot;, y = &quot;Probability Mass&quot;, title = &quot;Probability mass function of a discrete random variable&quot;) 8.1.1.5 Continuous Case The vigilant reader might have noticed that while the definition of a random variable allows for the function to map to the real numbers the coin and dice examples only uses mapping to the natural numbers. Just as with discrete random variables we can define continuous random variables by their cumulative distribution function. As you might have guessed the cumulative distribution function of a continuous random variable is continuous, i.e. there are no jumps. library(ggplot2) cont &lt;- data.frame(x=rnorm(2e5, 50, 100)) ggplot(cont, aes(x=x)) + stat_ecdf() + labs(y = &quot;Cumulative Probability&quot;, x = &quot;Observed value&quot;, title = &quot;Cumulative distribution function of a continuous random variable&quot;) + theme_bw() One example for a continuous random variable is the average profit of a store per week. Let’s think of the possible values: Profit could be negative if, for example, the payment to employees exceeds the contribution margin accrued from the products sold. Of course it can also be positive and technically it is not restricted to any range of values (e.g. it could exceed a billion, be below negative 10,000 or anywhere in between). Below you can see some (simulated) profit data. Observe that the CDF looks continuous. The red overlay is the CDF of the normal distribution (see chapter on probability distributions) which was used for the simulation. The final plot is a histogram of the data with the normal density (again in red). It shows that profits around 500 are more likely (higher bars) and the further away from 500 we get the less likely it is that a certain profit will be observed in a given week. Recall the definition of the probability density function shows the probability of a given outcome. 8.1.1.6 Definitions Sample Space: The set of all possible outcomes of a particular experiment is called the sample space of the experiment (Casella &amp; Berger, 2002, p. 1). Denoted \\(S\\). Random Variable: A function from a sample space \\(\\left(S\\right)\\) into the real numbers (Casella &amp; Berger, 2002, p. 27). Denoted \\(X\\). Cumulative distribution function: A function that defines the probability that a random variable \\(\\left(X\\right)\\) is less than or equal to an outcome (\\(x\\)) for all possible outcomes (Casella &amp; Berger, 2002, p. 29). Denoted \\[ F_X(x) = P_X(X \\leq x), \\text{ for all } x \\] Probability mass/density function: A function that defines the probability that a random variable \\(\\left(X\\right)\\) is equal to an outcome (\\(x\\)) for all possible outcomes. Denoted \\[ f_X(x)=P(X = x), \\text{ for all } x \\] Go to: Tossing Coins Sum of two dice Discrete Random Variables Continuous case 8.1.2 Probability Distributions This chapter is primarily based on: Casella, G., &amp; Berger, R. L. (2002). Statistical inference (Vol. 2). Pacific Grove, CA: Duxbury (chapters 2&amp;3). 8.1.2.1 Introduction In the previous chapter we talked about probability density/mass functions (PDFs/PMFs) and cumulative distribution functions (CDFs). We also discussed plots of those functions. A natural question to ask is “where do these distributions come from?”. It turns out that many random variables follow well known distributions, the properties of which have been studied extensively. Furthermore, many observations in the real world (e.g. height data) can also be approximated with theoretical distributions. Let’s consider our coin toss example. We did not actually toss thousands of coins to come up with their probability distribution. We modeled the population of coin tosses using their theoretical distribution (the binomial distribution). We say that a random variable \\(X\\) follows or has some distribution. Distributions have parameters that influence the shape of the distribution function and if we do not explicitly specify the parameters we usually speak of a family of distributions. If \\(X\\) follows the distribution \\(D\\) and \\(a,\\ b\\) are its parameters, we write: \\[ X \\sim D(a, b) \\] Two important properties of a distribution are the expected value and the variance. We usually want to know what outcome we expect on average given a distribution. For this, we can use the concept of an expected value, denoted \\(\\mathbb{E}[X]\\). On the other hand, the variance \\(\\left(Var(X)\\right)\\) gives us a measure of spread around the expected value. If the variance is high, values far away from the expected value are more likely. Similarly, if the variance is low, values far away from the mean are less likely. These concepts may seem somewhat abstract, but will become clear after a few examples. We will now introduce common families of distributions, starting again with discrete examples and then moving on to the continuous case. 8.1.2.2 Discrete Distributions For discrete distributions the expected value is defined as the sum of all possible values weighted by their respective probability. Intuitively, values that are very unlikely get less weight and those that are very likely get more weight. This can be written as \\[ \\mathbb{E}[X] = \\sum_{x} x f_{X}(x) = \\sum_x x P(X = x) \\] The variance is defined as \\[ Var(X) = \\mathbb{E}\\left[\\left(X - \\mathbb{E}[X] \\right)^2 \\right] = \\mathbb{E}[X^{2}] - ( \\mathbb{E}[X])^{2} \\] This is the expected squared deviation from the expected value. Taking the squared deviation always yields a positive value. Additionally, larger deviations are emphasized. This is visualized in the plot below, which shows the transformation from the deviation from the expected value to the squared deviation from the expected value. Some observations: The tosses that do not deviate from the mean and those that only deviate by 1 stay the same when squared. Those that are \\(-1\\) become \\(+1\\) and all others become positive and increase compared to their absolute value. 8.1.2.2.1 Binomial Distribution Our first example of a discrete distribution has to do with coin tosses again. It turns out that the random variable “number of heads observed” follows a very common distribution, the binomial distribution. This can be written as follows: \\(X\\) being the number of heads observed, \\[ X \\sim binomial(n, p) \\] where \\(n\\) is the number of coins and \\(p\\) is the probability of observing heads. Here \\(n,\\ p\\) are the parameters of the binomial distribution. The binomial distribution can be used whenever you conduct an experiment composed of multiple trials where there are two or more possible outcomes, one of which is seen as “success”. The idea is based on the concept of Bernoulli trials, which are basically a binomial distribution with \\(n=1\\). A binomial distribution can also be used for dice, if we are interested in the number of dice that show a particular value, say \\(1\\). Throw any number of dice, say \\(5\\). For each die check if it shows \\(1\\). If yes add 1, if no, do not add anything. The random variable is the final number and follows a binomial distribution with \\(p = \\frac{1}{6},\\ n = 5\\). So, given the parameters \\(p,\\ n\\) of the binomial distribution what are the expected value and the variance? Let’s start with the coin toss with a fair coin: Let \\(p = 0.5,\\ n = 1\\) and \\(X_{0}\\) is again the number of heads observed. We sum over all possibilities and weigh by the probability: \\[ 0.5 * 1 + 0.5 * 0 = 0.5 = \\mathbb{E}[X_{0}] \\] What happens if we change the probability of observing heads to \\(0.8\\)? Then the random variable \\(X_1\\) has expectation \\[ 0.8 * 1 + 0.2 * 0 = 0.8 = \\mathbb{E}[X_{1}] \\] What happens if we change the number of coins to \\(2\\) and keep \\(p = 0.8\\)? Then the random variable \\(X_2\\) has expectation \\[ \\underbrace{0.8 * 1 + 0.2 * 0}_{\\text{first coin}} + \\underbrace{0.8 * 1 + 0.2 * 0}_{\\text{second coin}} = 2 * 0.8 = 1.6 = \\mathbb{E}[X_{2}] \\] In general you can just sum up the probability of “success” of all the coins tossed. If \\(X\\sim binomial(n,\\ p)\\) then \\[ \\mathbb{E}[X] = n * p \\] for any appropriate \\(p\\) and \\(n\\). The variance is the expected squared deviation from the expected value. Let’s look at a single toss of a fair coin again (\\(p = 0.5,\\ n = 1\\)). We already know the expected value is \\(\\mathbb{E}[X_0] = 0.5\\). When we toss the coin we could get heads such that \\(x = 1\\) with probability \\(p = 0.5\\) or we could get tails such that \\(x = 0\\) with probability \\(1-p = 0.5\\). In either case we deviate from the expected value by \\(0.5\\). Now we use the definition of the expectation as the weighted sum and the fact that we are interested in the squared deviation \\[ Var(X_0) = 0.5 * (0.5^2) + 0.5 * (0.5^2) = 2 * 0.5 * (0.5^2) = 0.5 - 0.5^2 = 0.25 \\] What happens if we change the probability of observing heads to \\(0.8\\)? Now the expected value is \\(\\mathbb{E}[X_{1}] = 0.8\\) and we deviate from it by \\(0.2\\) if we get heads and by \\(0.8\\) if we get tails. We get \\[ Var(X_1) = \\underbrace{0.8}_{p(h)} * \\underbrace{(0.2^2)}_{deviation} + 0.2 * (0.8^2) = 0.8 - 0.8^2 = 0.16 \\] Generally, for any \\(n\\) and \\(p\\), the variance of the binomial distribution is given by \\[ Var(X_i) = n * (p-p^2) \\] or, equivalently: \\[ n * (p - p^2) = np - np^2 = np * (1-p) = Var(X_i) \\] The derivation of this equation can be found in the Appendix. You can work with the binomial distribution in R using the binom family of functions. In R, a distribution usually has four different functions associated with it, differentiated by the letter it begins with. The four letters these functions start with are r, q, p and d. rbinom(): Returns random draws from the binomial distribution with specified \\(p\\) and \\(n\\) values. pbinom(): Returns the cumulative probability of a value, i.e. how likely is the specified number or less, given \\(n\\) and \\(p\\). qbinom(): Returns the quantile (See Quantile Function) of a specified probability value. This can be understood as the inverse of the pbinom() function. dbinom(): Returns the value of the probability mass function, evaluated at the specified value (in case of a continuous distribution, it evaluates the probability density function). 8.1.2.2.2 Discrete Uniform Distribution The discrete uniform distribution assigns the same probability to all possible values. Below you can find the PMF and CDF of a uniform distribution that starts at one and goes to ten. To calculate the expected value of this distribution let’s first look at how to easily sum the numbers from \\(1\\) to some arbitrary \\(N\\). That is \\(1 + 2 + 3 + \\dots + N =\\) ?. Let \\(S = 1 + 2 + 3 + \\dots + N = \\sum_{i = 1}^N i\\). Then \\[\\begin{align*} S &amp;= 1 + 2 + 3 + \\dots + (N-2) + (N-1) + N \\\\ \\text{This can be rearranged to:} \\\\ S &amp;= N + (N-1) + (N-2) + \\dots + 3 + 2 + 1 \\\\ \\text{Summing the two yields:} \\\\ 2 * S &amp;= (1 + N) + (2 + N - 1) + (3 + N - 2) + \\dots + (N -2 + 3) + (N - 1 + 2) + (N + 1)\\\\ &amp;= (1 + N) + (1+N) + (1+N) + \\dots + (1+N) + (1+N) + (1+N) \\\\ &amp;= N * (1 + N) = 2 * S \\\\ \\text{It follows that:}\\\\ S&amp;= \\frac{N * (1 + N)}{2} \\end{align*}\\] The weight given to each possible outcome must be equal and is thus \\(p = \\frac{1}{N}\\). Recall that the expected value is the weighted sum of all possible outcomes. Thus if \\(X \\sim discrete\\ uniform(N)\\) \\[ \\mathbb{E}[X] = \\sum_{i = 1}^N p * i = \\sum_{i = 1}^N \\frac{1}{N}* i = \\frac{1}{N} \\sum_{i = 1}^N i = \\frac{1}{N} * S = \\frac{1}{N} * \\frac{N * (1 + N)}{2} = \\frac{(1 + N)}{2} \\] Figuring out the variance is a bit more involved. Since we already know \\(\\mathbb{E}[X]\\) we still need \\(\\mathbb{E}[X^{2}]\\). Again we apply our equal weight to all the elements and get \\[ \\mathbb{E}[X^{2}] = \\sum_{x = 1}^n x^2 * \\frac{1}{N} = \\frac{1}{N} \\sum_{x = 1}^N x^2 \\] Therefore we need to find out what \\(1 + 4 + 9 + 16 + \\dots + N^2\\) is equal to. Luckily, there exists a formula for that: \\[ \\sum_{x=1}^N x^2 = \\frac{N * (N + 1) * (2*N + 1)}{6} \\] Thus, \\[ \\mathbb{E}[X^{2}] = \\frac{(N + 1) * (2*N + 1)}{6} \\] and \\[ Var(X) = \\mathbb{E}[X^2] - \\mathbb{E}[X]^2 = \\frac{(N + 1) * (2*N + 1)}{6} - \\left(\\frac{(1 + N)}{2}\\right)^{2} = \\frac{(N+1) * (N-1)}{12} \\] Note that these derivations are only valid for a uniform distribution that starts at one. However the generalization to a distribution with an arbitrary starting point is fairly straightforward. 8.1.2.3 Continuous Distributions As mentioned in the last chapter, a distribution is continuous if the cumulative distribution function is a continuous function (no steps!). As a consequence we cannot simply sum up values to get an expected value or a variance. We are now dealing with real numbers and thus there are infinitely many values between any two arbitrary numbers that are not equal. Therefore, instead of the sum we have to evaluate the integral of all the possible values weighted by the probability density function (the continuous equivalent to the probability mass function). \\[ \\mathbb{E}[X] = \\int_{-∞}^{∞} x f_{X}(x) dx \\] where \\(f_X(x)\\) is the density of the random variable \\(X\\) evaluated at some point \\(x\\) and the integral over \\(x\\) (“\\(dx\\)”) has the same purpose as the sum over \\(x\\) before. 8.1.2.3.1 Uniform Distribution To illustrate the concept of the integral the continuous uniform distribution provides a simple example. As in the discrete case it assigns equal weight to each equally sized interval in the area on which the variable is defined (\\([a, b]\\)). Why each interval and not each value? Since there are infinitely many values between \\(a\\) and \\(b\\) (again due to real numbers) each individual value cannot be assigned a probability small enough for all of the probabilities to sum to \\(1\\) (which is a basic requirement of a probability). Thus we can only assign a probability to an interval, e.g. \\([0, 0.001]\\), of which only finitely many exist between \\(a\\) and \\(b\\), e.g. \\(a = -2\\) and \\(b = 1\\). In this example there exist \\(3,000\\) intervals of values \\([x, x + 0.001]\\). Since we are dealing with intervals the probability density can be thought of as the area under the PDF for a given interval or the sum of the areas of very small intervals within the chosen interval. The PDF is defined as \\[ f_X(x) = \\frac{1}{b-a} \\text{ if } x \\in [a, b], \\ 0 \\text{ otherwise} \\] That is, the weight \\(\\frac{1}{b-a}\\) is assign to values in the interval of interest and all other values have weight \\(0\\). As already mentioned all values between \\(a\\) and \\(b\\) have to be considered. Thus, in order to calculate the expected value and the variance we have to integrate over \\(x\\). \\[ \\mathbb{E}[X] = ∫_a^b x * \\frac{1}{b-a} dx = \\frac{b+a}{2} \\] If you plug in \\(a = 1\\) in the formula above you can see the relation to the discrete uniform distribution and the similar role of integral and summation. Notice also how the expectation operator “translates” to the integral. For the expectation of \\(X\\) we integrate over all \\(x\\), the possible realizations of \\(X\\), weighted by the PDF of \\(X\\). Now, in order to get the variance we want to calculate the expected squared deviation from the expected value. \\[\\begin{align*} Var(X) &amp;= \\mathbb{E}\\left[(X - \\mathbb{E}[X])^{2} \\right] = \\mathbb{E}\\left[\\left(X - \\frac{b+a}{2}\\right)^2\\right] \\\\ &amp;= ∫_a^b \\left(x - \\frac{b+a}{2}\\right)^2 * \\frac{1}{b-a} dx = \\frac{(b-a)^2}{12} \\end{align*}\\] Clearly the Uniform distribution can be used whenever we want to model a population in which all possible outcomes are equally likely. 8.1.2.3.2 Normal distribution The normal distribution is probably the most widely known one. Its PDF is the famous bell curve. It has two parameters \\(\\mu\\), and \\(\\sigma^2\\). \\(\\mu\\) is the mean and \\(\\sigma^2\\) the variance of the distribution. In the case of \\(\\mu = 0,\\ \\sigma^2 = 1\\) it is called the standard normal distribution. The Normal distribution has a few nice properties. It is symmetric around the mean which is nice whenever we want to express the believe that values are less likely the further we get away from the mean but we do not care in which direction. In addition, it can be used to approximate many other distributions including the Binomial distribution under certain conditions (see Central Limit Theorem). The normal distribution can be standardized, i.e. given any random normal variable, \\(X\\sim N(\\mu, \\sigma^2)\\), we can get a standard normal variable \\(Y \\sim N(0, 1)\\) where \\(Y = \\frac{X - \\mu}{\\sigma}\\). This means that we can perform calculations using the standard normal distribution and then recover the results for any normal distribution since for a standard normal \\(Y\\sim N(0,1)\\) we can get to any \\(X \\sim N(\\mu, \\sigma^{2})\\) by defining \\(X = \\mu + \\sigma * Y\\) by just rearranging the formula above. In the application below you can see the PDF and CDF of the normal distribution and set a mean and a standard deviation. Try to get an intuition about why this simple transformation works. First change only the mean and observe that the shape of the PDF stays the same and its location is shifted. Starting from a normal distribution with \\(μ = 0\\) and setting \\(\\mu = 4\\) is equivalent to adding \\(4\\) to each value (see table of percentiles). Similarly changing the standard deviation from \\(σ = 1\\) to \\(σ = 2,\\ 3,\\ 4, \\dots\\) is equivalent to multiplying each value with \\(2,\\ 3,\\ 4, \\dots\\). The normal PDF is defined as \\[ f(x | μ, σ) = \\frac{1}{\\sqrt{2πσ^{2}}} e^{-\\frac{1}{2}\\frac{(x-\\mu)^2}{σ^2}} \\] The first part \\(\\left(\\frac{1}{\\sqrt{2\\piσ^2}}\\right)\\) scales the density down at each point as the standard deviation is increased because \\(\\sigma\\) is in the denominator. When you increase the standard deviation in the application above you will see that the density gets lower on the whole range. Intuitively the total mass of \\(1\\) needs to be distributed over more values and is thus less in each region. The second part \\(\\left(e^{-\\frac{1}{2}\\frac{(x-μ)^2}{\\sigma^2}}\\right)\\) re-scales regions based on how far they are away from the mean due to the \\((x-\\mu)^2\\) part. Notice that values further away from the mean are re-scaled more due to this. The negative sign in the exponent means that the scaling is downward. \\(\\sigma^2\\) in the denominator tells us that this scaling is reduced for higher \\(\\sigma^2\\) and “stronger” for lower \\(\\sigma^2\\). In other words: as \\(\\sigma\\) is increased regions further away from the mean get more probability density. If the standard deviation is set to \\(1\\) for example there is almost no mass for values that deviate from the mean by more than \\(2\\). However, if we set \\(\\sigma = 10\\) the 75th percentile is at \\(6.75\\). That is, 25% of values lie above that value. Equivalently, if we take the integral from \\(6.75\\) to \\(∞\\) we will get \\(0.25\\). Remember that the integral is just the surface area under the curve in that interval. This is also equivalent to saying that if we draw from a normal distribution with \\(\\mu=0,\\ \\sigma = 10\\) the probability of getting at least \\(6.75\\) is 25%. The CDF is defined as \\[ P(X \\leq x) = \\frac{1}{\\sqrt{2 \\pi σ^2}} \\int_{-∞}^x e^{-\\frac{1}{2}\\frac{-(t-μ)^2}{σ^2}} dt \\] Notice that this is just the integral of the density up to a point \\(x\\). When using the Normal distribution in R one has to specify the standard deviation \\(\\sigma\\) rather than the variance \\(\\sigma^2\\). Of course sometimes it is easier to pass sqrt(variance) instead of typing in the standard deviation. For example if \\(Var(X) = 2\\) then \\(SD(X) = \\sqrt{2} = 1.41421356\\dots\\) and it is easier to call rnorm(10, 0, sqrt(2)) to generate 10 random numbers from a Normal distribution with \\(\\mu = 0, \\sigma^2 = 2\\). 8.1.2.3.3 \\(\\chi^2\\) Distribution The \\(\\chi^2\\) (“Chi-Squared”) distribution has only one parameter, its degrees of freedom. The exact meaning of degrees of freedom will be discussed later when we are talking about hypothesis testing. Roughly, they give the number of independent points of information that can be used to estimate a statistic. Naming the parameter of the \\(\\chi^2\\) distribution degrees of freedom reflects its importance for hypothesis testing. That is the case since many models assume the data to be normally distributed and the \\(\\chi^2\\) distribution is closely related to the normal distribution. Explicitly if \\(X \\sim N(0, 1)\\) then \\(X^2 \\sim \\chi^2(1)\\). That is, if we have one random variable with standard normal distribution and square it, we get a \\(\\chi^2\\) random variable with \\(1\\) degree of freedom. How to exactly count the variables that go into a specific statistic will be discussed at a later point. If multiple squared independent standard normal variables are summed up the degrees of freedom increase accordingly. \\[ Q = ∑_{i = 1}^k X_i^2,\\ X_{i} \\sim N(0,1) \\Rightarrow Q \\sim \\chi^2(k) \\] That is, if we square \\(k\\) normal variables and sum them up the result is a \\(\\chi^2\\) variable with \\(k\\) degrees of freedom Calculating the expected value, using the properties of the standard normal distribution, is simple. Let \\(X \\sim N(0,1)\\). Then \\(Var(X) = \\mathbb{E}[X^{2}] - \\mathbb{E}[X]^2 = \\sigma^2 = 1\\). Also, \\(\\mathbb{E}[X] = \\mu = 0\\). Therefore, \\(\\mathbb{E}[X^{2}] = 1\\). Thus, the expected value of one squared standard normal variable is \\(1\\). If we sum \\(k\\) independent normal variables we get \\[ \\mathbb{E}[Q] = \\sum_{i = 1}^k \\mathbb{E}[X^{2}] = \\sum_{i = 1 }^k 1 = k \\] The derivation of the variance is a bit more involved because it involves calculating \\[ \\mathbb{E}[Q^{2}] = \\mathbb{E}\\left[\\left(X^{2}\\right)^2\\right] = \\mathbb{E}[X^{4}] \\] where \\(Q\\sim \\chi^2(1)\\) and \\(X \\sim N(0,1)\\). However, above we claimed that the Normal distribution has only two parameters, \\(\\mu,\\text{ and } \\sigma^2\\), for which we only need \\(\\mathbb{E}[X]\\) and \\(\\mathbb{E}[X^{2}]\\) to fully describe the Normal distribution. These are called the first and second moments of the distribution. Equivalently, \\(\\mathbb{E}[X^{4}]\\) is the \\(4^{th}\\) moment. We can express the \\(4^th\\) moment in terms of the second moment for any variable that follows a Normal distribution. \\(\\mathbb{E}[X^{4}] = 3 * \\sigma^2\\) for any Normal variable and thus \\(\\mathbb{E}[X^{4}] = 3\\) for Standard Normal variables. Therefore, \\(\\mathbb{E}[Q^{2}] = 3 - \\mathbb{E}[Q]^2 = 2\\) for \\(Q \\sim \\chi^2(1)\\). In general \\(Var(Q) = 2k\\) for \\(Q \\sim \\chi^2(k)\\) due to the variance sum law which states that the variance of a sum of independent variables is equal to the sum of the variances. Notice that this does not hold if the variables are not independent, which is why the independence of the Normal variables that go into the \\(\\chi^2\\) distribution has been emphasized. 8.1.2.4 t-Distribution Another important distribution for hypothesis testing is the t-distribution, also called Student’s t-distribution. It is the distribution of the location of the mean of a sample from the normal distribution relative to the “true” mean (\\(\\mu\\)). Like the \\(\\chi^2\\) distribution it also has only one parameter called the degrees of freedom. However, in this case the degrees of freedom are the number of draws from the normal distribution minus 1. They are denoted by the Greek letter nu (\\(\\nu\\)). We take \\(n\\) draws from a Normal distribution with mean \\(\\mu\\) and standard deviation \\(\\sigma^2\\) and let \\(\\bar X = \\frac{1}{n} \\sum_{i = 1}^n x_i\\) the sample mean and \\(S^{2} = \\frac{1}{n-1}\\sum_{i = 1}^n (x_i - \\bar X)^2\\) the sample variance. Then \\[ \\frac{\\bar X - \\mu}{S/ \\sqrt{n}} \\] has a t-distribution with \\(\\nu = n-1\\) degrees of freedom. Why \\(n-1\\) and not \\(n\\) as in the \\(\\chi^2\\) distribution? Recall that when constructing a \\(\\chi^2(k)\\) variable we sum up \\(k\\) independent standard normally distributed variables but we have no intermediate calculations with these variables. In the case of the t-Distribution we “lose” a degree of freedom due to the intermediary calculations. We can notice this by multiplying and dividing the formula above by \\(\\sigma\\), the “true” variance of the \\(x_i\\). \\[\\begin{align*} &amp; \\frac{\\bar X - \\mu}{S/ \\sqrt{n}} \\\\ =&amp; \\frac{\\bar X - \\mu}{S/ \\sqrt{n}} \\frac{\\frac{1}{\\sigma/\\sqrt{n}}}{\\frac{1}{\\sigma/\\sqrt{n}}} = \\frac{(\\bar X - \\mu)/(\\sigma/\\sqrt{n})}{\\frac{(S/\\sqrt{n})}{(\\sigma/\\sqrt{n})}}\\\\ =&amp; \\frac{(\\bar X - \\mu)/(\\sigma/\\sqrt{n})}{\\frac{S}{\\sigma}}\\\\ =&amp; \\frac{(\\bar X - \\mu)/(\\sigma/\\sqrt{n})}{\\sqrt{S^2/\\sigma^2}} \\end{align*}\\] Now recall the definition of \\(S^2 = \\frac{1}{n-1} \\sum_{i=1}^n (x_i - \\bar X)^2\\) which is the sum of \\(n-1\\) independent normally distributed variables, divided by \\(n-1\\). Only \\(n-1\\) are independent since given an mean computed from \\(n\\) variables only \\(n-1\\) can be chosen arbitrarily. Let \\(\\bar X(n) = \\frac{1}{n} \\sum_{i=1}^n x_i\\) Then \\[ x_n = \\bar X(n) - \\sum_{i = 1}^{n-1} x_i \\] and thus \\(x_n\\) is not independent. We already know the distribution of \\(n-1\\) squared standard normally distributed variables. We introduced the \\(\\sigma\\) term in order to normalize the variable. Therefore the denominator is \\[ \\sqrt{\\frac{\\chi^2_{n-1}}{(n-1)}} \\] Notice that as the degrees of freedom approach infinity the t-Distribution becomes the Standard Normal Distribution. 8.1.2.5 F-Distribution The F-Distribution is another derived distribution which is important for hypothesis testing. It can be used to compare the variability (i.e. variance) of two populations given that they are normally distributed and independent. Given samples from these two populations the F-Distribution is the distribution of \\[ \\frac{S^2_1 / S^2_2}{\\sigma^2_1/\\sigma^2_2} = \\frac{S^2_1/\\sigma^{2}_1}{S^2_2/\\sigma^2_2} \\] As shown above both the numerator and the denominator are \\(\\chi^2(n-1)\\) divided by the degrees of freedom \\[ F_{n-1, m-1} = \\frac{\\chi^2_{n-1}/(n-1)}{\\chi^2_{m-1}/(m-1)} \\] 8.1.3 Appendix 8.1.3.1 Derivation of the varaince of the binomial distribution Notice that the sum follows this pattern for \\(n = 1\\) and any appropriate \\(p\\): \\[ Var(X_i) = p * (1-p)^2 + (1-p) * p^2 \\] If we expand the squared term and simplify: \\[\\begin{align*} Var(X_i) &amp;= p * (1 - 2*p + p^2) + p^2 - p^3 \\\\ &amp;= p - 2*p^2 + p^3 + p^2 - p^3 \\\\ &amp;= p - p^2 \\end{align*}\\] What happens if we change the number of coins to \\(2\\) and keep \\(p=0.8\\)? \\[ Var(X_2) = 0.8 * 0.2^2 + 0.2 * 0.8^2 + 0.8 * 0.2^2 + 0.2 * 0.8^2 = 2 * (0.8 * 0.2^2 + 0.2 * 0.8^2) = 2 * (0.8 - 0.8^2) = 0.32 \\] Since increasing \\(n\\) further simply adds more of the same terms we can easily adapt the general formula above for any appropriate \\(n\\) and \\(p\\): \\[ Var(X_i) = n * (p-p^2) \\] Equivalently this formula can be written as: \\[ n * (p - p^2) = np - np^2 = np * (1-p) = Var(X_i) \\] 8.2 Regression 8.2.1 Linear regression This chapter contains the Appendix for the chapter on regression analysis. In order to understand the coefficients in the multiple regression we can derive them separately. We have already seen that in the univariate case: \\[ \\hat{\\beta_1}=\\frac{COV_{XY}}{s_x^2} \\] We have also seen that we can isolate the partial effect of a single variable in a multiple regression. Specifically, we can calculate the coefficient of the i-th variable as \\[ \\hat{\\beta_{i}} = {COV(\\tilde{Y_{i}}, \\tilde{X_{i}}) \\over V(\\tilde{X_i})} \\] where \\(\\tilde{Y_{i}}\\) is the residual from the regression of \\(Y\\) on all variables except for the i-th and \\(\\tilde{X_i}\\) is the residual from the regression of \\(X_i\\) on all other variables. Let’s illustrate this with an example. x1 &lt;- rnorm(100, 5, 15) x2 &lt;- rnorm(100, 20, 10) + 3*x1 y &lt;- 50 + 2*x1 + 7*x2 + rnorm(100, 0, 5) We have added some randomness through the rnorm command so the coefficients are not exactly as we set them but close. Clearly x1 and x2 have partial influence on y. mod &lt;- lm(y~x1+x2) summary(mod) ## ## Call: ## lm(formula = y ~ x1 + x2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -12.9482 -2.4227 0.3595 3.1597 11.5191 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 49.88523 1.08974 45.78 &lt;0.0000000000000002 *** ## x1 1.86227 0.13939 13.36 &lt;0.0000000000000002 *** ## x2 7.03624 0.04617 152.41 &lt;0.0000000000000002 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.706 on 97 degrees of freedom ## Multiple R-squared: 0.9998, Adjusted R-squared: 0.9998 ## F-statistic: 3.036e+05 on 2 and 97 DF, p-value: &lt; 0.00000000000000022 To see the partial effect of x1 we run regressions for x1 on x2, as well as y on x2 and obtain the residuals \\(\\tilde{x1}\\) and \\(\\tilde{x2}\\). Notice that x1 is highly correlated with x2. regX1 &lt;- lm(x1~x2) summary(regX1) ## ## Call: ## lm(formula = x1 ~ x2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -9.5397 -2.7946 0.3462 2.1645 7.9205 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -6.620773 0.419971 -15.77 &lt;0.0000000000000002 *** ## x2 0.323652 0.007106 45.55 &lt;0.0000000000000002 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.41 on 98 degrees of freedom ## Multiple R-squared: 0.9549, Adjusted R-squared: 0.9544 ## F-statistic: 2074 on 1 and 98 DF, p-value: &lt; 0.00000000000000022 tildeX1 &lt;- residuals(regX1) regY1 &lt;- lm(y~x2) tildeY1 &lt;- residuals(regY1) We can use the residuals to create the partial plots as seen above. # This is the same as avPlot ggplot(data.frame(y = tildeY1, x = tildeX1), aes(x = x, y = y)) + geom_point(shape = 1) + geom_smooth(method = &#39;lm&#39;, se = FALSE, color = &quot;red&quot;) + labs(y = &quot;y | others&quot;, x = &quot;x1 | others&quot;) + theme_bw() avPlots(mod) And the regression of the residuals of y on x1 yields the same coefficient as in the original regression (minus the rounding errors). # This is the same coefficient as in the original regression summary(lm(tildeY1~tildeX1 - 1)) # -1 since we want no intercept ## ## Call: ## lm(formula = tildeY1 ~ tildeX1 - 1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -12.9482 -2.4227 0.3595 3.1597 11.5191 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## tildeX1 1.862 0.138 13.5 &lt;0.0000000000000002 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.658 on 99 degrees of freedom ## Multiple R-squared: 0.6479, Adjusted R-squared: 0.6443 ## F-statistic: 182.2 on 1 and 99 DF, p-value: &lt; 0.00000000000000022 print(&quot;Coefficient using the partial covariance:&quot;) ## [1] &quot;Coefficient using the partial covariance:&quot; cov(tildeY1, tildeX1)/var(tildeX1) ## [1] 1.862274 8.2.2 Logistic regression 8.2.2.1 Maximum likelihood estimation For non-linear models we cannot turn to linear algebra to solve for the unknown parameters. However, we can use a statistical method called maximum likelihood estimation (MLE). Given observed data and an assumption about their distribution we can estimate for which parameter values the observed data is most likely to occur. We do this by calculating the joint log likelihood of the observations given a choice of parameters (more on that below). Next, we change our parameter values a little bit and see if the joint log likelihood improved. We keep doing that until we cannot find better parameter values anymore (that is we have reached the maximum likelihood). In the following example we have a histogram of a data set which we assume to be normally distributed. You can now try to find the best parameter values by changing the mean and the variance. Recall that the normal distribution is fully described by just those two parameters. In each step the log likelihood is automatically calculated. The maximum likelihood so far is shown as the high score and the line in the lower graph shows the development of the log likelihood for the past couple of tries. 8.2.2.2 Estimation of the parameters \\(\\beta_i\\) Let’s return to the model to figure out how the parameters \\(\\beta_i\\) are estimated. In the data we observe the \\(y_i\\), the outcome variable that is either \\(0\\) or \\(1\\), and the \\(x_{i,j}\\), the predictors for each observation. In addition, by using the logit model we have assumed the following functional form. \\[ P(y_i = 1) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 * x_{1,i} + \\beta_2 * x_{2,i} + ... +\\beta_m * x_{m,i})}} \\] So far we have looked at estimating the \\(\\beta_i\\) with R but have not discussed how that works. In contrast to linear models (e.g. OLS) we cannot rely on linear algebra to solve for \\(\\beta_i\\) since the function is now non-linear. Therefore, we turn to a methodology called maximum likelihood estimation. Basically, we try out different \\(\\beta_i\\) and choose the combination that best its the observed data. In other words: choose the combination of \\(\\beta_i\\) that maximize the likelihood of observing the given data set. For each individual we have information about the binary outcome and the predictors. For those whose outcome is \\(1\\) we want to choose the \\(\\beta_i\\) such that our predicted probability of the outcome (\\(P(y_i = 1)\\)) is as close to \\(1\\) as possible. At the same time, for those whose outcome is \\(0\\) we would like the prediction (\\(P(y_i = 1)\\)) to be as close to \\(0\\) as possible. Therefore, we “split” the sample into two groups (outcome \\(1\\) and outcome \\(0\\)). For the first group each individual has the probability function shown above which we want to maximize for this group. For the other group we would need to minimize the joint probability since we want our prediction to be as close to \\(0\\) as possible. Alternatively we can easily transform the probability to \\(P(y_i = 0)\\) which we can then maximize. Since probabilities add up to \\(1\\) and we only have two possible outcomes \\(P(y_i = 0) = 1 - P(y_i = 1)\\). This is convenient because now we can calculate the joint likelihood of all observations and maximize a single function. We assume that the observations are independent from each other and thus the joint likelihood is just the product of the individual probabilities. Thus for the group of \\(n_1\\) individuals with outcome \\(1\\) we have the joint likelihood \\[ \\prod_{i=1}^{n_1} {1 \\over 1 + e^{-(\\beta_0 + \\beta_1 * x_{1,i} + \\beta_2 * x_{2,i} + ... +\\beta_m * x_{m,i})}} \\] For the second group of \\(n_2\\) individuals with outcome \\(0\\) we get the joint likelihood \\[ \\prod_{j=1}^{n_{2}} 1 - {1 \\over 1 + e^{-(\\beta_0 + \\beta_1 * x_{1,i} + \\beta_2 * x_{2,i} + ... +\\beta_m * x_{m,i})}} \\] In order to find the optimal \\(\\beta_i\\) we need to combine to two groups into one joint likelihood function for all observations. We can once again do that by multiplying them with each other. However, now we need to add an indicator in order to determine in which group the observation is. \\[ \\prod_{k = 1}^{n_1 + n_2} \\left({1 \\over 1 + e^{-(\\beta_0 + \\beta_1 * x_{1,i} + \\beta_2 * x_{2,i} + ... +\\beta_m * x_{m,i})}}\\right)^{y_i} \\times \\left( 1 - {1 \\over 1 + e^{-(\\beta_0 + \\beta_1 * x_{1,i} + \\beta_2 * x_{2,i} + ... +\\beta_m * x_{m,i})}}\\right)^{1-y_i} \\] The indicators \\((\\cdot)^{y_i}\\) and \\((\\cdot)^{1-y_i}\\) select the appropriate likelihood. To illustrate this consider an individual with outcome \\(y_j = 1\\) \\[ \\begin{align*} &amp;\\left({1 \\over 1 + e^{-(\\beta_0 + \\beta_1 * x_{1,j} + \\beta_2 * x_{2,j} + ... +\\beta_m * x_{m,j})}}\\right)^{y_j} \\times \\left( 1 - {1 \\over 1 + e^{-(\\beta_0 + \\beta_1 * x_{1,j} + \\beta_2 * x_{2,j} + ... +\\beta_m * x_{m,j})}}\\right)^{1-y_j}\\\\ =&amp;\\left({1 \\over 1 + e^{-(\\beta_0 + \\beta_1 * x_{1,j} + \\beta_2 * x_{2,j} + ... +\\beta_m * x_{m,j})}}\\right)^{1} \\times \\left( 1 - {1 \\over 1 + e^{-(\\beta_0 + \\beta_1 * x_{1,j} + \\beta_2 * x_{2,j} + ... +\\beta_m * x_{m,j})}}\\right)^{0}\\\\ =&amp;{1 \\over 1 + e^{-(\\beta_0 + \\beta_1 * x_{1,j} + \\beta_2 * x_{2,j} + ... +\\beta_m * x_{m,j})}} \\times 1\\\\ =&amp;{1 \\over 1 + e^{-(\\beta_0 + \\beta_1 * x_{1,j} + \\beta_2 * x_{2,j} + ... +\\beta_m * x_{m,j})}} \\end{align*} \\] Equivalently for an individual with outcome \\(y_s = 0\\): \\[ \\begin{align*} &amp;\\left({1 \\over 1 + e^{-(\\beta_0 + \\beta_1 * x_{1,i} + \\beta_2 * x_{2,i} + ... +\\beta_m * x_{m,i})}}\\right)^{y_i} \\times \\left( 1 - {1 \\over 1 + e^{-(\\beta_0 + \\beta_1 * x_{1,i} + \\beta_2 * x_{2,i} + ... +\\beta_m * x_{m,i})}}\\right)^{1-y_i}\\\\ =&amp;\\left({1 \\over 1 + e^{-(\\beta_0 + \\beta_1 * x_{1,j} + \\beta_2 * x_{2,j} + ... +\\beta_m * x_{m,j})}}\\right)^{0} \\times \\left( 1 - {1 \\over 1 + e^{-(\\beta_0 + \\beta_1 * x_{1,j} + \\beta_2 * x_{2,j} + ... +\\beta_m * x_{m,j})}}\\right)^{1}\\\\ =&amp;1\\times \\left( 1 - {1 \\over 1 + e^{-(\\beta_0 + \\beta_1 * x_{1,j} + \\beta_2 * x_{2,j} + ... +\\beta_m * x_{m,j})}}\\right)^{1}\\\\ =&amp; 1 - {1 \\over 1 + e^{-(\\beta_0 + \\beta_1 * x_{1,j} + \\beta_2 * x_{2,j} + ... +\\beta_m * x_{m,j})}} \\end{align*} \\] 8.2.2.3 Example Consider an experiment in which we want to find out whether a person will listen to the full song or skip it. Further, assume that the genre of the song is the only determining factor of whether somebody is likely to skip it or not. Each individual has rated the genre on a scale from 1 (worst) to 10 (best). Our model looks as follows: \\[ P(\\text{skipped}_i) = {1 \\over 1 + e^{-(\\beta_0 + \\beta_1 \\text{rating}_i)}} \\] The probability that individual \\(i\\) will skip a song is the logistic function with \\(X = \\beta_0 + \\beta_1 * \\text{rating}_i\\), where \\(\\text{rating}_i\\) is individual \\(i\\)’s rating of the genre of the song. Since we assume independence of individuals we can write the joint likelihood as \\[ \\prod_{i=1}^{N} \\left({1 \\over 1 + e^{-(\\beta_0 + \\beta_1 \\text{rating}_i)}}\\right)^{y_i} \\times \\left(1- {1 \\over 1 + e^{-(\\beta_0 + \\beta_1 \\text{rating}_i)}}\\right)^{(1-y_i)} \\] Notice that \\(y_i\\) is either equal to one or to zero. So for each individual only one of the two parts is not equal to one (recall that any real number to the power of 0 is equal to 1). The the part left of the plus sign is “looking at” individuals who skipped the song given their rating and the right part is looking at individuals who did not skip the song given their rating. For the former we want the predicted probability of skipping to be as high as possible. For the latter we want the predicted probability of not skipping to be as high as possible and thus write \\(1 - {1 \\over 1+ e^{-(\\beta_0 + \\beta_1 \\text{rating}_i)}}\\). This is convenient since we can now maximize a single function. Another way to simplify the calculation and make it computationally feasible is taking the logarithm. This will ensure that extremely small probabilities can still be processed by the computer (see this illustration). Manually applying this in R would looks as follows. We begin by simulating some data. ratingGenre is a vector of 10000 randomly generated numbers between 1 and 10. pSkip will be a vector of probabilities generated by applying the logistic function to our linear model, with the parameters \\(\\beta_0 = 1\\) and \\(\\beta_1 = -0.3\\). ratingGenre &lt;- sample(1:10, size = 10000, replace = TRUE) linModel &lt;- 1 - 0.3 * ratingGenre pSkip &lt;- 1/(1+exp(-linModel)) Now we have to sample whether a user skipped a song or not, based on their probability of skipping. The resulting vector skipped is composed of 0s and 1s and indicates whether or not a person skipped a song. skipped &lt;- 1:length(pSkip) for(i in 1:length(pSkip)){ skipped[i] &lt;- sample(c(1, 0), size = 1, prob = c(pSkip[i], 1-pSkip[i])) } Our simulated data now looks as follows: Figure 8.1: Percent of songs skipped as a function of the genre rating The visualization shows that an increase in genreRating leads to a decrease in the probability of a song being skipped. Now we want to perform maximum likelihood estimation and see how close we get to the true parameter values. To achieve this, we need a function that, given a value for \\(\\beta_0\\) and \\(\\beta_1\\), gives us the value of the log likelihood. The following code defines such a function. mlEstimate &lt;- function(beta_0, beta_1){ pred &lt;- 1/(1+exp(-(beta_0 + beta_1 * ratingGenre))) loglik &lt;- skipped * log(pred) + (1-skipped) * log(1 - pred) sum(loglik) } The log likelihood function has the following form. Figure 8.2: Plot of the log likelihood function As you can see, the maximum of the log likelihood function lies around -0.3, 1, the true parameter values. Now we need to find an algorithm that finds the combination of \\(\\beta_0\\) and \\(\\beta_1\\) that optimizes this function. There are multiple ways to go about this. The glm() function uses the built-in optimization function optim(). While we could do the same, we will use a slightly different approach to make the process more intuitive. We are going to employ something called grid maximization. Basically, we make a list of all plausible combinations of parameter values and calculate the log likelihood for each combination. Then we simply select the parameter combination that has the highest log likelihood. prob_beta_0 and prob_beta_1 are now vectors that contain 100 plausible values for each parameter. prob_beta_0 &lt;- seq(0.5, 1.5, length.out = 100) prob_beta_1 &lt;- seq(-1, 0, length.out = 100) # Print one vector as an example prob_beta_0 ## [1] 0.5000000 0.5101010 0.5202020 0.5303030 0.5404040 0.5505051 0.5606061 ## [8] 0.5707071 0.5808081 0.5909091 0.6010101 0.6111111 0.6212121 0.6313131 ## [15] 0.6414141 0.6515152 0.6616162 0.6717172 0.6818182 0.6919192 0.7020202 ## [22] 0.7121212 0.7222222 0.7323232 0.7424242 0.7525253 0.7626263 0.7727273 ## [29] 0.7828283 0.7929293 0.8030303 0.8131313 0.8232323 0.8333333 0.8434343 ## [36] 0.8535354 0.8636364 0.8737374 0.8838384 0.8939394 0.9040404 0.9141414 ## [43] 0.9242424 0.9343434 0.9444444 0.9545455 0.9646465 0.9747475 0.9848485 ## [50] 0.9949495 1.0050505 1.0151515 1.0252525 1.0353535 1.0454545 1.0555556 ## [57] 1.0656566 1.0757576 1.0858586 1.0959596 1.1060606 1.1161616 1.1262626 ## [64] 1.1363636 1.1464646 1.1565657 1.1666667 1.1767677 1.1868687 1.1969697 ## [71] 1.2070707 1.2171717 1.2272727 1.2373737 1.2474747 1.2575758 1.2676768 ## [78] 1.2777778 1.2878788 1.2979798 1.3080808 1.3181818 1.3282828 1.3383838 ## [85] 1.3484848 1.3585859 1.3686869 1.3787879 1.3888889 1.3989899 1.4090909 ## [92] 1.4191919 1.4292929 1.4393939 1.4494949 1.4595960 1.4696970 1.4797980 ## [99] 1.4898990 1.5000000 Next we create a data frame that contains all possible combinations of prob_beta_0 and prob_beta_1. The expand.grid() function does exactly that. params &lt;- expand.grid(prob_beta_0, prob_beta_1) # Print df params With the apply() function we can calculate the log likelihood for each value in the data frame. Note that the params data frame now has a now column containing the log likelihood. params$loglik &lt;- apply(params, 1, function(x) mlEstimate(x[1], x[2])) # print df params Next we simply find the highest log likelihood value and the associated parameter values. maxLik &lt;- which.max(params$loglik) params[maxLik, ] As you can see, our method comes pretty close to the true parameter values. For comparison, here is the same model calculated with the glm() function. summary(glm(skipped ~ ratingGenre, family = binomial(link = &#39;logit&#39;))) ## ## Call: ## glm(formula = skipped ~ ratingGenre, family = binomial(link = &quot;logit&quot;)) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.4895 -0.8638 -0.5806 1.0095 2.0606 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 1.009642 0.046579 21.68 &lt;0.0000000000000002 *** ## ratingGenre -0.300527 0.008365 -35.93 &lt;0.0000000000000002 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 13140 on 9999 degrees of freedom ## Residual deviance: 11632 on 9998 degrees of freedom ## AIC: 11636 ## ## Number of Fisher Scoring iterations: 3 8.2.2.3.1 Sum of LN vs Product To illustrate why the natural log (ln) is important for computations with numbers close to zero (such as joint probabilities), we create a vector of fictitious probabilities and multiply them with each other. As we can see, already a small number of values close to 0 will lead to their product being erroneously interpreted as being 0. If we are only interested in comparing magnitude (as in the case of likelihoods) we can safely apply the ln, since it is a monotonically increasing function and will thus not change the ranking. probs &lt;- c(0.000001, 0.000001, 0.000001, 0.000001, 0.000001, 0.000001, 0.000001, 0.000001, 0.000001, 0.000001, 0.000001, 0.000001, 0.000001, 0.000001, 0.000001, 0.000001, 0.000001, 0.000001, 0.000001, 0.000001, 0.000001, 0.000001, 0.000001, 0.000001, 0.000001, 0.000001, 0.000001, 0.000001, 0.000001, 0.000001, 0.000001, 0.000001, 0.000001, 0.000001, 0.000001, 0.000001, 0.000001, 0.000001, 0.000001, 0.000001, 0.000001, 0.000001, 0.000001, 0.000001, 0.000001, 0.000001, 0.000001, 0.000001, 0.000001, 0.000001, 0.000001, 0.000001, 0.000001, 0.000001, 0.000001, 0.000001) prod(probs) ## [1] 0 lProbs &lt;- log(probs) # this is the ln sum(lProbs) ## [1] -773.6686 probs2 &lt;- rbind(probs, probs, probs, probs, probs, probs, probs, probs, probs, probs) # now 10 times as many values lProbs2 &lt;- log(probs2) sum(lProbs2) ## [1] -7736.686 "],
["assignments.html", "9 Assignments 9.1 Assignment 2 (Hypothesis Testing) 9.2 Assignment 3 (Hypothesis Testing 2)", " 9 Assignments 9.1 Assignment 2 (Hypothesis Testing) As a marketing manager at a video streaming service, you are interested in the effect of online advertising on the number of streams that a movie receives. To test the effect of online advertising on streams, you select a representative sample of 200 movies and randomly assign 100 movies to be included in an online advertising campaign. The other half of the sample serves as the control group. You run the experiment for one week and collect data regarding the number of streams for each movie from this period. Overall, the data set includes the following variables: movieID: unique movie ID streams_sd: number of streams in SD-quality streams_hd: number of streams in HD-quality online_advertising: indicator whether a movie was included in the online advertising campaign (0 = no, 1 = yes) Apply appropriate statistical methods to answer the following questions: Compute the 95% confidence interval for the mean number of streams for movies in SD and HD quality and provide an interpretation of the interval Your historical data tells you that the movies in SD and HD quality received 2,600 and 1,700 streams in the previous week, respectively. Please test if the number of streams that the movies received (irrespective of whether they were included in the experiment or not) in the week of the experiment is significantly different from the previous week for SD and HD movies. Is there a significant difference in streams between movies that were included in the online advertising campaign and those that were not included? (Please conduct the test for SD and HD movies and also compute the effect size Cohen’s d) Is there a significant difference in streams between movies in HD and SD quality? (Please also compute the effect size Cohen’s d) Assume that you plan to run an experiment with two groups to test two different advertising strategies. You randomly assign movies to the control and experimental conditions and your goal is to test if there is a significant difference between the groups regarding the number of streams that the movies receive. How many movies would you need to include in each group of your experiment if you assume the effect size to be 0.3 for a significance level of 0.05 and power of 0.8? When answering the questions, please remember to address the following points, where appropriate: Formulate the corresponding hypotheses and choose an appropriate statistical test Provide the reason for your choice and discuss if the assumptions of the test are met Convert the variables to the appropriate type (e.g., factor variables) Create appropriate graphs to explore the data (e.g., boxplot, bar chart, histogram) Provide appropriate descriptive statistics for the variables Report and interpret the test results accurately (including confidence intervals) Finally, don’t forget to report your research conclusion in an appropriate way When you are done with your analysis, click on “Knit to HTML” button above the code editor. This will create a HTML document of your results in the folder where the “assignment.Rmd” file is stored. Open this file in your Internet browser to see if the output is correct. If the output is correct, submit the HTML file via Learn@WU. The file name should be “assignment2_studendID_name.html”. Load and inspect data Let´s load the data first and inspect the contained variables: movie_data &lt;- read.table(&quot;https://raw.githubusercontent.com/IMSMWU/MRDA2018/master/data/assignment2.dat&quot;, sep = &quot;\\t&quot;, header = TRUE) #read in data head(movie_data) str(movie_data) ## &#39;data.frame&#39;: 200 obs. of 4 variables: ## $ movieID : int 1 2 3 4 5 6 7 8 9 10 ... ## $ online_advertising: int 0 0 0 0 0 1 1 1 0 1 ... ## $ streams_sd : int 2365 1752 1351 2495 3883 3933 3849 2770 1146 3033 ... ## $ streams_hd : int 1432 2305 1197 1112 1364 1202 1363 3502 2047 1104 ... Load packages Next, we load the packages that we will be using to answer the questions: library(pastecs) library(ggplot2) library(psych) library(pwr) library(lsr) library(reshape2) Question 1 To compute the confidence intervals for SD and HD streams we will need three things: 1) the mean \\(\\bar x\\), 2) the standard error (\\(s \\over \\sqrt{n}\\)), and 3) the critical value for a t-distribution (\\(t_{crit}\\); we will use a t-distribution, because we are not sure of the variance in the population). #Calculate components of confidence interval formula mean_sd &lt;- mean(movie_data$streams_sd) mean_hd &lt;- mean(movie_data$streams_hd) sd_sd &lt;- sd(movie_data$streams_sd) sd_hd &lt;- sd(movie_data$streams_hd) n &lt;- nrow(movie_data) se_sd &lt;- sd_sd/sqrt(n) se_hd &lt;- sd_hd/sqrt(n) df &lt;- n-1 t_crit &lt;- qt(0.975, df) Now the confidence intervals for streams in SD and HD quality can be computed as: #Interval for SD moviess ci_lower_sd &lt;- mean_sd - t_crit * se_sd ci_upper_sd &lt;- mean_sd + t_crit * se_sd #Interval for HD movies ci_lower_hd &lt;- mean_hd - t_crit * se_hd ci_upper_hd &lt;- mean_hd + t_crit * se_hd Hence, the CI for SD movies is given by: ci_lower_sd ## [1] 2527.979 ci_upper_sd ## [1] 2827.851 \\(CI_{SD} = [2527.97,2827.85]\\) Similarly, the CI for HD movies is given by ci_lower_hd ## [1] 1728.51 ci_upper_hd ## [1] 1940.52 \\(CI_{HD} = [1728.51,1940.52]\\) The intervals can be interpreted as follows: If we would take 100 samples, calculate the mean and confidence interval for each of them, then the true population mean would be included in 95% of these intervals. Question 2 To find out whether our data for SD and HD streams differs significantly from the previous week (2600 for SD; 1700 for HD), we will conduct a one sample t-test. This is appropriate, because 1) our data is on an interval scale, and 2) the sampling distribution can be considered as normally distributed due to the fairly large sample size (n=200; see central limit theorem). Our null hypothesis states that there is no difference between the quantity of SD/HD streams watched in the current week, compared to the previous week. Rejecting the null hypotheses/accepting the alternative hypothesis would mean that there indeed was a difference between the two weeks. So for our SD streams we could formulate our hypothesis as follows: \\[H_0: \\mu_0 = 2600 \\\\ H_1: \\mu_0 \\neq 2600 \\] The same approach can be used for our HD streams: \\[H_0: \\mu_0 = 1700 \\\\ H_1: \\mu_0 \\neq 1700 \\] We can first have a quick look at the descriptive statistics: describe(movie_data$streams_sd) ## vars n mean sd median trimmed mad min max range skew kurtosis ## X1 1 200 2677.91 1075.28 2732 2653.68 1064.51 654 5709 5055 0.2 -0.41 ## se ## X1 76.03 describe(movie_data$streams_hd) ## vars n mean sd median trimmed mad min max range skew kurtosis ## X1 1 200 1834.52 760.23 1668.5 1761.9 578.21 246 4305 4059 0.85 0.42 ## se ## X1 53.76 As we can see, the differences between SD/HD and the week before don´t seem to be extraordinary high. To visualize the distribution of the data, we can create histograms: ggplot(movie_data,aes(streams_sd)) + geom_histogram(col = &quot;black&quot;, fill = &quot;darkblue&quot;) + labs(x = &quot;Number of SD stremas&quot;, y = &quot;Frequency&quot;) + theme_bw() ggplot(movie_data,aes(streams_hd)) + geom_histogram(col = &quot;black&quot;, fill = &quot;darkblue&quot;) + labs(x = &quot;Number of HD streams&quot;, y = &quot;Frequency&quot;) + theme_bw() We can now conduct a one sample t-test to test for significance. t.test(movie_data$streams_sd, mu = 2600, alternative = &quot;two.sided&quot;) ## ## One Sample t-test ## ## data: movie_data$streams_sd ## t = 1.0247, df = 199, p-value = 0.3067 ## alternative hypothesis: true mean is not equal to 2600 ## 95 percent confidence interval: ## 2527.979 2827.851 ## sample estimates: ## mean of x ## 2677.915 t.test(movie_data$streams_hd, mu = 1700, alternative = &quot;two.sided&quot;) ## ## One Sample t-test ## ## data: movie_data$streams_hd ## t = 2.5023, df = 199, p-value = 0.01314 ## alternative hypothesis: true mean is not equal to 1700 ## 95 percent confidence interval: ## 1728.51 1940.52 ## sample estimates: ## mean of x ## 1834.515 For SD streams, we can conclude that the average number of SD streams watched in this week (2677.92) were not significantly different from the 2600 streams watched in the previous week, t(199) = 1.025, p &gt; .05 (95% CI = [2528; 2828]). This can be seen from the fact that the p-value is larger than 0.05. This is also evidenced by the fact that the null hypothesis (2600) is included in the range of plausible values given by the confidence interval. However for HD streams we see that the perceived mean in our sample (1834.52) is significantly higher compared to the previous week t(199) = 2.502, p &lt;.05 (95% CI = [1729; 1941]). This can be seen from the fact that the p-value is smaller than 0.05. This is also evidenced by the fact that the null hypothesis (1700) is not included in the range of plausible values given by the confidence interval. Question 3 First we will analyse whether the advertising campaign had an effect on SD streams. We need to formulate a hypothesis which we can test. In this case, the null hypothesis is that the campaign had no effect on the mean number of streams, i.e. that there is no difference in the mean number of streams between the two populations. The alternative hypothesis states that the campaign did have an effect, meaning that there is a difference in the mean number of streams between the populations. In more formal notation this is: \\[H_0: \\mu_0 = \\mu_1 \\\\ H_1: \\mu_0 \\neq \\mu_1\\] We need to transform the variable online_advertising into a factor variable for some of our analyses: # Transform into factor variable movie_data$online_advertising &lt;- factor(movie_data$online_advertising, levels = c(0,1), labels = c(&quot;no&quot;, &quot;yes&quot;)) A good way to get a feeling for the data is to compute descriptive statistics and create appropriate plots. Since we are testing differences in means, a plot of means would be appropriate. # Descriptive statistics for SD streams, split by online advertising describeBy(movie_data$streams_sd, movie_data$online_advertising) ## ## Descriptive statistics by group ## group: no ## vars n mean sd median trimmed mad min max range skew kurtosis ## X1 1 100 2150.24 897.3 2088.5 2112.99 1188.3 705 4170 3465 0.25 -1.02 ## se ## X1 89.73 ## ------------------------------------------------------------ ## group: yes ## vars n mean sd median trimmed mad min max range skew kurtosis ## X1 1 100 3205.59 978.01 3147 3193.69 966.66 654 5709 5055 0.08 0.04 ## se ## X1 97.8 # Plot of means ggplot(movie_data, aes(online_advertising, streams_sd)) + geom_bar(stat = &quot;summary&quot;, color = &quot;black&quot;, fill = &quot;white&quot;, width = 0.7) + geom_pointrange(stat = &quot;summary&quot;) + labs(x = &quot;Online Advertising&quot;, y = &quot;Number of SD streams&quot;) + theme_bw() As we can see in both the descriptive statistics and the plot, the mean of the number of streams is higher where online_advertising = “yes”, i.e. for the movies that were included in the marketing campaign. To test whether or not this difference is significant, we need to use a two sample t-test. We use an inpependent-means t-test because we have different movies in each group (i.e., the movies in one condition are indpendent of the movies in the other condition). The requirements are clearly met: Our dependent variable is on an interval scale Since we have more than 30 observations per group we do not really have to concern ourselves with whether the data is normally distributed or not (see central limit theorem) If a movie was inluded in the campaign or not was assigned randomly R automatically performs Welch’s t-test, which corrects for unequal variance Thus we can perform the test in R t.test(streams_sd ~ online_advertising, data = movie_data) ## ## Welch Two Sample t-test ## ## data: streams_sd by online_advertising ## t = -7.9513, df = 196.55, p-value = 0.0000000000001418 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -1317.1019 -793.5981 ## sample estimates: ## mean in group no mean in group yes ## 2150.24 3205.59 The test is significant, since the p-value is smaller than 0.05, leading us to reject the null hypothesis that there is no difference in the mean number of streams. The p-value states the probability of finding a difference of the observed magnitude or higher, if the null hypothesis was in fact true (i.e., if there was in fact no difference between the populations). In effect, this means that the advertising campaign had an effect on the average number of times a video was streamed. Another thing we can extract from this test result is the confidence interval around the difference in means. Since 0 is not included in the interval, it is not a plausible value, cofirming the conclusion to reject the null hypothesis. The standardized effect size can be computed using the cohensD function: cohensD(streams_sd ~ online_advertising, data = movie_data) ## [1] 1.124481 This maginitude of the effect size (1.12) suggests that the effect of online advertising on the number of SD streams is large. The same can be done analogously for HD streams: # Descriptive statistics for HD streams, split by online advertising stats &lt;- describeBy(movie_data$streams_hd, movie_data$online_advertising) print(stats) ## ## Descriptive statistics by group ## group: no ## vars n mean sd median trimmed mad min max range skew kurtosis ## X1 1 100 1475.55 395.04 1483 1490.08 436.63 246 2305 2059 -0.34 -0.2 ## se ## X1 39.5 ## ------------------------------------------------------------ ## group: yes ## vars n mean sd median trimmed mad min max range skew kurtosis ## X1 1 100 2193.48 863.34 2133 2179.2 962.95 585 4305 3720 0.21 -0.73 ## se ## X1 86.33 # Plot of means ggplot(movie_data, aes(online_advertising, streams_hd)) + geom_bar(stat = &quot;summary&quot;, color = &quot;black&quot;, fill = &quot;white&quot;, width = 0.7) + geom_pointrange(stat = &quot;summary&quot;) + labs(x = &quot;Online Advertising&quot;, y = &quot;Number of HD streams&quot;) + theme_bw() Again, the summary statistics and the plot seem to indicate that there is a difference in means. Using the same reasoning as before, we can conclude that we need a two sample t-test to determine whether this difference is signficant (note that two sample t-test means the same as independent-means t-test). t.test(streams_hd ~ online_advertising, data = movie_data) ## ## Welch Two Sample t-test ## ## data: streams_hd by online_advertising ## t = -7.5617, df = 138.71, p-value = 0.00000000000494 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -905.6522 -530.2078 ## sample estimates: ## mean in group no mean in group yes ## 1475.55 2193.48 Again, the p-value is so low that any sensible signifcance level would lead us to reject the null hypothesis, suggesting that there is a difference in mean the number of streams between videos included in the campaign and those that aren’t. Calculate the standardized effect size: cohensD(streams_hd ~ online_advertising, data = movie_data) ## [1] 1.069388 The meagnitude of the effect size indicates again that this effect is large, although it is somewhat smaller than for SD streams. Question 4 Next we want to examine whether HD and SD streams have similar numbers on average. The null hypothesis here is that there is no difference in the mean number of HD streams and the mean number of SD streams for the same movies. Because the observations come from the same population of movies, we refer to the difference in the means for the same populaltion as \\(\\mu_D\\) when stating our hypotheses. The alternative hypothesis states that that there is a difference between the streams in HD and SD quality for the same movies. In mathematical notation this can be written as \\[H_0: \\mu_D = 0 \\\\ H_1: \\mu_D \\neq 0\\] Again, we start with descriptive statistics to get a feel for the data. # Descriptive statistics for HD and SD streams psych::describe(movie_data$streams_sd) ## vars n mean sd median trimmed mad min max range skew kurtosis ## X1 1 200 2677.91 1075.28 2732 2653.68 1064.51 654 5709 5055 0.2 -0.41 ## se ## X1 76.03 psych::describe(movie_data$streams_hd) ## vars n mean sd median trimmed mad min max range skew kurtosis ## X1 1 200 1834.52 760.23 1668.5 1761.9 578.21 246 4305 4059 0.85 0.42 ## se ## X1 53.76 # Plot of means movie_data_long &lt;- melt(movie_data[, c(&quot;streams_sd&quot;, &quot;streams_hd&quot;)]) names(movie_data_long) &lt;- c(&quot;stream&quot;, &quot;number&quot;) ggplot(movie_data_long, aes(stream, number)) + geom_bar(stat = &quot;summary&quot;, color = &quot;black&quot;, fill = &quot;white&quot;, width = 0.7) + geom_pointrange(stat = &quot;summary&quot;) + labs(x = &quot;Group&quot;, y = &quot;Listening time (hours)&quot;) + ggtitle(&quot;Means and standard errors of streams&quot;) + theme_bw() It appears that there is a difference in the means. To test whether it is significant, we again need a t-test. However, this time we need a slightly different version of the t-test because the same movies are observed for HD and SD streams (i.e., the same movies are available in both formats). This means that we need a dependent means t-test. This test is also knownas the paired samples t-test. The other assumptions are virtually identical to the independent-means t-test. The test can be executed in R by adding paired = TRUE to the code. t.test(y = movie_data$streams_sd, x = movie_data$streams_hd, paired = TRUE) ## ## Paired t-test ## ## data: movie_data$streams_hd and movie_data$streams_sd ## t = -11.214, df = 199, p-value &lt; 0.00000000000000022 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -991.7133 -695.0867 ## sample estimates: ## mean of the differences ## -843.4 The p-value is again lower than the chosen signifance level of 5% (i.e., p &lt; .05), which means that we reject the null hypothesis that there is no difference in the mean number of streams in HD and SD quality. Make sure you interpret the p-value correctly. It refers to the probability of observing a difference of the observed magnitude (or larger) between streams in HD and SD quality, assuming that there was in fact no difference between the formats. The confidence interval confirms the conclusion to reject the null hypothesis since \\(0\\) is not contained in the range of plausible values. Now let’s find out how strong this effect is. cohensD(movie_data$streams_sd, movie_data$streams_hd) ## [1] 0.9057351 A standardized effect size of approx. 0.9 tells us that this effect is large. Question 5 The question of how many movies we would need to include in each sample of our experiment can be answered quite comfortably with a power calculation function in R. pwr.t.test(d = 0.3, sig.level = 0.05, power = 0.8, type = c(&quot;two.sample&quot;), alternative = c(&quot;two.sided&quot;)) ## ## Two-sample t test power calculation ## ## n = 175.3847 ## d = 0.3 ## sig.level = 0.05 ## power = 0.8 ## alternative = two.sided ## ## NOTE: n is number in *each* group To achive our desired effect size of 0.3, a significance level of 0.5 and a power of 0.8 we would need to include at least 175 movies per group in our sample. 9.2 Assignment 3 (Hypothesis Testing 2) The data file contains customer information from an online fashion shop. In an experiment, the customers were exposed to different types of online advertising over the past year (randomly assigned) and now you wish to analyze the results. The following variables are included in the data set: customerID: unique customer ID revenue: revenue per cusomer for the past year (in EUR) gender: 0=male, 1=female retargeting: type of online advertising that the customer was exposed to (3 levels: 1 = no advertising, 2 = generic retargeting, 3 = dynamic retargeting) customerRank: ranking of customers according to their expenditure level (low rank = valuable customer, high rank = less valuable customer) conversion: indicator variable, indicating if a customer converted in the previous campaign (0 = no conversion, 1 = conversion) Use R and appropriate analytical techniques to answer the following questions: Has the types of online advertising an effect on revenue? Are there significant differences between the individual groups? Is the customer ranking significantly influenced by the type of online advertising? Are there significant differences between the individual groups? Does the conversion rate in the previous campaign differ between male and female customers? When answering the questions, please remember to address the following points, where appropriate: Formulate the corresponding hypotheses and choose an appropriate statistical test Provide the reason for your choice and discuss if the assumptions of the test are met Convert the variables to the appropriate type (e.g., factor variables) Create appropriate graphs to explore the data (e.g., boxplot, bar chart, histogram) Provide appropriate descriptive statistics for the variables Report and interpret the test results accurately (including confidence intervals) Finally, don’t forget to report your research conclusion in an appropriate way When you are done with your analysis, click on “Knit to HTML” button above the code editor. This will create a HTML document of your results in the folder where the “assignment3.Rmd” file is stored. Open this file in your Internet browser to see if the output is correct. If the output is correct, submit the HTML file via Learn@WU. The file name should be “assignment3_studendID_name.html”. Load data rm(list = ls()) customer_data &lt;- read.table(&quot;https://raw.githubusercontent.com/IMSMWU/MRDA2018/master/data/assignment3.csv&quot;, sep = &quot;;&quot;, header = TRUE) #read in data head(customer_data) str(customer_data) ## &#39;data.frame&#39;: 296 obs. of 6 variables: ## $ revenue : int 3866 1576 2667 772 2702 1277 2023 2170 3103 3067 ... ## $ gender : int 1 1 1 1 1 1 1 1 1 1 ... ## $ retargeting: int 3 1 1 1 3 2 1 3 3 1 ... ## $ customerID : int 2 7 8 12 13 14 22 24 26 31 ... ## $ rank : int 1 159 37 259 35 203 103 87 17 19 ... ## $ conversion : int 1 1 0 1 1 1 0 1 1 0 ... Data Preparation As always, the first step is to load required packages (packages that have not been used as often in the course will be loaded as required to show which packages contain certain functions) and to load and inspect the data. library(plyr) library(ggplot2) library(psych) library(Hmisc) Next we are going to recode some of the variables into factors and give them more descriptive level names. customer_data$retargeting &lt;- factor(customer_data$retargeting, levels = c(1,2,3), labels = c(&quot;no retargeting&quot;, &quot;generic retargeting&quot;, &quot;dynamic retargeting&quot;)) customer_data$gender &lt;- factor(customer_data$gender, levels = c(1,0),labels = c(&quot;female&quot;,&quot;male&quot;)) customer_data$conversion &lt;- factor(customer_data$conversion, levels = c(1,0), labels = c(&quot;conversion&quot;,&quot;no conversion&quot;)) Question 1 To answer whether the type of advertising has an effect on revenue we need to formulate a testable null hypothesis. In our case the null hypothesis is stating that the average level of sales is equal for all advertising types. In mathematical notation this implies: \\[H_0: \\mu_1 = \\mu_2 = \\mu_3 \\] The alternate hypothesis is simply that the means are not all equal, i.e., \\[H_1: \\textrm{Means are not all equal}\\] If you wanted to put this in mathematical notation, you could also write: \\[H_1: \\exists {i,j}: {\\mu_i \\ne \\mu_j} \\] The appropriate test for such a hypothesis is one-way ANOVA since we have a metric scales dependent variable and a categorical independent variable with more than two levels. Next we will calculate summary statistics for the data and produce an approppriate plot. describeBy(customer_data$revenue,customer_data$retargeting) ## ## Descriptive statistics by group ## group: no retargeting ## vars n mean sd median trimmed mad min max range skew kurtosis ## X1 1 106 1414.31 714.21 1437.5 1386.42 786.52 100 3356 3256 0.31 -0.53 ## se ## X1 69.37 ## ------------------------------------------------------------ ## group: generic retargeting ## vars n mean sd median trimmed mad min max range skew kurtosis ## X1 1 90 1502.63 681.3 1445 1491.79 729.44 78 2885 2807 0.12 -0.74 ## se ## X1 71.82 ## ------------------------------------------------------------ ## group: dynamic retargeting ## vars n mean sd median trimmed mad min max range skew kurtosis ## X1 1 100 2208.28 796.85 2277.5 2218.12 735.37 145 3866 3721 -0.16 -0.51 ## se ## X1 79.68 ggplot(customer_data, aes(retargeting, revenue)) + stat_summary(fun.y = mean, geom = &quot;bar&quot;, fill = &quot;White&quot;, colour = &quot;Black&quot;) + stat_summary(fun.data = mean_cl_normal, geom = &quot;pointrange&quot;) + labs(x = &quot;Experimental group (promotion level)&quot;, y = &quot;Number of sales&quot;) + theme_bw() Both the summary statistics and the plot hint at the fact that the means may not be equal. Especially the difference between dynamic retargeting and no retargeting/ generic regtargeting seem to be quite high. Before we move to the formal test, we need to see if a series of assumptions are met, namely: Distributional assumptions Homogeneity of variances Independence of observations The last assumption is satisfied due to the fact that the observations were randomly assigned to the advertisement groups. To see if we need to worry about distributional assumptions we first take a look at the number of observations in each advertising group. #check number of observations by group table(customer_data$retargeting) ## ## no retargeting generic retargeting dynamic retargeting ## 106 90 100 Due to the fact that there are always more than 30 observations in each group we can rely on the central limit theorem to satisfy the distributional assumptions. Homogeneity of variances can be checked with Levene’s test (implemented as leveneTest() from the car package). The null hypothesis of this test is that the variances are equal, with the alternative hypothesis being that the variances are not all equal. #Homogeneity of variances test: library(car) leveneTest(revenue ~ retargeting, data=customer_data, center=mean) ## Levene&#39;s Test for Homogeneity of Variance (center = mean) ## Df F value Pr(&gt;F) ## group 2 1.2455 0.2893 ## 293 The test result is insignificant (for a signifcance level of 5 %), meaning that we do not reject the null hypothesis of equal variances and can operate under the assumption that the variances are equal. Since all assumptions are fulfilled we can move on to conducting the actual ANOVA using the aov() function. #Anova: aov &lt;- aov(revenue~retargeting, data = customer_data) summary(aov) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## retargeting 2 37966043 18983022 35.26 0.000000000000019 *** ## Residuals 293 157734112 538342 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The p-value is smaller than 0.05, which we chose as our significance level, meaning that we reject the null hypothesis of the means being equal in the three advertising groups. Next we will briefly inspect the residuals of the ANOVA to see if the assumptions of the test really are justified. #Inspect residuals plot(aov,1) The first plot gives us a feel for the distribution of the residuals of the three groups. The residuals seem to be roughly equally distributed, which speaks for the fact that the homogeneity of variances assumptions is fulfilled. plot(aov,2) The second plot is a QQ-plot of the residuals, meant as a quick visual check to see if the normality assumption is fulfilled. Leading up to the test we only checked if there were more than 30 observations per group to satisfy the normality assumption but despite this being fulfilled it is still important to check the normality of the residuals, as any strange behaviour here may indicate problems with the model specification. To further confirm that the residuals are roughly normally distributed we employ the Shapiro-Wilk test. The null hypothesis is that the distribution of the data is normal, with the alternative hypothesis positing that the data is not normally distributed. shapiro.test(resid(aov)) ## ## Shapiro-Wilk normality test ## ## data: resid(aov) ## W = 0.99378, p-value = 0.2637 The p value is far above any widely used significance level and thus we can not reject the null hypothesis of normal distribution, which further implies that the normality assumption is fulfilled. The ANOVA result only tells us that the means of the three groups are not equal, but it does not tell us anything about which pairs of means are unequal. To find this out we need to conduct post hoc tests to test the following null hypotheses for the respective pairwise comparisons. \\[1) H_0: \\mu_1 = \\mu_2; H_1 = \\mu_1 \\neq \\mu_2 \\\\ 2) H_0: \\mu_2 = \\mu_3; H_1 = \\mu_2 \\neq \\mu_3 \\\\ 3) H_0: \\mu_1 = \\mu_3; H_1 = \\mu_1 \\neq \\mu_3 \\] Here we will conduct both the Bonferroni correction as well as Tukey’s HSD test, however either would be sufficient for your homework. Bonferroni’s correction conducts multiple pairwise t-tests, with the null hypothesis being that of equal means in each case and the alternative hypothesis stating that the means are unequal. #bonferroni pairwise.t.test(customer_data$revenue, customer_data$retargeting, data=customer_data, p.adjust.method = &quot;bonferroni&quot;) ## ## Pairwise comparisons using t tests with pooled SD ## ## data: customer_data$revenue and customer_data$retargeting ## ## no retargeting generic retargeting ## generic retargeting 1 - ## dynamic retargeting 0.00000000000042 0.00000000051570 ## ## P value adjustment method: bonferroni The Bonferroni test reinforces what we saw in our plot earlier, namely that not all of the means might be significantly different from each other. We can only reject the null hypothesis in the cases: dynamic regargeting vs. no retargeting dynamic regargeting vs. generig retargeting But there seems to be no difference in the means of generic retargeting vs. no retargeting. Tukey’s HSD similarly compares pairwise means, corrected for family-wise errors (both of the post hoc tests would have been considered correct). #tukey correction using the mult-comp package library(multcomp) tukeys &lt;- glht(aov, linfct = mcp(retargeting = &quot;Tukey&quot;)) summary(tukeys) ## ## Simultaneous Tests for General Linear Hypotheses ## ## Multiple Comparisons of Means: Tukey Contrasts ## ## ## Fit: aov(formula = revenue ~ retargeting, data = customer_data) ## ## Linear Hypotheses: ## Estimate Std. Error t value ## generic retargeting - no retargeting == 0 88.32 105.17 0.840 ## dynamic retargeting - no retargeting == 0 793.97 102.28 7.762 ## dynamic retargeting - generic retargeting == 0 705.65 106.61 6.619 ## Pr(&gt;|t|) ## generic retargeting - no retargeting == 0 0.679 ## dynamic retargeting - no retargeting == 0 &lt;0.00001 *** ## dynamic retargeting - generic retargeting == 0 &lt;0.00001 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## (Adjusted p values reported -- single-step method) Tukey’s correction confirms the conclusion from the Bonferroni test from above. While there seems to be no difference in the means of generic retargeting vs. no retargeting, dynamic retargeting seems to differ significantly from both generic retargeting and no retargeting. Tukey’s HSD further let’s us estimate the difference in means with corresponding confidence intervals. confint(tukeys) ## ## Simultaneous Confidence Intervals ## ## Multiple Comparisons of Means: Tukey Contrasts ## ## ## Fit: aov(formula = revenue ~ retargeting, data = customer_data) ## ## Quantile = 2.3553 ## 95% family-wise confidence level ## ## ## Linear Hypotheses: ## Estimate lwr upr ## generic retargeting - no retargeting == 0 88.3220 -159.3805 336.0245 ## dynamic retargeting - no retargeting == 0 793.9687 553.0574 1034.8800 ## dynamic retargeting - generic retargeting == 0 705.6467 454.5551 956.7382 # The mar parameter changes the margins around created plots. This is done so the labels on the side of the Tukey plot are visible (however, this was not expected). par(mar = c(5, 20, 4, 2)) plot(tukeys) It is clearly visible that just the CIs of generic retargetring vs. no retargeting cross the 0 bound, which further indicates that the differences in means are statistically not significantly different from 0. From a reporting standpoint we can say that revenue is higher when using dynamic retargeting vs. no retargeting or generic retargeting, but there is no sifnificant difference between the sales for products in the dynamic retargeting vs. no retargeting conditions. Managerially, this menas that only dynamic retargetting helps us to increase sales. Question 2 For this question we want to examine whether customer ranks are signifcantly different for different types of advertising. Because we are dealing with data on an ordinal scale, we can not use ANOVA for this type of question. The non-parametric counterpart is the Kruskal-Wallis test, which tests for differences in medians between groups. Hence, the null hypothesis is that the medians are equal in each group and the alternative hypothesis is that there is a difference between at least one pair of groups in terms of the median. \\[H_0: \\bar{\\mu}_1 = \\bar{\\mu}_2 = \\bar{\\mu}_3 \\] \\[H_1: \\textrm{The meadians are not all equal} \\] Or, alternatively \\[H_1: \\exists {i,j}: {\\bar \\mu_i \\ne \\bar \\mu_j} \\] A good way to visualize ordinal data is through a boxplot. ggplot(data = customer_data, aes(x = retargeting, y = rank)) + geom_boxplot() + theme_bw() + labs(x = &quot;&quot;, y = &quot;Rank&quot;) The boxplot seems to indicate that the medians are unequal. At least for dynamic retargeting our customer ranks seem to be lower than the ones of no retargeting or generic retargeting. The only assumption that we require for this test is that the dependent variable is at least ordinal, which is fulfilled for customer ranks. Hence we can move on to performing the test in R. #ordinal data so we use a non-parametric test kruskal.test(rank ~ retargeting, data = customer_data) ## ## Kruskal-Wallis rank sum test ## ## data: rank by retargeting ## Kruskal-Wallis chi-squared = 54.163, df = 2, p-value = ## 0.000000000001732 The p-value is below any sensible signifcance level and thus we reject the null hypothesis of equal medians. This means that the median rank of customers is different for different types of retargeting, implying that the type of retargeting has an effect on the customer rank. To further see which of the medians are unequeal we perform the Nemenyi post hoc test, which can be found in the PCMCR package in R. The null hyptohesis is that the pairwise medians are equal, while the alternative hypothesis is that the pairwise medians are unequal. library(PMCMR) posthoc.kruskal.nemenyi.test(x = customer_data$rank, g = customer_data$retargeting, dist = &quot;Tukey&quot;) ## ## Pairwise comparisons using Tukey and Kramer (Nemenyi) test ## with Tukey-Dist approximation for independent samples ## ## data: customer_data$rank and customer_data$retargeting ## ## no retargeting generic retargeting ## generic retargeting 0.67 - ## dynamic retargeting 0.000000000022 0.000000030987 ## ## P value adjustment method: none Similar to question 1 we can see that there seems to be no difference in (median) customer ranks of no retargeting vs. generic retargeting. On the other side ranks of dynamic retargeting seem to be significantly different from both no retargeting and generic retargeting. This implies that just dynamic retargeting leads to different customer ranks. Question 3 To find out whether our conversion rate differs between our female and male customers, we can use a test for proportions instead of a test for mean differences. To test for the equality of proportions (and therefore no difference between them) we can use a \\(\\chi^2\\) test. Our null hypothesis in this case states that the proportions of conversion are equal for females and males. Our alternative hypothesis states that these proportions are unequal. \\[H_0: \\pi_1 = \\pi_2 \\\\ H_1: \\pi_1 \\neq \\pi_2\\] First letÂ´s create a summary plot to get a feeling for the data. #conditional relative frequencies rel_freq_table &lt;- as.data.frame(prop.table(table(customer_data$gender, customer_data$conversion), 1)) names(rel_freq_table) &lt;- c(&quot;gender&quot;, &quot;conversion&quot;,&quot;freq&quot;) # changing names of the columns rel_freq_table ggplot(rel_freq_table, aes(x = gender, y = freq, fill = conversion)) + #plot data geom_col(width = .7) + #position geom_text(aes(label = paste0(round(freq*100,0),&quot;%&quot;)), position = position_stack(vjust = 0.5), size = 4) + #add percentages ylab(&quot;Proportion of conversions&quot;) + xlab(&quot;gender&quot;) + # specify axis labels theme_bw() We see that our conversion seems to be better for our female customers, but letÂ´s check whether these proportions are significantly different. n1 &lt;- nrow(subset(customer_data, gender == &quot;female&quot;)) #number of observations for females n2 &lt;- nrow(subset(customer_data, gender == &quot;male&quot;)) #number of observations for males n1_conv &lt;- nrow(subset(customer_data, gender == &quot;female&quot; &amp; conversion == &quot;conversion&quot;)) #number of conversions for females n2_conv &lt;- nrow(subset(customer_data, gender == &quot;male&quot; &amp; conversion == &quot;conversion&quot;)) #number of conversions for males prop.test(x = c(n1_conv, n2_conv), n = c(n1, n2), conf.level = 0.95) ## ## 2-sample test for equality of proportions with continuity correction ## ## data: c(n1_conv, n2_conv) out of c(n1, n2) ## X-squared = 24.236, df = 1, p-value = 0.0000008523 ## alternative hypothesis: two.sided ## 95 percent confidence interval: ## 0.1548639 0.3586496 ## sample estimates: ## prop 1 prop 2 ## 0.3851351 0.1283784 The test showed that the conversion rate for females was 26% higher compared to male customers. This difference is highly significant \\(\\chi^2\\) (1) = 24.2, p &lt; .05 (95% CI = [0.16,0.36]), which means that we can reject our null hypothesis of equal probability and state that there indeed is a difference between our male and female customers respective their conversion rate. "],
["questionnaire-design.html", "10 Questionnaire design 10.1 Questionnaire design process 10.2 Questionnaire in Qualtrics 10.3 Question types and data analysis", " 10 Questionnaire design Welcome to the questionnaire design guide! An aim of this course is to develop your ability to translate business problems into actionable research questions and to design an adequate research plan to answer these questions. Therefore, you need to be equiped with knowledge on how to create a survey and properly conduct a research. Generally, what you can expect from the survey design is similar to what one experiences in a relationship. If you try to take more than you commit, it doesn’t work out. Now on a serious note, if you follow guidelines mentioned here, you will certainly avoid usual traps your fellow collegues were caught in. In a research process, conducting a survey is a part of (primary) data collection. Before we collect data, we have to make sure that preceding steps are correctly done. However, in the following sections we will focus on the process of designing a questionnaire. Eventually, you will be able to collect relevant data and apply appropriate statistical tests. 10.1 Questionnaire design process A structured questionnaire is a research instrument designed to elicit specific information from a sample of a target population. Usually it is used in a standardized way with fixed-alternative questions (same questions and response options for all respondents). An objective of a questionnaire is threefold: to translate the information need into a set of specific questions that the respondent can and will answer, to motivate, and encourage respondents to become involved, to cooperate, and to complete the questionnaire, to minimize response error. In order to meet these objectives, a questionnaire design process suggests the following sequence of steps: 10.1.1 Specification of the information needed The questionnaire design should be aligned with the research design! In order to do make it aligned, it is necessary to review components of the problem and the approach. In particular, you should review the research questions, hypotheses and characteristics that influence the research design. If you are interested in the causal effect of one particular (independent) variable on another (dependent) variable, think about an experimental design that might allow you to manipulate this variable. In this case, you particularly have to decide on the following: Which variable to manipulate? Whether to use a between-subjects or within-subjects design? The cause-effect sequence (the cause must occur before the effect) The number of experimental conditions Potential interactions and relationships with other variables (does the effect depend on another variable?) What you need to be careful about is the effect of reversed causation. The effect refers to the situation where the causal relationship could possible have an opposite direction from what we assumed at the first place. For instance, it is often assumed that an increase in individual income leads to increase in well-being (happiness). However, some researches suggest that this causation could have an opposite direction, i.e. that actually increase in well-being of an individual leads to an increase in income. Here are some examples of causal research design applications: To assess how a product’s country-of-origin impacts attractiveness across different countries. To analyse the effects of rebranding on customer loyalty. If you would like to analyze the effects of multiple categorical or continuous (independent) variables on one continuous (dependent) variable, you might use a regression model. When doing this, you particularly have to decide on: How to measure the dependent variable (DV). This is particularly important, since you need a variable that is powerful in uncovering variation between subjects (e.g., open-ended questions, such as “How much are you willing to pay for this product” are good candidates). Moreover, you also need to consider the nature of your DV,i.e. whether it is an interval variable, ordinal or categorical variable. The nature of your DV will heavily influence your choice of a correct statistical test. How to measure the independent variables (IV) (single-item vs. multi-item scales, categorical vs. continuous). Bear in mind that the nature of the IV, together with DV, affects your choice of a statistical test as well. What other variables might cause the effect that you would like to investigate (to prevent omitted variable bias, i.e. variables that are not part of your model but still influence the dependent variable). Potential interactions (e.g., is the effect of variable X stronger for group A vs. B?) 10.1.2 Specify the interviewing method In the next step you should review the type of interviewing method you will use. At this point you need to think in which setting you aim to conduct your survery. For instance, should you do it in a face-to-face setting or rather online. Here you can find some advantages and disadvantages of online surveys: Additionally, here is the list of the online tools you can use to conduct an online survey (usually for free): Qualtrics Google form Survey monkey Free online surverys Kwik surveys 10.1.3 Determine the content of questions In this step you are starting to work on the content of you questions. There are several questions you should ask yourself when writing questions: Is the question necessary? Will I obtain the needed information? Are several questions needed instead of one? What type of data can I collect by asking that question (categorical or continuious)? In your survey try to avoid asking double-barrelled questions.Those are a single question that attempts to cover two issues. Such questions can be confusing to respondents and result in ambiguous responses. Instead, you might ask multiple questions in order to obtain the inteded information. Incorrect: Do you think Nike Town offers better variety and prices than other Nike stores? Correct: Do you think Nike Town offers better variety than other Nike stores? Do you think Nike Town offers better prices than other Nike stores? 10.1.4 Inability and unwillingness to answer The quality of collected data you highly depends on your ability to address correct participants. Therefore, you need to make sure that your respondents are able to meaningfully answer your questions. Examples: Not every household member might be informed about monthly expenses for groceries purchases if someone else makes these purchases. Use filter questions that measure familiarity and product use. Include a “don’t know” option. If you ask participants for monteray values (e.g. how much are you ready to pay for the XY product?) across several EU, make sure you indicate correct currency (e.g. HRK for Croatia or HUF for Hungary). Think about how mobile friendly is the layout of your survey (if it is an online survey). Good case practices suggest that there should not be more than 2 questions per page (for online surveys displayed on mobile phones). If you are asking participants to recall certain brands for instance, make sure you use unaided recall question: Example of unaided recall question: What brands of soft drinks do you remember being advertised on TV last night? Example of aided recall question: Which of these brands were advertised last night on TV? a) Coca-Cola b) Pepsi c) Red Bull d) Evian e) Don’t know If you are asking participants to list something, the good case practice is to minimize the effort required by respondents: Incorrect: Please list all the departments from which you purchased merchandise on your most recent shopping trip to department store X. Correct: Please check all the departments from which you purchased merchandise on your most recent shopping trip to a department store: a) Women’s dresses b) Men’s apparel c) Children’s apparel d) Cosmetics e) Jewelry f) Other (please specify) ___________ In a case you are asking for information that could be considered sensitive (e.g. money, family life, political beliefs, religion), they should come at the end of the questionnaire. Moreover, it is recommendable to provide response categories rather than asking for specific figures: Incorrect: What is your household’s exact annual income? Correct: Which one of the following categories best describes your household’s annual gross income? a) under 25.001 € b) 25.001€ to 50.000 € c) 50.001€ to 75.000 € d) 75.001€ to 100.000 € e) over 100.000 € 10.1.5 Decide on measurement scales and scaling techniques Every statistical analysis requires that variables have a specific levels of measurement. Measurement scales you choose for your questions in a survey will affect the answers you get and eventually statistical test you can apply. For instance, it would not make sense to compute an average of genders. An average of a categorical variable does not make much sense. Moreover, if you tried to compute the average of genders defined in numeric values (e.g. male=0, female=1), the output would be interpretable. Therefore, it is crucial to become familiar with possibilities of each scale before you choose to add another question to your survey. Consequently, chances to obtain data you did not intend to collect and chances that you will not be able to apply tests you intended are significantly lower. In the following table you can get a quick overview of possibilities per each measurement scale. : In the table below you can find general procedure for choosing a correct analysis based on the measurement scale of your data and number of variables. It shows statistical analyses we covered during the course and aims to help you choose among them based on the nature of dependent variables on the side, and the nature and the number of your independent variables on the other side: When it comes to scaling techniques, they are meant to study the relationship between objects. The basic scaling techniques classification is on comparative and non-comparative scales. The noncomparative scale each object is scaled independently of the other objects. The resulting data is supposed to be measured in an interval and ratio scaled. Comparative scales (or nonmetric scaling) compare direclty the stimulus object. For example, the respondent might be asked directly about his preference between domestic and foreign beer brands. As a result, the comparative data collected can only be interpreted in relative terms. In the following sections we will walk through both types of comparative scales and briefly introduce them. 10.1.5.1 Comparative scale: Paired Comparison Respondent is presented with two objects and asked to select one according to some criterion. The nature of resulting data is ordinal Assumption of transitivity (if X &gt; Y and Y &gt; Z, then X &gt; Z) enables the paired comparison data to be converted into a rank order. To do so, you need to indetify the number of times the object is preferred by adding up all the matrices. Effective when the number of objects is limited as it requires the direct comparison, and a bigger number of objects makes the comparison becomes unmanagable. Example: For each pair, please indicate which of the two brands of beer in the pair you prefer. 10.1.5.2 Comparative scale: Rank Order Allow a certain set of brands or products to be simultaneously ranked based upon a specific attribute or characteristic. The rank order scaling is a good proxy for to the shopping setting as there are simultanious comparisons of objects. The rank order scaling results in the data of ordinal nature. Example: Rank the various brands of beer in order of preference. Begin by picking out the one brand that you like most and assign it a number 1. Then find the second most preferred brand and assign it a number 2. Continue this procedure until you have ranked all the brands of beer in order of preference. No two brands should received the same rank number. 10.1.5.3 Comparative scale: Constant sum Respondents allocate a constant sum of units (e.g., points, dollars) among a set of stimulus objects with respect to some criterion. Constant sum is similar to rank order, but it carries specific units. The resulting data does not just indicate important factors, but also by how much a factor supersedes another one. Constant sum scaling can be used to observe the comparative significance respondents assigned to various factors of a subject. Example: There are 8 attributes of bottled beers. Please allocate 100 points among the attributes so that your allocation reflects the relative importance you attach to each attribute. Basic analysis of constant-sum data involves tabulation of responses and presenting them as either quantities (e.g., “on average, 7 points were allocated to”high alcohol level“), or, as proportions (”On average, 7% of points were allocated to “high alcohol level”). 10.1.5.4 Non-Comparative Scales: Continuous Rating Scales Participants rate the objects by placing a mark at the appropriate position on a line that runs from one extreme of the criterion variable to the other. One of the advantages of the continuous rating scale is that it is easy to administer. Once the ratings are collected, you can splits up the obtained ratings into categories and then assign those depending on the category in which the ratings fall. 10.1.5.5 Non-Comparative Scales: Itemized Rating Scales The respondents are provided with a scale that has a number or brief description associated with each category. The categories are ordered in terms of scale position, and the respondents are required to select the specified category that best describes the object being rated. The commonly used itemized rating scales are the Likert, semantic differential and Stapel scales. 10.1.5.5.1 Itemized Rating Scales: Likert scale Requires respondents to indicate their attitude towards the given object through the degree of agreement or disagreement with each of a series of statements within typically five or seven categories. Reversed code of some items increases validity. One limitation is time required to answer a question on a Likert scale. Compared to other itemized scaling techniques, Likert scale is more time consuming as each respondent is required to read every statement given in a questionnaire before assigning a numerical value to it. In the table below you can find a couple of commonly measured constructs in marketing research such as attitude, importance, purchase intention and similar. 10.1.5.5.2 Itemized Rating Scales: Semantic Differential Typically, participants rate objects on a number of itemized, seven-point rating scales bounded at each end by one of two bipolar adjectives. Semantic differential can measure respondent attitudes towards something (products,concepts, items, people…). It helps you find the repondent’s position is on a scale between two bipolar adjectives such as “Sweet-Sour” or “Bright-Dark”. In comparison to Likert scale, which uses generic scales (e.g. extremely dissatisfied to extremely satisfied), semantic differential questions are posed within the context of evaluating attitudes. Widely used rating scale in marketing research due to its versatility When creating a semantical difference question, you should consider the following: Number of categories: Balanced vs. unbalanced: Odd/even number of categories: Forced vs. non-forced response Verbal description: 10.1.6 Questionnaire structure The sequnece of questions in a questionnaire could play imporant role. For instance, more sensitive questions (such as demographic-related questions) are usually placed at the end as they can trigger change in respondent’s behavior. If you plan to conduct an online survey, then you need to think about the respondent’s experience while doing your questionnaire. For instance, spread the content over more short pages and do not have fewer long pages. In online surveys, two questions on one page is a useful rule of thumb. Generally, respondents are reluctant to read and fill out long questionnaire pages. Hence, long pages will lead to a higher dropout rate. In order to reduce dropout rate state how long the survey will approximately take in the introduction of the questionnaire. Take into account that tools like Qualtrics provide the estimated response time in the survey overview. Consider that the most of people usually use their phones to fill it out. Think about how the questionnaire will appear on a phone screen too. In that regard, think of length of questions especially. In the end, the questionnaire structure has to be aligned with the research design. For example, if your research design features an experiment, this needs to be reflected in the questionnaire (e.g., you need to assign the respondents randomly to the experimental conditions in case of a between-subjects comparison). 10.1.6.1 Questionnaire structure for a between-subjects design In a between-subject design you randomly assign each respondent to different experimental conditions. They would then complete tasks only in the condition to which they are assigned. For instance, we would like to test the effect of two advertisements on purchase intention. Therefore, one group of (randomly assigned) respondents will be exposed to one advertisement version while the other group (of randomly assigned respondents) will be exposed to another version. After that, both groups of respondents should express their willingness to buy the advertised product. Evenutally, if the dependent variable (e.g. willingness to buy) is measured on interval or ratio scale, then you can use independent t-test to compare group means. The whole experimental design should be organised as following: 10.1.6.2 Questionnaire structure for a within-subjects design This type of experimental design involves exposing each respondent to all of the user experimental conditions you’re testing. This way, each respondent will test all of the conditions. For instance, we would like to test again the effect of two advertisements on purchase intentions, but this time in a within-subject design. First, each respondent will be exposed to the first version of advertisement and right after that asked to rate his/her willingness to buy the advertised product. Subsequently, each participant will be shown another version of advertisement and again rate his/her willingness to purchase the advertised product. Finally, we can compare group means with paired sample t-test (given that data is measured on interval or ratio scale). 10.1.7 Question wording Generally, question wording should enable each respondent to understand questions and to be able to answer them with reliability. Reliability means that, if a respondent was asked the same question again, he/she would give the same answer again. A number of common problems regarding the question wording have been identified, so we will address the most important ones. In order to ensure reliability, the issue in terms of who, what, when and where should be defined in each question. Example: Which brand of shampoo do you use? Who (the respondent): It is not clear whether this question relates to the individual respondent or the respondent’s total household. What (the brand of shampoo): It is unclear how the respondent is to answer this question if more than one brand is used. When (unclear): The time frame is not specified in this question. The respondent could interpret it as meaning the shampoo used this morning, this week, or over the past year. Where (not specified): At home, at the gym? Where? A more clearly defined question is: Which brand or brands of shampoo have you personally used at home during the last month? In the case of more than one brand, please list all the brands that apply. Use ordinary words. Words should match the vocabulary level of the participants. Incorrect: “Do you think the distribution of soft drinks is adequate?” Correct: “Do you think soft drinks are easily available when you want to buy them?” Avoid double negative form. Double negative question forms can confuse respondents, especially when they need to answer with “Agree” or “Disagree”. Incorrect: Do you think that it is not uncommon that boys play basketball? Correct: In your opinion, is it common that boys play basketball? Avoid leading questions.Leading questions clue the participant to what the answer should be. Such questions introduce a bias in a particular direction. Incorrect: “Is Colgate your favorite toothpaste?” Correct: “What is your favorite brand of toothpaste?” Avoid ambiguous words. Words such as usually, normally, frequently, often, regularly, and other similar words, do not define frequency clearly enough. Incorrect: “In a typically month, how often do you go to a movie theater to see a movie?” a) Never b) Occasionally c) Sometimes d) Often e) Regularly Correct: “In a typically month, how often do you go to a movie theater to see a movie?” a) Less than once b) 1 or 2 times c) 3 or 4 times d) More than 4 times 10.1.8 Choose adequate order One of the last steps in a process of designing a questionnaire is choosing adequate order of questions and instructions for respondents. At the begining, you should provide a short and easy-to-understand introduction to the topic. Use simple language and avoid technical terms (e.g., not many people will know the terms “manufacturer brand” and “store brand”). Additionally, in the introduction you should state how long the survey will approximately take. The opening questions should be interesting, simple and non-threatening. They are crucial because it is the respondent’s first exposure to the questionnaire and is likely to set the tone for the rest of questions in the questionnaire. If too difficult to understand, or sensitive in some way, respondents are likely to stop answering your questions. Qualifying questions (or screening questions) should serve as the opening questions (if applicable). Their purpose is to identify a potential respondent that is eligable to proceed with the research survey. After the opening part, you should establish an optimal question flow. General questions should precede the specific questions. Questions on one subject, or one particular aspect of a subject, should be grouped together. It may feel confusing to be asked to return to some subject they thought they already gave their opinions about. As respondents are moving towards the end of the questionnaire, they are likely to become increasingly indifferent and might give careless answers. Therefore, questions of special importance should ideally be included in the earlier part of the questionnaire. Finally, you should pay particular attention to provide all prescribed definitions and explanations before you ask a question. This ensures that the questions are undestood in consistent way by every respondent. 10.1.9 Test your questionnaire Finally, before you distribute the final questionnaire, there are some things to consider. First, you should always pretest your questionnaire before sharing it! Test all aspects of the questionnaire (content, wording, sequence, form &amp; layout, etc.). If possible, use respondents in the pretest that are similar to those who will be included in the actual survey. Ideally, the pretest sample size should be small (in a real scenario this could varyfrom 15 to 30 respondents; for the group project, a lower number will be sufficient). After each significant revision of the questionnaire, conduct another pretest, using a different sample of respondents. Eventually, code and analyze the responses obtained from the pretest so that you make sure that you collected information you intended to collect. After testing your questionnaire you should be able to determine whether: The questions are properly framed The questions wording triggers any biases The questions are placed in the optimal order The questions are understandable Specifying questions are needed or some need to be eliminated 10.2 Questionnaire in Qualtrics A questionnaire creation in Qualtrics starts with creation of a Qulatrics project. Each project consists of a survey, distribution record, and collection of responses and reports. There are three ways to create a questionnaire.First, you can create a new survey project from scratch. Second, you can create a new questionnaire from a copy of an existing questionnaire. Eventually, you can create from a template in your Survey Library, or from an exported QSF file. In order to create a completely new questionnaire, you need to do the following: Go to the Projects page by clicking the Qualtric XM logo or clicking Projects on the top-right. Create new project by clicking the blue button on the right side. In the “Create your own” section click on the survey button. Enter a name for your survey and get started with a survey creation. If you would like to create a new questionnaire on a basis of an already existing one, then you choose “From a Copy”. Subseqeuntly, you need to indicate the questionnaire you would like to copy. Now you are good to go! If there is a questionnaire in the Qualtrics Library you would like to use, then you need to choose “From Library”, and indicate one library name in the dropdown menu. 10.3 Question types and data analysis In this chapter we will encounter the nature of data you collect when conducting a survey. It will help you choose a type of a question depending on the nature of data you want to collect and on the type of statistical tests you want to apply. Here you can find an example of a questionnaire in Qualtrics with guidelines and suggestions related to each question type. 10.3.1 Multiple choice with a single answer Multiple Choice with a single answer is a type of closed-ended question that lets respondents select one answer from a defined list of choices. Type of data you obtain is categorical, and the output comes in the following form: Table 10.1: Multiple Choice Question with Single Answer In a typical week, how many hours do you spend watching movies or TV series on Netflix? 3 4 5 4 5 2 What to do with this data now? First, we need to load it in R and prepare for analysis. The numbers you see in the output R recognizes as numeric. In order to conduct statistical modeling and properly visualize our results, we need to convert our data to a factor class. A factor (or coding variable) represents different groups of data by using numbers (integers). In fact, factors appear as numeric variables, but they hold meaning of labels/names of data groups, i.e. nominal variable. These data groups are represented in a form of ‘levels’. In our case, our multiple choice question output will contain 4 data groups (‘Grocery Store’, ‘Online shop’, ‘Specialised coffee shop’, ‘other’) after converting it to factor: # Convert numeric value to factors qualtrics$&#39;In a typical week, how many hours do you spend watching movies or TV series on Netflix?&#39; &lt;- factor(qualtrics$&#39;In a typical week, how many hours do you spend watching movies or TV series on Netflix?&#39;, levels = c(1:5), labels = c(&#39;Never&#39;,&#39;1-2 hours&#39;,&#39;3-4 hours&#39;,&#39;5-6 hours&#39;,&#39;more than 6 hours&#39;)) qualtrics$` Selected Choice_1` &lt;- factor(qualtrics$` Selected Choice_1`,levels = c(1:2),labels = c(&quot;Male&quot;,&quot;Female&quot;)) qualtrics$` Selected Choice` &lt;- factor(qualtrics$` Selected Choice`, levels = c(1:2), labels=c(&quot;Austria&quot;,&quot;Germany&quot;)) # Table table(qualtrics$&#39;In a typical week, how many hours do you spend watching movies or TV series on Netflix?&#39;) ## ## Never 1-2 hours 3-4 hours 5-6 hours ## 19 18 22 35 ## more than 6 hours ## 23 table(qualtrics$` Selected Choice`) #countries ## ## Austria Germany ## 35 82 table(qualtrics$` Selected Choice_1`) #gender ## ## Male Female ## 49 68 Second, you might want to visualize your results. In order to do so, the data format needs to be in the appropriate format.Here we proceed with data fromat adaptation from the point where we stopped: # Converting long format to the visualisation-friendly format mlc_visualisation &lt;- as.data.frame(table(qualtrics$&#39;In a typical week, how many hours do you spend watching movies or TV series on Netflix?&#39;)) # Naming columns names(mlc_visualisation) &lt;- c(&#39;Time&#39;,&#39;Count&#39;) # Observing knitr::kable(mlc_visualisation) Time Count Never 19 1-2 hours 18 3-4 hours 22 5-6 hours 35 more than 6 hours 23 The simplest way to visualize data obtained from multiple choice question with a single answer is a bar chart: ## Basic bar chart labels &lt;- as.character(mlc_visualisation$Time) #Save labels for x-axis in the barplot barplot(mlc_visualisation$Count, # Column to visualize xlab=&#39;Time&#39;, # X-axis label ylab = &#39;Count(answers)&#39;, # Y-axis label names.arg = labels, main = &#39;How many hours do you spend watching movies or series on Netflix?&#39;) # Title R package ggplot2 allows you to create visually appealing graphs: ## ggplot2 bar chart library(ggplot2) p &lt;- ggplot(data=mlc_visualisation, aes(x=Time, y=Count, fill=Time)) + geom_bar(stat=&#39;identity&#39;) + theme_minimal() + labs(title = &quot;In a typical week, how many hours do you spend watching movies or series on Netflix?&quot;) p Another R library which can help you make amazing interactive charts in a minute is plotly. Here we use a function called ggplotly(), which allows you to turn any ggplot2 chart interactive. Since we have already created a bar chart using ggplot2 and saved it as “p”, we will just turn it into plotly graph: ## ggplotly bar chart library(plotly) ggplotly(p) An improved version of ggplot2 package is the packaged called ggvis, which is still in developing: ## ggvis bar chart library(ggvis) ggvis(mlc_visualisation, x = ~Time, y = ~Count, fill=~Time) Renderer: SVG | Canvas Download Data type collected from the previous question is ordinal as we are able to make a natural order of the levels. Since it is ordinal data type, it belongs to categorical data. For the analysis of categorical data we can use Chi-square test or Fisher’s test if a count for some level is less than 5. 10.3.1.1 Fischer’s exact Fisher’s exact test is used to test a hypothesis with data obtained from multiple choice questions with single answer. Results from multiple choice questions with multiple answers are treated with different test. Application: when you have 1 dependent variable and 1 independent variable with 2 or more levels/factors Used when frequency in at least one cell is less than 5 . When frequencies in each cell are greater than 5, Chi-square test should be used. Hypothesis: Is there a significant difference in frequencies between values observed in cells and values expected in cells ? (R for Marketing and Research Analytics) H0: There is no relationship between the two categorical variables.Therefore, two categorical variables are independent. Knowing the value of one variable does not help to predict the value of the other variable. H1: There is a relationship between the two categorical variables.Therefore, two categorical variables are dependent.Knowing the value of one variable helps to predict the value of the other variable. Usually, this type of test is used on 2x2 contingency tables. However, it can be applicable on contingency tables of larger dimensions. Example: We would like to know whether a number of hours spent watching Netflix depends on the respondents’ country of origin. # Creation of contingency table fisher_test_table &lt;-table(qualtrics$` Selected Choice`,qualtrics$&#39;In a typical week, how many hours do you spend watching movies or TV series on Netflix?&#39;) # Check how our contigency table looks like fisher_test_table ## ## Never 1-2 hours 3-4 hours 5-6 hours more than 6 hours ## Austria 3 7 6 11 8 ## Germany 16 11 16 24 15 # Since we have a count less than 5, we should apply Fisher&#39;s test instead of Chi-square. # Fisher&#39;s test test &lt;- fisher.test(fisher_test_table) test ## ## Fisher&#39;s Exact Test for Count Data ## ## data: fisher_test_table ## p-value = 0.575 ## alternative hypothesis: two.sided # p-value test$p.value ## [1] 0.5750401 From the output and from test$p.value we see that the p-value is higher than the significance level of 5%. Like any other statistical test, if the p-value is higher than the significance level, we can not reject the null hypothesis. In our case, not rejecting the null hypothesis for the Fisher’s exact test of independence means that there is no significant relationship between the two categorical variables. Therefore, knowing the value of one variable does not help to predict the value of the other variable. 10.3.1.2 Chi-square test: Goodness of fit &amp; Independence test Goodness of fit Application: when you only have 1 dependent variable and none independent variables Hypothesis: Is there a significant difference in frequencies between values observed in cells and values expected in cells ? (R for Marketing and Research Analytics) H0: There is no significant difference between the observed and the expected frequencies. H1: There is a significant difference between the observed and the expected frequencies. If we don’t specify expected frequency per cell (see in the code below), then it is expected that all cells show an eqaul frequency. Example :‘Do the numbers of respondents who are spending different amount of hours watching Netflix significantly differ from each other?’ Note that we did not assume any specific distribution, so we are assuming that each count will have the same or similar number. # Creating table (mlc_chi_square &lt;- table(qualtrics$&#39;In a typical week, how many hours do you spend watching movies or TV series on Netflix?&#39;)) ## ## Never 1-2 hours 3-4 hours 5-6 hours ## 19 18 22 35 ## more than 6 hours ## 23 # Chi-square test (without given expected values = equal values ) chisq.test(mlc_chi_square) ## ## Chi-squared test for given probabilities ## ## data: mlc_chi_square ## X-squared = 7.9145, df = 4, p-value = 0.09476 The p-value of the test is higher than 0.05. We can conclude that the numbers of respondents who spent different amount of hours watching Netflix are commonly distributed. Observed distribution does not differ significantly from the expected. This result does not surprise if you take a look at the values for each level in the table we created before conducting the test. There you can see that count of answers in each level is more or less not deviating too much. It is visible if you take a look at the previous visualisations as well. If we are interested in testing more specific distribution, i.e. expect that 40% of our respondents are watching Netflix 3-4 hours, we can introduce corresponding distribution in the test. # Expected values in percentages for each alternative. The sum must be 1. expected_values &lt;- c(0.10, # We expect that 10% of our respondents do not watch Netflix at all (&quot;Never&quot;). 0.20, # We expect that 20% of our respondents watch Netflix 1-2 hours a week. 0.40, # We expect that 40% of our respondents watch Netflix 3-4 hours a week. 0.20, # We expect that 20% of our respondents watch Netflix 5-6 hours a week. 0.10 # We expect that 10% of our respondents watch Netflix more than 6 hours a week. ) # Chi-square test with expected values chisq.test(mlc_chi_square, p=expected_values) ## ## Chi-squared test for given probabilities ## ## data: mlc_chi_square ## X-squared = 35.607, df = 4, p-value = 0.0000003486 This time the p-value of the test is lower than 0.05.We have an evidence that observed distribution does significantly differ from the expected distribution (10%/20%/40%/20%/10%). Chi-Square Test of Independence Application: when you have 1 dependent variable and 1 independent variable with 2 or more levels/factors Hypothesis: Is there an association between categorical variable X and categorical variable Y? H0: There is no association between the two variables. H1: There is an association between the two variables. Example: Is there an association between gender and the hours spent watching Neflix during a week? # Creation of contingency table chi_square_table &lt;-table(qualtrics$` Selected Choice_1`,qualtrics$&#39;In a typical week, how many hours do you spend watching movies or TV series on Netflix?&#39;) # Chi-square independence test chisq.test(chi_square_table) ## ## Pearson&#39;s Chi-squared test ## ## data: chi_square_table ## X-squared = 1.5739, df = 4, p-value = 0.8135 Since the p-value (0.8135) is higher than the significance level (0.05), we cannot reject the null hypothesis. Thus, we conclude that there is no association relationship between gender and number of hours spent watching Netflix. Therefore, we can say that the hours spent is independent from the gender of participant. 10.3.2 Multiple choice with multiple answers Before we conduct any test, we will do some simple calculatios and visualise our data. # Rename columns colnames(qualtrics)[38] &lt;- &quot;ja!Naturlich&quot; colnames(qualtrics)[39] &lt;- &quot;Clever&quot; colnames(qualtrics)[40] &lt;- &quot;Spar Vital&quot; colnames(qualtrics)[41] &lt;- &quot;...&quot; # Replacing NA with 0 qualtrics$`ja!Naturlich`[is.na(qualtrics$`ja!Naturlich`)]=0 qualtrics$Clever[is.na(qualtrics$Clever)]=0 qualtrics$`Spar Vital`[is.na(qualtrics$`Spar Vital`)]=0 qualtrics$...[is.na(qualtrics$...)]=0 # Calculating frequency, percentage of respondents and percentage of cases df.cochran &lt;- data.frame(Frequnecy = colSums(qualtrics[38:41]), Share_of_respondents = (colSums(qualtrics[38:41])/sum(qualtrics[38:41]))*100, Share_of_cases =((colSums(qualtrics[38:41]))/nrow(qualtrics[38:41]))*100) # Observing df.cochran # Visualisation barplot(df.cochran[,3], names.arg = row.names(df.cochran), main = &quot;% of Respondents familiar with brands&quot;, xlab = &quot;Brand&quot;,ylab = &quot;Percentage&quot;) The visualisation above depicts the fact that more than 60% percent of people are familiar with the brand “ja!Naturlich”, while we can not say the same for other brands considered in our question. For the analysis of results collected with multiple choice question with multiple possible answers, we can use Cochran’s Q test.Although we did not mention it before, it is not too different from what you have already learned about other tests. The Cochran’s Q test and associated multiple comparisons require the following assumptions: 1. Responses are dichotomous and from k number of matched samples. 2. The subjects are independent of one another and were selected at random from a larger population. 3. The sample size is sufficiently “large”. (As a rule of thumb, the number of subjects for which the responses are not all 0’s or 1’s, n, should be ≥ 4 and nk should be ≥ 24) In a within-subjects experiment design with three or more observations of a dichotomous(= just two levels such as “Yes” or “No”) categorical outcome, you utilize Cochran’s Q test to assess main effects.Similarly, in our multiple choice question with multiple answers we have the same respondent going through three or more potential answers with dichotomous(=yes or no) categorical outcome. library(nonpar) # Creation of matrix matrix.cochran &lt;- cbind(qualtrics$`ja!Naturlich`, qualtrics$Clever, qualtrics$`Spar Vital`, qualtrics$`...`) # Turning NAs to 0 matrix.cochran[is.na(matrix.cochran)]=0 # Cochran test cochrans.q(matrix.cochran, alpha = 0.05) ## ## Cochran&#39;s Q Test ## ## H0: There is no difference in the effectiveness of treatments. ## HA: There is a difference in the effectiveness of treatments. ## ## Q = 25.5681818181818 ## ## Degrees of Freedom = 3 ## ## Significance Level = 0.05 ## The p-value is 0.0000117439848891232 ## There is enough evidence to conclude that the effectiveness of at least two treatments differ. ## The p-value less than 0.05 indicates that there is enough evidence to conclude that some of the store brands are better known among our respondents than other. In order to take a closer look at it, we need to conduct a post hoc test. library(DescTools) list.cochran &lt;- list(qualtrics$`ja!Naturlich`, qualtrics$Clever, qualtrics$`Spar Vital`, qualtrics$...) # imaginary brand # Replacing NAs in the list with 0 in order to be able to run the test list.cochran &lt;- rapply(list.cochran, f=function(x) ifelse(is.na(x),0,x), how=&quot;replace&quot; ) # Post hoc test (Dunn Test) DunnTest(list.cochran, method=&quot;bonferroni&quot;) ## ## Dunn&#39;s test of multiple comparisons using rank sums : bonferroni ## ## mean.rank.diff pval ## 2-1 -36 0.1093 ## 3-1 -18 1.0000 ## 4-1 -74 0.0000073 *** ## 3-2 18 1.0000 ## 4-2 -38 0.0761 . ## 4-3 -56 0.0014 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 From the results of the Dunn Test, we can see that there is a big difference between 1 (“ja!Natürlich”) and 4(“…”), as well as between 4(“…”) and 3(“Spar Vital”). 10.3.3 Rank order question A rank order question asks respondents to compare items to each other by placing them in order of preference. Note that the data obtained from a rank order question shows an order of a respondent’s pereference, but not the difference between items. For instance, if the the most important feature of a fitness tracker for a respondendt XY is “Measuring steps” and the second most important feature “Calories burned”, we don’t know for how much more important is the former one in comparison to the latter one. Intuitive question to ask is the following: which feature of the fitness tracker is the most important for our respondents? We can answer this question by calculating a mean rank for each feature. Before we do so, we will create a separate data frame and add columns of the response data. rank.data &lt;- data.frame(qualtrics$` Measuring steps`, qualtrics$` Calories burned`, qualtrics$` Measuring heartbeat`, qualtrics$` Exercise tracking`, qualtrics$` Measuring distance`) colnames(rank.data)&lt;-c(&quot;Measuring steps&quot;,&quot;Calories burned&quot;,&quot;Measuring heartbeat&quot;,&quot;Exercise tracking&quot;,&quot;Measuring distance&quot;) First information we would like to know is how many preference combinations there are, and how repetitive they are. We can obtain that information by creating a summary of the ranking data frame we created. library(pmr) test &lt;- rankagg(rank.data) test ## n ## [1,] 2 1 3 4 5 10 ## [2,] 1 3 2 4 5 19 ## [3,] 2 3 1 4 5 17 ## [4,] 1 2 4 3 5 4 ## [5,] 4 2 1 3 5 3 ## [6,] 3 2 1 5 4 15 ## [7,] 1 3 5 2 4 10 ## [8,] 1 2 4 5 3 10 ## [9,] 2 4 1 5 3 9 ## [10,] 1 2 5 4 3 9 ## [11,] 5 4 3 1 2 3 ## [12,] 2 3 4 5 1 8 The matrix we received as an output is the summary of our ranking data. It shows that, for instance, the preference combination “2,1,3,4,5” repeats 10 times in the data frame. More specifically, it means that there are 10 respondents who prefer the item 2(“Calories burned”) the most, then the item 1(“Measuring steps”), and so on. Now we can calculate the mean rank for each feature and conclude which feature is the most important to our respondents: # Mean rank of each fitness tracker feature destat(test)$mean.rank ## [1] 1.811966 2.581197 2.598291 4.051282 3.957265 As we can observe from the output, the item 1(“Measuring steps”) shows the best mean rank among all items. Therefore, we can assume that the “Measuring steps” is most important for our respondents. However, in order to statistically prove it and become sure that this is not just by mere chance, we can conduct Friedman rank sum test. Friedman rank sum test is used to identify whether there are any statistically significant differences between the distributions of 3 or more paired groups. It is used when the normality assumptions for using one-way repeated measures ANOVA are not met. Another case when Friedman rank rum test is used is when the dependent variable is measured on an ordinal scale, as in our case. Before we conduct the Friedman rank sum test, we will visualise our data: # We have just turned our data frame from the wide format to the long format by using function melt(). If we take a look at head and tail of our new data frame, we can see that it contains just two columns, &quot;Rank&quot; and &quot;Feature&quot;. rank.data.long &lt;- reshape2::melt(rank.data,value.name = &quot;Rank&quot;,variable.name = &quot;Feature&quot;, stringsAsFactors=TRUE) tail(rank.data.long) head(rank.data.long) # Visualisation ggstatsplot::ggwithinstats( data = rank.data.long, x = Feature, y = Rank, type = &quot;np&quot;, pairwise.comparisons = TRUE, # show pairwise comparison test results title = &quot;What features are important to you when evualting fitness trackers?&quot;) ## Note: 95% CI for effect size estimate was computed with 100 bootstrap samples. ## Note: Shapiro-Wilk Normality Test for Rank: p-value = &lt; 0.001 ## Note: Bartlett&#39;s test for homogeneity of variances for factor Feature: p-value = &lt; 0.001 Already from the advanced visualisation, that includes Friedman rank sum test and pairwise comparison, we can have an insight in significance of differences among features. # Friedman test friedman.test(as.matrix(rank.data)) ## ## Friedman rank sum test ## ## data: as.matrix(rank.data) ## Friedman chi-squared = 176.42, df = 4, p-value &lt; 0.00000000000000022 Friedman rank sum test has a p-value lower than 0.05, so we can conclude that here are significant differences between at least two features (what we have already seen in our visualisation). Even though we have identified differences between preferences towards features in our advanced visualisation, we will conduct a post hoc test in order to demonstrate traditional way of calculating pairwise comparisons. knitr::kable(wilcox_test(Rank ~ Feature, paired = TRUE, p.adjust.method = &quot;bonferroni&quot;, data = rank.data.long)) .y. group1 group2 n1 n2 statistic p p.adj p.adj.signif Rank Measuring steps Calories burned 117 117 1369.0 0.000000 0.000 **** Rank Measuring steps Measuring heartbeat 117 117 2231.0 0.000753 0.008 ** Rank Measuring steps Exercise tracking 117 117 354.0 0.000000 0.000 **** Rank Measuring steps Measuring distance 117 117 367.5 0.000000 0.000 **** Rank Calories burned Measuring heartbeat 117 117 3214.5 0.512000 1.000 ns Rank Calories burned Exercise tracking 117 117 610.5 0.000000 0.000 **** Rank Calories burned Measuring distance 117 117 940.0 0.000000 0.000 **** Rank Measuring heartbeat Exercise tracking 117 117 1235.0 0.000000 0.000 **** Rank Measuring heartbeat Measuring distance 117 117 1307.5 0.000000 0.000 **** Rank Exercise tracking Measuring distance 117 117 3534.5 0.816000 1.000 ns The output table provides us with p-values referring to significance of difference in mean ranks of each pair. For instance, the first 4 rows proves that the differences between the mean rank of the feature “Measuring steps” and each of the rest of features are significant. Consequently, we can conclude that this feature is by far the most important among our respondents. Another question that may be interesting to explore is whether there are any complementary features ? Or features which overlap each other in its functionality? In order to have a look at that, we can investigate the correlation between ranks assigned to each feature. #Correlation Matrix cor.matrix&lt;-cor(rank.data, method=c(&#39;spearman&#39;)) cor.matrix ## Measuring steps Calories burned Measuring heartbeat ## Measuring steps 1.00000000 -0.04651331 -0.6569094 ## Calories burned -0.04651331 1.00000000 -0.2221626 ## Measuring heartbeat -0.65690943 -0.22216264 1.0000000 ## Exercise tracking 0.29633223 -0.10838758 -0.3255840 ## Measuring distance -0.05958032 -0.11694481 -0.3817895 ## Exercise tracking Measuring distance ## Measuring steps 0.2963322 -0.05958032 ## Calories burned -0.1083876 -0.11694481 ## Measuring heartbeat -0.3255840 -0.38178948 ## Exercise tracking 1.0000000 -0.47176821 ## Measuring distance -0.4717682 1.00000000 At the first glance we can observe a lot of negative values, meaning that many features correlate negatively relative to each other. In order to make the interpretation easier, we will try to visualise correlations in a form of a correlation matrix. library(ggcorrplot) ggcorrplot(cor.matrix) From the correlation matrix we can confirm that almost all features negatively correlate to each other. An exception is the relationship between feature “Measuring steps” and “Exercise tracking”, which correlates positively. This matrix can be useful for digging deeper in relationship between preferences for features. For instance, we can assume that feature “Measuring steps” and “Exercise tracking” correlate positively because users see them as complementary features. Moreover, if we say that walking is a type of exercise (in case of longer walking routes), we can assume that users, who ranked “Exercise tracking” high, ranked “Measuring steps” high as well, because they perceive it as another type of “Exercise tracking”. 10.3.4 Constant Sum question If you wish to obtain information about how much one attribute is preferred over another one, you may use a constant sum scale. The total box should always be displayed at the bottom to make it easier for respondents.A constant sum question permits collection of ratio data type. With data obtained we would be able to express the relative importance of the options. Table 10.2: Constant Sum Question Location Price Ambience Customer Service id 32 23 32 13 1 25 30 22 23 2 19 21 30 30 3 20 20 20 40 4 30 30 10 30 5 0 20 20 60 6 # Compute descriptive statistics library(pastecs) res &lt;- stat.desc(constant.sum) round(res[,1:4],2) # Creation of the long version of data frame constant.sum.long &lt;-melt(constant.sum[,-5], variable.name =&quot;Factor&quot; ,value.name = &quot;Points&quot;) constant.sum.long # Boxplot ggplot2 p&lt;-constant.sum.long %&gt;% filter(Factor!=&quot;id&quot;) %&gt;% ggplot(aes(x=Factor, y=Points, fill= Factor)) + geom_boxplot() + theme_minimal() + ggtitle(&quot;What factors do you consider when choosing a place to go for a dinner?&quot;) + xlab(&quot;&quot;) ggplotly(p) With the data collected we are able to answer the question: what factor is the most important for our respondents when they go out for a dinner? library(robCompositions) constSum(constant.sum,100)[,-5] In order to anwser this question we need to conduct a repeated measures ANOVA. This type of ANOVA is used for analyzing data where the same subjects are measured more than once. In our case we have every respondent measured on each of the factors (locations, price, ambience and customer service). Repeated measures ANOVA is an extension of the paired-samples t-test. This test is also referred to as a within-subjects ANOVA. In the within-subject experimental design the same individuals are measured on the same outcome variable under different time points or conditions. We need to check all assumptions that need to be fulfilled in order to deploy this type of ANOVA. There are three assumputions that need to check. The first to check that each level of the independent variable is approximately normally distributed. Since we have more than 30 observations at each level, we do not need to proceed further due to the central limit theorem. Second assumption referrs to extreme outliers. Let’s have a look at potential outliers: # Outliers constant.sum.long %&gt;% group_by(Factor) %&gt;% identify_outliers(Points) As we cannot identify any extreme outliers, we can proceed with deploying repeated measures ANOVA. # Formatting data constant.sum.aov &lt;- gather(constant.sum, key = &quot;Factor&quot;, value = &quot;Points&quot;, ` Location`,` Price`,` Ambience`,` Customer Service`) # One-way repeated measures ANOVA res.aov &lt;- anova_test(data = constant.sum.aov, dv = Points,wid = id ,within = Factor) get_anova_table(res.aov) # Post hoc test pairwise.t.test(constant.sum.long$Points,constant.sum.long$Factor, paired = T, p.adjust.method = &quot;holm&quot;) ## ## Pairwise comparisons using paired t tests ## ## data: constant.sum.long$Points and constant.sum.long$Factor ## ## Location Price Ambience ## Price 0.0000000000000027 - - ## Ambience 0.0000000003153773 0.030 - ## Customer Service &lt; 0.0000000000000002 0.742 0.079 ## ## P value adjustment method: holm Now we can clearly see that our respondents consider price more than location, or ambience, while customer service is perceived almost equally important as prices. ggstatsplot::ggwithinstats( data = constant.sum.long %&gt;% filter(Factor!=&quot;id&quot;), # excluding &quot;id&quot; column from the data x = Factor, y = Points, type = &quot;p&quot;, pairwise.comparisons = TRUE, # show pairwise comparison test results title = &quot;What factors do you consider when choosing a place to go for a dinner?&quot;) ## Note: Shapiro-Wilk Normality Test for Points: p-value = &lt; 0.001 ## Note: Bartlett&#39;s test for homogeneity of variances for factor Factor: p-value = &lt; 0.001 10.3.5 Text or number entry question A text or number entry question is a recommended type of question if you are interested in obtaining ratio data type. We will use this type of question together with a constant sum question type to collect data that can be analysed with regression analysis. Note that in this case we treat constant sum data as ratio data and therefore assume that 0 means complete absence. Here is a glimpse in answers on how important is each factor to our respondents when it comes to dinning outside: Table 10.3: Constant sum question Location Price Ambience Customer Service 32 23 32 1 25 30 22 43 19 21 30 34 20 20 20 46 30 30 10 17 0 20 20 4 Additionally, we asked our respondents how much are they willing to spend on dinner on average. In order to handle data easier, we will create a new data frame where we merge all the data together: dinner &lt;- subset(qualtrics, select = c(&quot; Location&quot;,&quot; Price&quot;,&quot; Ambience&quot;,&quot; Customer Service&quot;, &quot; Willingness-to-pay (in EUR)&quot;)) knitr::kable(head(dinner)) Location Price Ambience Customer Service Willingness-to-pay (in EUR) 32 23 32 1 29 25 30 22 43 77 19 21 30 34 52 20 20 20 46 31 30 30 10 17 22 0 20 20 4 35 Before we conduct a linear regression analysis, we need to take a look at correlation matrix: correlation &lt;-cor(dinner, method=c(&#39;pearson&#39;)) correlation ## Location Price Ambience ## Location 1.0000000 -0.31732620 -0.36134355 ## Price -0.3173262 1.00000000 -0.21962027 ## Ambience -0.3613436 -0.21962027 1.00000000 ## Customer Service -0.1668810 0.08894752 -0.02405881 ## Willingness-to-pay (in EUR) 0.1414540 -0.07438388 -0.32550607 ## Customer Service Willingness-to-pay (in EUR) ## Location -0.16688104 0.14145397 ## Price 0.08894752 -0.07438388 ## Ambience -0.02405881 -0.32550607 ## Customer Service 1.00000000 0.12125571 ## Willingness-to-pay (in EUR) 0.12125571 1.00000000 From our data we see, for instance, that some negative correlation between willingness to pay and importance of ambiance as well as some positive correlation between importance of customer service and willingness-to-pay. Let us observe descriptive statistics as well: knitr::kable(psych::describe(dinner)) vars n mean sd median trimmed mad min max range skew kurtosis se Location 1 117 12.14530 10.85823 10 11.25263 14.8260 0 40 40 0.3585257 -0.8903393 1.003844 Price 2 117 31.48718 16.22079 30 29.83158 14.8260 0 100 100 1.5662904 4.1917874 1.499613 Ambience 3 117 25.76068 13.97822 20 25.09474 14.8260 0 60 60 0.3807401 -0.3100357 1.292286 Customer Service 4 117 49.35897 29.47777 47 49.29474 40.0302 0 98 98 0.0342022 -1.1897398 2.725221 Willingness-to-pay (in EUR) 5 117 32.99145 26.26801 30 30.28421 29.6520 0 110 110 0.8007002 0.0124325 2.428479 We see that difference between mean and median does not suggest (at the first sight) great effect of outliers. Let us now do linear regression analysis: mlr.dinner &lt;- lm(` Willingness-to-pay (in EUR)` ~ ` Location` + ` Price` + ` Ambience`+` Customer Service`, data = dinner) summary(mlr.dinner) ## ## Call: ## lm(formula = ` Willingness-to-pay (in EUR)` ~ ` Location` + ` Price` + ## ` Ambience` + ` Customer Service`, data = dinner) ## ## Residuals: ## Min 1Q Median 3Q Max ## -40.810 -18.205 -3.314 14.059 74.274 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 55.31553 11.57393 4.779 0.00000538 *** ## ` Location` -0.06739 0.25556 -0.264 0.792503 ## ` Price` -0.28455 0.16117 -1.765 0.080205 . ## ` Ambience` -0.69755 0.19088 -3.654 0.000394 *** ## ` Customer Service` 0.10988 0.07931 1.386 0.168646 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 24.72 on 112 degrees of freedom ## Multiple R-squared: 0.1449, Adjusted R-squared: 0.1144 ## F-statistic: 4.745 on 4 and 112 DF, p-value: 0.001421 Out of all factors of importance when dinning out, the only one that suggests significance at 0.05 level of significance is ambience. From the summary we can conclude that increase in importance of ambience by 1 point, leads to decrease in willingness to pay by -0.697554. confint(mlr.dinner) ## 2.5 % 97.5 % ## (Intercept) 32.38327198 78.24779707 ## ` Location` -0.57374787 0.43897062 ## ` Price` -0.60389395 0.03479312 ## ` Ambience` -1.07575993 -0.31934814 ## ` Customer Service` -0.04725424 0.26701295 From confidence intervals, We can conclude that when we do not consider any of given factors (location, price, ambience and customer service), willingness to pay will be somewhere between 32.383272EUR and 78.2477971EUR. Besides that, for each increase in importance of dinner ambiance by one point, there will be an average decrease of willingness to pay between -1.0757599 and -0.3193481. ggcoefstats(x = mlr.dinner, title = &quot;Willingness to pay predicted by importance of factors&quot;) There are couple of things we need to consider when we do multiple linear regression. One of them are potential outliers in our data. Here we identify and visualize them: # Outliers outlier_values &lt;- boxplot.stats(mlr.dinner$residuals)$out # outlier values. outlier_values ## 12 44 49 ## 70.56037 64.19796 74.27359 We identified observations that belong to outlier values. We can even visualize them too: boxplot(mlr.dinner$residuals, main=&quot;Willingnes to pay&quot;, boxwex=0.1) In addition, we need to observe whether there are any influential observations: plot(mlr.dinner,4) A rule of thumb to determine whether an observation should be classified as influential or not is to look for observation with a Cook’s distance &gt; 1 .We see from the graph that there are no influential observations. Another thing to consider is linearity, i.e. that the relationship between the dependent and the independent variable can be reasonably approximated in linear terms: # Linear specification library(car) avPlots(mlr.dinner) In our example it does not seem that linear relationships can be reasonably assumed for all variables. As we already learned, another important assumption of the linear model is that the error terms have a constant variance (i.e., homoscedasticity): # Breusch-Pagan Test library(lmtest) bptest(mlr.dinner) ## ## studentized Breusch-Pagan test ## ## data: mlr.dinner ## BP = 1.1478, df = 4, p-value = 0.8866 The null hypothesis for this test is that the error variances are all equal, and our result is insignificant. Therefore, this assumption is met. Another assumption to be met is that the error term is normally distributed. One way to check for normal distribution of the data is to employ statistical with the null hypothesis that the data is normally distributed. One of these is a Shapiro–Wilk test: shapiro.test(resid(mlr.dinner)) ## ## Shapiro-Wilk normality test ## ## data: resid(mlr.dinner) ## W = 0.94757, p-value = 0.0001763 When the assumption of normally distributed errors is not met (as it is not met in our case), this might again be due to a misspecification of your model, in which case it might help to transform your data. Finally, we need to check for multicollinearity, the case when there is a strong linear relationship between the independent variables: correlation &lt;-cor(dinner, method=c(&#39;pearson&#39;)) correlation ## Location Price Ambience ## Location 1.0000000 -0.31732620 -0.36134355 ## Price -0.3173262 1.00000000 -0.21962027 ## Ambience -0.3613436 -0.21962027 1.00000000 ## Customer Service -0.1668810 0.08894752 -0.02405881 ## Willingness-to-pay (in EUR) 0.1414540 -0.07438388 -0.32550607 ## Customer Service Willingness-to-pay (in EUR) ## Location -0.16688104 0.14145397 ## Price 0.08894752 -0.07438388 ## Ambience -0.02405881 -0.32550607 ## Customer Service 1.00000000 0.12125571 ## Willingness-to-pay (in EUR) 0.12125571 1.00000000 By observing our correlation matrix, we can see that non of the coefficients suggest values close to 0.8 or 0.9. Consequently, we conclude that there are no concerns regarding the multicolinearity between independent variables. "],
["xgboost.html", "11 XGBoost 11.1 What is machine learning? 11.2 Gradient Boosting 11.3 xgboost package", " 11 XGBoost References: XGBoost: A Scalable Tree Boosting System Hands on machine-learning with R Predicting marketing performance with ML A data-driven approach to predict the success of bank telemarketing Extensive tutorial Gradient Boosting Essentials in R Using XGBOOST A Gentle Introduction to XGBoost for Applied Machine Learning Gradient Boosting in R 11.1 What is machine learning? In essence, the road to machine learning starts with regression. Some typical use cases of machine learning in marketing are: Segmenting customers based on common attributes or purchasing behavior for targeted marketing Predicting coupon redemption rates for a given marketing campaign Predicting customer churn so an organization can perform preventative intervention In its core, these tasks all seek to learn from data. In order to do so, we use a given set of features to train an algorithm and extract information we need. These algorithms, known as learners too, can be divided according to the amount and type of supervision needed during training. We distinguish supervised learners which construct predictive models, and unsupervised learners which build descriptive models. Which type you will need to use depends on the learning task you hope to accomplish. In this chapter we will focus on supervised machine learning, more specifically, on one method called extreme gradient boosting. 11.2 Gradient Boosting When we talk about machine learning, there are two quite important factors that drive successful application: effective statistical models that are capable of capturing the complex data dependencies and how scalable are learning systems that learn the model from large datasets. Among the machine learning methods commonly used in practice are gradient tree boosting methods. Gradient boosting machines (GBMs) are a very popular machine learning method, and in this chapter we will introduce you R package “xgboost” and show how it can be used for marketing purposes. It is a scalable machine learning system for tree boosting and GMBs have proven successful across many domains and are one of the leading methods you can find across Kaggle competitions. When it comes to marketing, it can be used in uplift modeling, i.e. can help a company to identify those who are likely to buy products as a result of receiving a discount or a personalized advertisement. Consequently, it helps a company to maximize profits by keeping advertising costs and overall efforts to the minimum. In the perspective of data analysis and marketing, performance of a marketing campaign can be predicted using algoritham such as GBMs. For instance, in the banking industry optimizing targeting for telemarketing used to be one of the main issues, especially under a growing pressure induced by financial crisis in 2008. A commercial bank from Portugal used data-driven model to predict the result of a telemarketing phone call to sell long term deposits. It was a valuable tool to support client selection decisions made by bank managers. As a result, they identified that inbound calls and an increase in other, highly relevant attributes (such as agent experience or duration of pre-vious calls) enhance the probability for a successful deposit sell. 11.2.1 Elements of supervised machine learning XGBoost is used in supervised machine learning. Let us decompose the meaning of supervised machine learning. Supervised machine learning can be described as a process in which training data with multiple features (also called: predictor variables, independent variables, attributes, predictor) is used to predict a target variable (also called, dependent variable, response, outcome measurement). The final outcome of supervised machine learning is a predictive model, so it is important to define what a model is. A model, in the context of supervised machine learning, contains a mathematical structure or algorithm by which the prediction of a target variable is made from the multiple features used as input. For instance, algorithm that helps you to predict the sale price of your house based on the house attributes. Another important term in the context of machine learning are parameters. They denote undetermined part that we need to learn from data. Finally, as we said, models we build with supervised machine learning are predictive models. “Supervised” refers to a supervisory role of the target variable, which indicates the task that model needs to learn. More specifically, it means that the data which is used for training a model contains target variable. Given a set of training data, the learning algorithm attempts to find the combination of feature values that results in a predicted value as close to the actual target variable as possible. 11.2.2 Principle behind boosting Boosting can be explained as a sequential process. That means that at each particular iteration, a new weak model is trained with respect to the error of the whole ensemble learned by that time. A weak model is one whose predictions (error rate) are only slightly better than random guessing. In simple words, in each iteration a better model is created by adding a new weak model to the existing one, where the purpose of the weak model is to slightly improve the remaining errors of the existing model. This process slowly learns from data and tries to improve its prediction in subsequent iterations. Among other, boosting is used for solving both regression and classification problems. Below you can find an illustrative example and explanation for each of them. In the illustration above you can see 4 boxes with pluses and minuses within them representing observations. The ultimate goal of a model we need to develop is of classification nature, i.e. to classify pluses (“+”) and minuses (-) within a box as accurate as possible. It is important to mention that at the begining all observations are assigned equal weights. However, weights are subject to change after each iteration as misclassified observations in one iteration will be assigned higher weight in the next one. Opposingly, observations that are correctly classified in one iteration will be assigned lower weight in the subsequent iteration. In the box 1, the first weak learner identified “+” signs just on the left side of the box. It simply misclassified three “+” signs in the middle “-” upper part and recognized only the two on the left side. Consequently, it split the box in two parts (blue and light red), meaning that everything that appears in the blue “-” marked area is classified as “+”, while the rest is classified as “-”. Although our prediction model at this point does not do great job, it contains information useful for the next weak learner that is being added in the box 2. Next weak learner assigns more weight to three “+” signs that were previously misclassified. Similarly to the previous split, the weak learner split the box 2 again in blue “-” and red “-” marked area. Again, everything in the blue area (left from the splitting line) was classified as “+”, including three minus signs being misclassified. The rest was classified as -\". Even though our predicting model looks a bit better, its classification is still incorrect. In the box 3, our model is becoming even better in classifying. It splitted the box horizontally, so that everything below the line was classified as “-” and above the line as “+”. Despite the progress we still have some missclassified “-” in the blue-marked area as well as wrongly classified “+” below the splitting line (circled signs in the box 3). Finally, in the box 4 we see the result from combining information obtained from numerous weak learners. It is a weighted combination of the weak classifiers resulting in a strong classifier.Each classifier individually proved pretty poor performance in predicting, as they all show certain misclassification error. However, after combining them, the ultimate goal to classify all points correctly is reached and strong classifier created. Figure 11.1: Boosted regression tree predictions (courtesy of Brandon Greenwell) To understand the whole concept easier, try to follow the image above. On the one hand, the blue curve depicts the real underlying function, while the points depict observations. Moreover, observations include some noise, i.e. errors. On the other hand, you can observe red curve representing constantly improving boosted prediction. More specifically, it illustrates the adjusted predictions after each additional sequential tree is added to the algorithm. At the beginning, you can observe large errors (.i.e. big deviation of the red curve from the blue one) which the boosted algorithm reduces pretty fast. However, as the predictions (.i.e. red curve) get closer to the true underlying function (i.e.blue curve), the contribution to model improvement of each additional tree is smaller and smaller. In the end, the predicted values nearly match to the true underlying function. 11.3 xgboost package There is an extensive list of packages with GBMs and its variations. However, the most popular implementations which we will cover here is certainly xgboost, which is quite fast and efficient. 11.3.1 Introduction XGboost stands for “Extreme Gradient Boosting”, which is an efficient implementation of gradient boosting framework. The xgboost package has been quite popular and successful on Kaggle for data mining competitions. # Turn off scientific notation options(scipen = 9999) # Helper packages library(dplyr) # for general data wrangling needs # Modeling packages library(xgboost) # for fitting extreme gradient boosting library(rsample) # for split of data set in the training data and test data library(AmesHousing)# data set library(caret) # for resampling and model training library(plotly) library(recipes) library(pdp) library(knitr) library(gbm) library(mlr) library(ggplot2) To explain gradient boosting with xgboost, we will use typical Ames Iowa Housing data set and try to build a model that predict sale price for houses. # Ames housing data ames &lt;- AmesHousing::make_ames() # Ensure correct naming library(janitor) ames&lt;- ames%&gt;%clean_names() # Use mlr package to get an overview of your data knitr::kable(head(summarizeColumns(ames))) name type na mean disp median mad min max nlevs ms_sub_class factor 0 NA 0.6317406 NA NA 1 1079 16 ms_zoning factor 0 NA 0.2242321 NA NA 2 2273 7 lot_frontage numeric 0 57.64778 33.4994408 63.0 25.2042 0 313 0 lot_area integer 0 10147.92184 7880.0177594 9436.5 3024.5040 1300 215245 0 street factor 0 NA 0.0040956 NA NA 12 2918 2 alley factor 0 NA 0.0675768 NA NA 78 2732 3 11.3.2 Data preparation First, we need to deal with data preparation. When using xgboost package, it is necessary to convert the categorical variables into numeric using one hot encoding. What is one hot encoding? Usually, when between several categories exist ordinal relationship (e.g. variable “place” can be “1st”, “2nd” and so on), all you need to do is so called the integer encoding. In the ordinal variable (such as “place”) the integer values have a natural ordered relationship between each other, so machine learning algorithms are able to understand this relationship. However, for categorical variables where no ordinal relationship exists (e.g. variable “pet” with “dog”, “cat” and “rabbit”), the integer encoding is not sufficient and you need one hot encoding. In the “pet” example, there are 3 categories, thus 3 binary variables are required. Therefore, “1” value is placed in the binary variable for the respective “color” and “0” values for the other colors. cat(tabl) dog cat rabbit 1 1 0 0 2 0 1 0 3 0 0 1 We can apply one hot encoding to our data set by using R’s base function model.matrix. In the code below, ~.+0 leads to encoding of all categorical variables without producing an intercept. # One hot encoding (turning the test data into matrix with all numerical values) ames.he &lt;- model.matrix(~.+0,data = ames) # Save variable names due to more practical addressing columns later on setcol &lt;- colnames(ames.he) Next,we need to break our data set into training and test data, while ensuring we have consistent distributions between the training and test sets. # Data split on test and train data set.seed(1234) # Create partition library(caret) index &lt;- caret::createDataPartition(ames$sale_price, p = 0.7, list = F) index is a matrix with just one column that contains approximately 70% of rows (i.e. numbers of rows) from our original data set. Now we use index to address columns we want to assign to our train data (ames_train), and columns that we want to assign to our test data (ames_test). Note that by using “-” in front of index we assign to ames_test all observations except those that are in index. # Split data set in train and test data ames_train &lt;- ames.he[index, ] ames_test &lt;- ames.he[-index, ] If we take lake look at the dimensions of our test and train data, we can see that our split was successful. dim(ames.he) # 2930 observations in total dim(ames_train)# 2053 observations for training data dim(ames_test) # 877 observations for testing data We are still not done with preparation of data. Since our task is to build a predictive model for house pricing based on multiple features, our target variable (sale price) needs to be excluded from the test data. The “real” target variable data (the real sales price) will be used at the end when we test accuracy of our predictive model. # Test data ## Matrix containing all columns from the test data except dependent variable &quot;Sale_Price&quot; colnames(ames_test[,300:308])# &quot;sale_price&quot; is a column number 308 ## [1] &quot;sale_typeVWD&quot; &quot;sale_typeWD &quot; &quot;sale_conditionAdjLand&quot; ## [4] &quot;sale_conditionAlloca&quot; &quot;sale_conditionFamily&quot; &quot;sale_conditionNormal&quot; ## [7] &quot;sale_conditionPartial&quot; &quot;sale_price&quot; &quot;longitude&quot; # Addressing &quot;Sale Price&quot; column in matrix and excluding it ames_x_test &lt;- as.matrix(ames_test[,-308]) # No &quot;Sale Price&quot; anymore here! It used to be among last columns. colnames(ames_x_test)[300:308] ## [1] &quot;sale_typeVWD&quot; &quot;sale_typeWD &quot; &quot;sale_conditionAdjLand&quot; ## [4] &quot;sale_conditionAlloca&quot; &quot;sale_conditionFamily&quot; &quot;sale_conditionNormal&quot; ## [7] &quot;sale_conditionPartial&quot; &quot;sale_price&quot; &quot;latitude&quot; For training purposes, target variable (sale price) needs to be excluded from the rest of features, but not totally from the train data set. # Train data set # Matrix containing all columns from the train data except dependent variable &quot;Sale_Price&quot; colnames(ames_train[,300:308])# &quot;sale_price&quot; is a column number 308 ## [1] &quot;sale_typeVWD&quot; &quot;sale_typeWD &quot; &quot;sale_conditionAdjLand&quot; ## [4] &quot;sale_conditionAlloca&quot; &quot;sale_conditionFamily&quot; &quot;sale_conditionNormal&quot; ## [7] &quot;sale_conditionPartial&quot; &quot;sale_price&quot; &quot;longitude&quot; # Addressing &quot;Sale Price&quot; column in matrix and excluding it ames_x_train &lt;- as.matrix(ames_train[,-308]) # No &quot;Sale Price&quot; anymore here! colnames(ames_x_train[,300:308]) ## [1] &quot;sale_typeVWD&quot; &quot;sale_typeWD &quot; &quot;sale_conditionAdjLand&quot; ## [4] &quot;sale_conditionAlloca&quot; &quot;sale_conditionFamily&quot; &quot;sale_conditionNormal&quot; ## [7] &quot;sale_conditionPartial&quot; &quot;sale_price&quot; &quot;latitude&quot; Therefore, the target variable is going to be stored separately because the learning algorithm in a predictive model attempts to discover and model the relationships among the target variable and the other features. # Dependent/Target variable &quot;Sales_Price&quot; from the train data in a from of a vector ames_y_train &lt;- ames_train[,308] 11.3.3 Engineering In order to create a good predictive model, usually the most of time is spent optimizing parameters. Before we start training our model, let us take a closer look at what parameters we need to handle. There are 3 categories XGBoost parameters can be divided into: General parameters Boosting parameters Tree-specific parameters General parameters will not be discussed in further details, but it consists of 3 parameters: booster - determines the booster type (gbtree, gblinear or dart) to use. For regression, you can use any. By default it is gbtree (which we will use as well). nthread - refers to the number of cores activated when computing. By default it uses maximum cores available, which leads to the fastest computation. silent - refers to turning on (“1”) running messages in R console. By default “0” is set, so that console does not get flooded with messages. For general parameters we will be using default options. Next, booster parameters control the performance of the selected booster(gbtree in our case). At this moment we will introduce just the main ones: nrounds - set the maximum number of iterations. eta - stands for the learning rate. It determines the rate at which our model learns patterns in data. After every iteration, it shrinks the feature weights to reach the best optimum. Smaller learning rates lead to longer computation time. It is important to note that smaller learning rates should be supported by increasing number of iterations. Otherwise, the risk of reaching the optimum is more likely. Usually, it lies between 0.01 - 0.3. max_depth - which determines the maximum depth of each tree. Generally, it is stands that larger the depth, more complex the model and consequently higher chances of overfitting. 4.min_child_weight - minimum number of observations required in each terminal node subsample - percent of training data to sample for each tree colsample_bytrees - percent of columns to sample from for each tree early_stopping_rounds - stopping the training model as soon as evaluation metric (for regression that is “RMSE”) does not improve for a given number of rounds Finally, learning task parameters define methods for the loss function and model evaluation: objective- for linear regression it should be set to “reg:linear”. eval_metric - this parameter depends on objective. Here we set metrics used to evaluate a model’s accuracy on validation data. When “reg:linear” set as objective, default metric is RMSE. A package with useful tools for parameter optimization is mlr. It includes extensive list of parameters for any type of algorithm. We can take a look at list with parameters for regression and check parameters we just discussed. # Parameters for regression getParamSet(&quot;regr.xgboost&quot;) ## Type len Def ## booster discrete - gbtree ## watchlist untyped - &lt;NULL&gt; ## eta numeric - 0.3 ## gamma numeric - 0 ## max_depth integer - 6 ## min_child_weight numeric - 1 ## subsample numeric - 1 ## colsample_bytree numeric - 1 ## colsample_bylevel numeric - 1 ## colsample_bynode numeric - 1 ## num_parallel_tree integer - 1 ## lambda numeric - 1 ## lambda_bias numeric - 0 ## alpha numeric - 0 ## objective untyped - reg:linear ## eval_metric untyped - rmse ## base_score numeric - 0.5 ## max_delta_step numeric - 0 ## missing numeric - ## monotone_constraints integervector &lt;NA&gt; 0 ## tweedie_variance_power numeric - 1.5 ## nthread integer - - ## nrounds integer - - ## feval untyped - &lt;NULL&gt; ## verbose integer - 1 ## print_every_n integer - 1 ## early_stopping_rounds integer - &lt;NULL&gt; ## maximize logical - &lt;NULL&gt; ## sample_type discrete - uniform ## normalize_type discrete - tree ## rate_drop numeric - 0 ## skip_drop numeric - 0 ## scale_pos_weight numeric - 1 ## refresh_leaf logical - TRUE ## feature_selector discrete - cyclic ## top_k integer - 0 ## predictor discrete - cpu_predictor ## updater untyped - - ## sketch_eps numeric - 0.03 ## one_drop logical - FALSE ## tree_method discrete - auto ## grow_policy discrete - depthwise ## max_leaves integer - 0 ## max_bin integer - 256 ## callbacks untyped - ## Constr Req Tunable Trafo ## booster gbtree,gblinear,dart - TRUE - ## watchlist - - FALSE - ## eta 0 to 1 - TRUE - ## gamma 0 to Inf - TRUE - ## max_depth 0 to Inf - TRUE - ## min_child_weight 0 to Inf - TRUE - ## subsample 0 to 1 - TRUE - ## colsample_bytree 0 to 1 - TRUE - ## colsample_bylevel 0 to 1 - TRUE - ## colsample_bynode 0 to 1 - TRUE - ## num_parallel_tree 1 to Inf - TRUE - ## lambda 0 to Inf - TRUE - ## lambda_bias 0 to Inf - TRUE - ## alpha 0 to Inf - TRUE - ## objective - - FALSE - ## eval_metric - - FALSE - ## base_score -Inf to Inf - FALSE - ## max_delta_step 0 to Inf - TRUE - ## missing -Inf to Inf - FALSE - ## monotone_constraints -1 to 1 - TRUE - ## tweedie_variance_power 1 to 2 Y TRUE - ## nthread 1 to Inf - FALSE - ## nrounds 1 to Inf - TRUE - ## feval - - FALSE - ## verbose 0 to 2 - FALSE - ## print_every_n 1 to Inf Y FALSE - ## early_stopping_rounds 1 to Inf - FALSE - ## maximize - - FALSE - ## sample_type uniform,weighted Y TRUE - ## normalize_type tree,forest Y TRUE - ## rate_drop 0 to 1 Y TRUE - ## skip_drop 0 to 1 Y TRUE - ## scale_pos_weight -Inf to Inf - TRUE - ## refresh_leaf - - TRUE - ## feature_selector cyclic,shuffle,random,greedy,thrifty - TRUE - ## top_k 0 to Inf - TRUE - ## predictor cpu_predictor,gpu_predictor - TRUE - ## updater - - TRUE - ## sketch_eps 0 to 1 - TRUE - ## one_drop - Y TRUE - ## tree_method auto,exact,approx,hist,gpu_hist Y TRUE - ## grow_policy depthwise,lossguide Y TRUE - ## max_leaves 0 to Inf Y TRUE - ## max_bin 2 to Inf Y TRUE - ## callbacks - - FALSE - To build a well-performing predictive model many iterations are necessary. Therefore, in order to determine how good or bad one model predicts the target variable, performance evaluation needs to be conducted. A technique that will be used to help us in evaluating performance of our future machine learning models is called k-fold cross-validation technique. K-fold cross-validation evaluates a model by training a couple of models on subsets of the available input data and evaluating them on the complementary subset of the data. In this process, training data is split into k groups (i.e. folds) of approximately equal size. Then the model is fit on k−1 folds and the remaining fold is used in computation of the model performance. This procedure is repeated k times, where each time, a different fold is treated as the validation set (i.e. used in computation of the model performance). Thus, the final cross-validation k-fold estimate is computed by averaging the k test errors. The final output provided is an approximation of the error we may expect on unseen data. The first model to pass to the k-fold cross validation will be built using default parameters. As default for number of iterations (nrounds) is zero, we will set it on 200. set.seed(1234) ames_xgb &lt;- xgb.cv( data = ames_x_train, # matrix with train data without sale price label = ames_y_train, # numerical vector with sale price with train data nrounds = 200, # number of iterations objective = &quot;reg:linear&quot;, # parameter referring to the function to be me minimised (RMSE) nfold = 10, # data is randomly partitioned into n-fold equal size subsamples. params = list( # defining the list of parameters eta = 0.3, # learning rate max_depth = 6, # maximal depth of tree min_child_weight = 1, # minimum number of observations required in each terminal node subsample = 1, # percent of training data to sample for each tree colsample_bytree = 1 # percent of columns to sample from for each tree ), verbose = 0 # print the statistics during the process (1 or 0) ) (eval&lt;- ames_xgb$evaluation_log %&gt;% dplyr::summarise( ntrees.train = which(train_rmse_mean == min(train_rmse_mean))[1], rmse.train = min(train_rmse_mean), ntrees.test = which(test_rmse_mean == min(test_rmse_mean))[1], rmse.test = min(test_rmse_mean))) After conducting the 10-fold cross validation, we sorted the output so that it shows us at what iteration (round) our model reached the lowest errors when fitted to the seen part of training data (0.0008097) and unseen part of the training data (). Side note: unseen part of the training data (rmse.test) has nothing to do with test data from the initial split we did at the very beginNing of the chapter and named as ames_test . Here we talk about unseen data in the process of k-fold cross-validation. Unsurprisingly, our model performed very well when fitted to the seen data, suggesting the RMSE being``. Here we see an evidence of overfitting. In other words, our model fits the training part of the training data very well (i.e. suggests low RMSE for train data), but is not generalizable, i.e. when confronted with unseen data, its predictions are not as good as for the trained part(i.e. suggests high RMSE for unseen, test data). # Plot error vs number trees pe&lt;-ggplot(ames_xgb$evaluation_log) + geom_line(aes(iter, train_rmse_mean), color = &quot;red&quot;) + geom_line(aes(iter, test_rmse_mean), color = &quot;blue&quot;) + xlab(label= &quot;Iteration (round)&quot;)+ ylab(label = &quot;Root Mean Square Error&quot;)+ ggtitle(label = &quot;10-fold Cross-validation&quot;) ggplotly(pe) The gap between the blue and the red line in the graph above depicts the performance difference of our model when fitted to the seen data (eval$rmse.train) and when fitted to the unseen data (eval$rmse.test). Our goal is to make our model perform with unseen data as good as possible, i.e. to minimize RMSE as much as possible. To pursue that goal, parameters, that we discussed earlier, need to be as optimally tuned as possible. Therefore, some parameters should be adapted. For the following cross-validation process, we will slightly adapt parameters: Decrease the learning rate eta from 0.3 to 0.03 Set early_stopping_rounds at 50 Reduce maximum tree depth max_depth to 3 Increase minimum number of observations required in each terminal node min_child_weight to 3 Reduce subsample to 0.5 colsample_bytree reduced to 0.5 set.seed(1234) ames_xgb1 &lt;- xgb.cv( data = ames_x_train, # matrix with train data without sale price label = ames_y_train, # numerical vector with sale price with train data nrounds = 2301, # number of iterations objective = &quot;reg:linear&quot;, # indicating regression model early_stopping_rounds = 50, # stopping the training model as soon as evaluation metric (RMSE) does not improve for a given number of rounds nfold = 10, # data is randomly partitioned into nfold equal size subsamples params = list( eta = 0.03, # learning rate max_depth = 3, # maximal depth of tree min_child_weight = 3, # minimum number of observations required in each terminal node subsample = 0.5, # percent of training data to sample for each tree colsample_bytree = 0.5 # percent of columns to sample from for each tree ), verbose = 0 ) # Checking results (eval1&lt;-ames_xgb1$evaluation_log %&gt;% dplyr::summarise( ntrees.train = which(train_rmse_mean == min(train_rmse_mean))[1], rmse.train = min(train_rmse_mean), ntrees.test = which(test_rmse_mean == min(test_rmse_mean))[1], rmse.test = min(test_rmse_mean),)) # Plot error vs number trees pr1 &lt;-ggplot(ames_xgb1$evaluation_log) + geom_line(aes(iter, train_rmse_mean), color = &quot;red&quot;) + geom_line(aes(iter, test_rmse_mean), color = &quot;blue&quot;) + ggtitle(label = &quot;10-fold Cross-validation&quot;) ggplotly(pr1) We can see that the training error increased. However, the error on unseen data reaches a minimum RMSE of with iterations. With simple adaptation of our parameters we managed to decrease it to some extent. At this point it should be clear that it would take incredible effort to manually compute those errors for each possible combination of parameters that would potentially decrease the error further. Luckily, there are more elegant, automated solution for it. We can create a hyperparameter search grid along with columns to dump results in. Each row of the grid contains a combination of parameters we would like to model: # Hyperparameter grid hyper_grid &lt;- expand.grid( eta = c(.01,0.3), max_depth = c(1,3,5,7), min_child_weight = 3, subsample = 0.5, colsample_bytree = 0.5, gamma = c(0, 1, 10, 100, 1000), lambda = c(0, 1e-2, 0.1, 1, 100, 1000, 10000), alpha = c(0, 1e-2, 0.1, 1, 100, 1000, 10000), rmse = 0, # a place to dump RMSE results trees = 0 # a place to dump required number of trees ) # Head of the hyperparameter grid kable(head(hyper_grid)) eta max_depth min_child_weight subsample colsample_bytree gamma lambda alpha rmse trees 0.01 1 3 0.5 0.5 0 0 0 0 0 0.30 1 3 0.5 0.5 0 0 0 0 0 0.01 3 3 0.5 0.5 0 0 0 0 0 0.30 3 3 0.5 0.5 0 0 0 0 0 0.01 5 3 0.5 0.5 0 0 0 0 0 0.30 5 3 0.5 0.5 0 0 0 0 0 Besides those parameters we discussed, xgboost provides additional hyperparameters alpha, gamma and lambda that can help to constrain model complexity and reduce overfitting. We introduced them in the grid as well. With the code above we create a pretty large search grid consisting of 1960 different hyperparameter combinations to model. It is important to note that running such a grid in a loop procedure could take a couple of hours. We will create such a loop procedure to loop through and apply a xgboost model for each hyperparameter combination (1960 in our case) and finally provide us the results in the hyper_grid data frame. # Grid search for(i in seq_len(nrow(hyper_grid))) { set.seed(1234) m &lt;- xgb.cv( data = ames_x_train, label = ames_y_train, nrounds = 4000, objective = &quot;reg:linear&quot;, early_stopping_rounds = 50, nfold = 10, verbose = 0, params = list( eta = hyper_grid$eta[i], max_depth = hyper_grid$max_depth[i], min_child_weight = hyper_grid$min_child_weight[i], subsample = hyper_grid$subsample[i], colsample_bytree = hyper_grid$colsample_bytree[i], gamma = hyper_grid$gamma[i], lambda = hyper_grid$lambda[i], alpha = hyper_grid$alpha[i] ) ) hyper_grid$rmse[i] &lt;- min(m$evaluation_log$test_rmse_mean) hyper_grid$trees[i] &lt;- m$best_iteration } # Results hyper_grid %&gt;% filter(rmse &gt; 0) %&gt;% arrange(rmse) %&gt;% glimpse() Here is a glimpse of results we obtained after several hours of processing: ## Observations: 98 ## Variables: 10 ## $ eta &lt;dbl&gt; 0.01, 0.01, 0.01, 0.01, 0.01, 0.0… ## $ max_depth &lt;dbl&gt; 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, … ## $ min_child_weight &lt;dbl&gt; 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, … ## $ subsample &lt;dbl&gt; 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5… ## $ colsample_bytree &lt;dbl&gt; 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5… ## $ gamma &lt;dbl&gt; 0, 1, 10, 100, 1000, 0, 1, 10, 10… ## $ lambda &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, … ## $ alpha &lt;dbl&gt; 0.00, 0.00, 0.00, 0.00, 0.00, 0.1… ## $ rmse &lt;dbl&gt; 20488, 20488, 20488, 20488, 20488… ## $ trees &lt;dbl&gt; 3944, 3944, 3944, 3944, 3944, 381… In the first “column” we see the combination of parameters given that results in the lowest estimated error (RMSE) possible for the combination given. Subsequently, we will use those parameters to enhance prediction performance of our model. # The list of optimal hyperparameters params_optimal &lt;- list( eta = 0.01, max_depth = 3, min_child_weight = 3, subsample = 0.5, colsample_bytree = 0.5, lambda = 1 ) # Train final model with optimal combination of the given parameters set.seed(1234) xgb.fit.optimal &lt;- xgboost( params = params_optimal, data = ames_x_train, label = ames_y_train, nrounds = 3944, objective = &quot;reg:linear&quot;, verbose = 0 ) After computing the final model, we can make inferences about how features (i.e. variables in our data set besides sale price) are influencing our model. Measurement of feature importance occurs based on the sum of the reduction in the loss function (e.g. SSE) attributed to each variable at each split in a respective tree. In other words, it is the relative contribution of the respective feature to the model computed by taking each feature’s contribution for each tree in the model. Therefore, those features with the highest average decrease in SSE (for regression) are identified as the one with the highest contribution. Thus, these features are among most important ones. To visualize feature importance plot we need to create importance matrix first then plot it with ggplot-based function “xgb.ggplot.importance”. # Construct importance matrix importance_matrix &lt;- xgb.importance(model = xgb.fit.optimal) # Variable importance plot with ggplot2 xgb.ggplot.importance(importance_matrix, top_n = 15) As we identified the most relevant features, now we can try to understand how the response variable (i.e. predicted sale price) changes based on these variables. For this we can use partial dependence plots (PDPs). They show the marginal effect one or two features have on the predicted outcome. Let’s consider the latitude variable. The PDP plot below displays the average change in predicted sales price as we vary latitude while holding all other variables constant. More specifically, it shows the movement of the predicted sales price as the square footage of the ground floor in a house changes, while holding other variables constant. It is important to mention that PDPs are valid as long as the target variable (sale price) and the variable under observation (latitude) are not correlated. If this assumption is violated, the averages calculated for the partial dependence plot will include data points that are very unlikely or even impossible. library(pdp) pdp &lt;- xgb.fit.optimal %&gt;% pdp::partial(pred.var = &quot;gr_liv_area&quot;, n.trees = 3944, grid.resolution = 100, train = ames_x_train) %&gt;% autoplot(rug = TRUE, train = ames_x_train) + scale_y_continuous(labels = scales::dollar) + ggplot2::xlab(label= &quot;Ground floor living area&quot;)+ ggplot2::ylab(label=&quot;Predicted sale price&quot;)+ ggtitle(&quot;PDP - Influence of the ground floor size on a house sale price&quot;) ggplotly(pdp) We use predict function to predict unseen observations from the test data set we created at the beginning of the chapter. Since we already know the real sale prices from the test data set, we will be able to calculate the error of our predictive model. # Predict values for test data optimal pred.optimal &lt;- predict(xgb.fit.optimal, ames_x_test) # Results with optimal parameters RMSE(pred.optimal, ames_test[,308]) ## [1] 0.005351052 Finally, we can nicely visualize predicted and actual sale price. # Plot predictions vs actual sale price pred.optimal &lt;- as.data.frame(pred.optimal) ames_test_pred&lt;- data.frame(ames_test[,308],pred.optimal) colnames(ames_test_pred) &lt;- c(&quot;sale_price&quot;,&quot;pred.optimal&quot;) p&lt;-ggplot(ames_test_pred, aes(x = pred.optimal, y = sale_price, Predicted=pred.optimal, Tested=sale_price)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = TRUE)+ xlab(label = &quot;Predicted sale price&quot; )+ ylab(label = &quot;Test sale price&quot;)+ ggtitle(label = &quot;Predicted sale price vs sales price&quot;) ggplotly(p,tooltip = c(&quot;Predicted&quot;,&quot;Tested&quot;)) "],
["exercises.html", "12 Exercises 12.1 Exercise in Machine learning (IN PROGRESS)", " 12 Exercises 12.1 Exercise in Machine learning (IN PROGRESS) In this exercise you will fit a gradient boosting model using xgboost package to predict the number of e-scooters rented in an hour. Features in the data set are related to weather conditions and other important information. Your task will be to train the model on data from one month (i.e. May) and do prediction on data for another month (i.e. June). 12.1.1 Exercise to download (IN PROGRESS) In this exercise you will fit a gradient boosting model using xgboost package to predict the number of e-scooters rented in an hour. Features in the data set are related to weather conditions and other important information. Your task will be to train the model on data from one month (i.e. May) and do prediction on data for another month (i.e. June). "]
]
